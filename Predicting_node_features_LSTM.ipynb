{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import os\n",
    "import mne\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used to define data paths\n",
    "ROOT_DIR = './'\n",
    "RAW_DIR = 'data/matlabPREPROCESSED'\n",
    "PROCESSED_DIR = 'data/graphProcessedData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_names = os.listdir(RAW_DIR)\n",
    "raw_file_names.sort()\n",
    "all_subjects = [scipy.io.loadmat(f'{RAW_DIR}/{name}') for name in raw_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [subject['data'] for subject in all_subjects]\n",
    "all_labels = [subject['labels'] for subject in all_subjects]\n",
    "labels = np.stack(all_labels)\n",
    "data = np.stack(all_data)\n",
    "# Keep just EEG signals and remove first 3 seconds\n",
    "data = data[:,:,:32,128*3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 40, 32, 7680)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 32, 7680)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_participant_data = data[0]\n",
    "first_participant_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 7680)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_participant_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f41d5981430>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(first_participant_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_participant(participant_data, look_back = 1, window_size=128, overlap_sequences=True, n_videos=40):\n",
    "    \"\"\"\n",
    "    For a certain participant, create input and target features\n",
    "    :param participant_data: Original participant_data. Shape (40, 32, 7680)\n",
    "    :param look_back: Number of windows included as input features\n",
    "    :param window_size: Number of data points per window\n",
    "    :param overlap_sequences: Wether to add 1 or window_size to get the next sequence.\n",
    "    :return: \n",
    "        inputs: Input features with shape (N, look_back, 32, window_size)\n",
    "        targets: Target with shape (N, 32, window_size)\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for video in participant_data[:n_videos]:\n",
    "        idx = 0\n",
    "        while idx + window_size * (look_back + 1) <= video.shape[1]:\n",
    "#             print(idx, idx+window_size*look_back)\n",
    "#             print(idx+window_size*look_back,idx+window_size*(look_back+1))\n",
    "            inpt = video[:,idx:idx+window_size*look_back]\n",
    "            inpt = rearrange(inpt,'a (b c) -> b a c',c=window_size)\n",
    "            tget = video[:,idx+window_size*look_back:idx+window_size*(look_back+1)]\n",
    "            inputs.append(inpt)\n",
    "            targets.append(tget)\n",
    "            if overlap_sequences:\n",
    "                idx += 10\n",
    "            else:\n",
    "                idx += window_size\n",
    "#             print(idx)\n",
    "#             print(tget.shape)\n",
    "#         print(len(inputs))\n",
    "    \n",
    "    return inputs,targets\n",
    "\n",
    "look_back = 10\n",
    "window_size = 128\n",
    "inputs, targets = create_data_for_participant(first_participant_data,look_back,window_size,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25120, 25120)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs),len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 32, 128), (32, 128))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].shape, targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.34449797,  3.12143258, -1.99934033, ..., -6.6957948 ,\n",
       "        -6.31902842, -3.33689666],\n",
       "       [ 3.65577633,  4.18299247, -0.07301596, ..., -2.96785212,\n",
       "        -3.93465645, -0.15646287],\n",
       "       [ 2.03966913,  1.76355649,  0.09397287, ..., -3.41369664,\n",
       "        -4.38745875, -0.71169342],\n",
       "       ...,\n",
       "       [ 4.70944462,  3.91674961,  2.76729718, ...,  0.69950872,\n",
       "         0.64455754, -1.39503669],\n",
       "       [-3.04320454, -2.87813307,  0.99223196, ...,  0.86336261,\n",
       "         3.22549374,  1.51269125],\n",
       "       [-2.06397904, -0.02035828,  2.45051504, ..., -1.59205962,\n",
       "         0.97492084, -0.47998268]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, lstm_cells,look_back):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_channels, in_channels*2, lstm_cells)\n",
    "        self.lin = nn.Linear(in_channels*2*look_back,in_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        x = rearrange(out,'a b c -> b (a c)')\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DEAPPrediction(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, targets, transform=None):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {'input': self.inputs[idx], 'target': self.targets[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.FloatTensor(inputs)\n",
    "targets = torch.FloatTensor(targets)\n",
    "\n",
    "train_inputs = inputs[:int(len(inputs)*0.8)]\n",
    "train_targets = targets[:int(len(inputs)*0.8)]\n",
    "test_inputs = inputs[int(len(inputs)*0.8):]\n",
    "test_targets = targets[:int(len(inputs)*0.8)]\n",
    "\n",
    "train_dataset = DEAPPrediction(train_inputs,train_targets)\n",
    "test_dataset = DEAPPrediction(test_inputs,test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count: 1249408\n",
      "Epoch 1 ; Train loss: 18.458124379443515 ; Test loss: 17.624909789698897\n",
      "Epoch 2 ; Train loss: 18.06024005306754 ; Test loss: 17.865214353913714\n",
      "Epoch 3 ; Train loss: 17.830401031834306 ; Test loss: 17.9649560831155\n",
      "Epoch 4 ; Train loss: 17.71068220837101 ; Test loss: 18.177021099503634\n",
      "Epoch 5 ; Train loss: 17.6795483819998 ; Test loss: 18.139592808522995\n",
      "Epoch 6 ; Train loss: 17.64960427800561 ; Test loss: 17.99579418692619\n",
      "Epoch 7 ; Train loss: 17.6362078903587 ; Test loss: 18.18427670229772\n",
      "Epoch 8 ; Train loss: 17.631008177046564 ; Test loss: 18.116469146339757\n",
      "Epoch 9 ; Train loss: 17.629169544596582 ; Test loss: 18.164171935646397\n",
      "Epoch 10 ; Train loss: 17.6219698350141 ; Test loss: 18.134632007331604\n",
      "Epoch 11 ; Train loss: 17.623152884708087 ; Test loss: 18.03022819567638\n",
      "Epoch 12 ; Train loss: 17.617887911523223 ; Test loss: 18.06616436903644\n",
      "Epoch 13 ; Train loss: 17.617204763327436 ; Test loss: 18.147487713273165\n",
      "Epoch 14 ; Train loss: 17.616498350337814 ; Test loss: 18.058627632772847\n",
      "Epoch 15 ; Train loss: 17.618097955254232 ; Test loss: 18.076220573133725\n",
      "Epoch 16 ; Train loss: 17.612340439656737 ; Test loss: 18.040236667462974\n",
      "Epoch 17 ; Train loss: 17.612244050214244 ; Test loss: 18.208990176012563\n",
      "Epoch 18 ; Train loss: 17.610758793581823 ; Test loss: 18.098572591307818\n",
      "Epoch 19 ; Train loss: 17.612451495638318 ; Test loss: 18.108447463649092\n",
      "Epoch 20 ; Train loss: 17.610187743120132 ; Test loss: 18.106265232061883\n",
      "Epoch 21 ; Train loss: 17.608404780649074 ; Test loss: 18.135141494167836\n",
      "Epoch 22 ; Train loss: 17.605484224428796 ; Test loss: 18.160104162374118\n",
      "Epoch 23 ; Train loss: 17.608501949128073 ; Test loss: 18.185782845612543\n",
      "Epoch 24 ; Train loss: 17.605996262495683 ; Test loss: 18.131280267314548\n",
      "Epoch 25 ; Train loss: 17.60696676430429 ; Test loss: 18.126836418346237\n",
      "Epoch 26 ; Train loss: 17.606577426764616 ; Test loss: 18.133473651424335\n",
      "Epoch 27 ; Train loss: 17.605615564212677 ; Test loss: 18.005822479345237\n",
      "Epoch 28 ; Train loss: 17.608115581949804 ; Test loss: 18.158278939070975\n",
      "Epoch 29 ; Train loss: 17.60482139192569 ; Test loss: 18.080507041542393\n",
      "Epoch 30 ; Train loss: 17.60662251824786 ; Test loss: 18.068493715517082\n",
      "Epoch 31 ; Train loss: 17.606043583268573 ; Test loss: 18.12148572229276\n",
      "Epoch 32 ; Train loss: 17.6044072573352 ; Test loss: 18.104472397239345\n",
      "Epoch 33 ; Train loss: 17.605902337724235 ; Test loss: 18.217044793876113\n",
      "Epoch 34 ; Train loss: 17.603789179188432 ; Test loss: 18.080931019631162\n",
      "Epoch 35 ; Train loss: 17.60339654175339 ; Test loss: 18.193769539997078\n",
      "Epoch 36 ; Train loss: 17.604913040331216 ; Test loss: 18.142734205646878\n",
      "Epoch 37 ; Train loss: 17.60787301306512 ; Test loss: 18.107892662096933\n",
      "Epoch 38 ; Train loss: 17.60702382227418 ; Test loss: 18.115381890801107\n",
      "Epoch 39 ; Train loss: 17.605996532804646 ; Test loss: 18.14205752208734\n",
      "Epoch 40 ; Train loss: 17.606302937124944 ; Test loss: 18.070100195088962\n",
      "Epoch 41 ; Train loss: 17.606185750596843 ; Test loss: 18.08166930326231\n",
      "Epoch 42 ; Train loss: 17.608946223167855 ; Test loss: 18.071177695207535\n",
      "Epoch 43 ; Train loss: 17.604355749810576 ; Test loss: 18.196358030768717\n",
      "Epoch 44 ; Train loss: 17.606370851492425 ; Test loss: 18.120411939681716\n",
      "Epoch 45 ; Train loss: 17.60278287207245 ; Test loss: 18.248528680983622\n",
      "Epoch 46 ; Train loss: 17.606016313953763 ; Test loss: 18.102273916742604\n",
      "Epoch 47 ; Train loss: 17.604419662694262 ; Test loss: 18.128564852817803\n",
      "Epoch 48 ; Train loss: 17.605475336123423 ; Test loss: 18.079733951835877\n",
      "Epoch 49 ; Train loss: 17.60536532645013 ; Test loss: 18.045197614438973\n",
      "Epoch 50 ; Train loss: 17.606572852772512 ; Test loss: 18.172687591261166\n",
      "Epoch 51 ; Train loss: 17.605335322155316 ; Test loss: 17.99330004309393\n",
      "Epoch 52 ; Train loss: 17.605025976326814 ; Test loss: 18.07371664958395\n",
      "Epoch 53 ; Train loss: 17.604207682761416 ; Test loss: 18.134140609935592\n",
      "Epoch 54 ; Train loss: 17.599944743381183 ; Test loss: 18.168961962317205\n",
      "Epoch 55 ; Train loss: 17.604696311768453 ; Test loss: 18.04818564007996\n",
      "Epoch 56 ; Train loss: 17.603137728514945 ; Test loss: 18.126215673555993\n",
      "Epoch 57 ; Train loss: 17.601979864630728 ; Test loss: 18.091468865704385\n",
      "Epoch 58 ; Train loss: 17.6041279476919 ; Test loss: 18.06787185304484\n",
      "Epoch 59 ; Train loss: 17.602153635328744 ; Test loss: 18.114060571998547\n",
      "Epoch 60 ; Train loss: 17.602873548580583 ; Test loss: 18.19545827853452\n",
      "Epoch 61 ; Train loss: 17.603409553029735 ; Test loss: 18.07488413525235\n",
      "Epoch 62 ; Train loss: 17.603341289386627 ; Test loss: 18.124447294101593\n",
      "Epoch 63 ; Train loss: 17.601619102392988 ; Test loss: 18.105741968580112\n",
      "Epoch 64 ; Train loss: 17.60525365543973 ; Test loss: 18.093603972416773\n",
      "Epoch 65 ; Train loss: 17.60490590447833 ; Test loss: 18.02153154846969\n",
      "Epoch 66 ; Train loss: 17.606202172625597 ; Test loss: 18.069382989482516\n",
      "Epoch 67 ; Train loss: 17.605194799459664 ; Test loss: 18.068535579997263\n",
      "Epoch 68 ; Train loss: 17.60436913010421 ; Test loss: 18.09869206786915\n",
      "Epoch 69 ; Train loss: 17.601813589691357 ; Test loss: 18.06752014767592\n",
      "Epoch 70 ; Train loss: 17.601636718033227 ; Test loss: 17.99844246153619\n",
      "Epoch 71 ; Train loss: 17.605343606061997 ; Test loss: 18.105232080836206\n",
      "Epoch 72 ; Train loss: 17.604211467086888 ; Test loss: 18.079374052157068\n",
      "Epoch 73 ; Train loss: 17.605640807728857 ; Test loss: 18.090048662416496\n",
      "Epoch 74 ; Train loss: 17.60245432519609 ; Test loss: 18.196558976628978\n",
      "Epoch 75 ; Train loss: 17.605492506816887 ; Test loss: 18.07516173344509\n",
      "Epoch 76 ; Train loss: 17.602497946684526 ; Test loss: 18.020983301150572\n",
      "Epoch 77 ; Train loss: 17.607119678691696 ; Test loss: 18.151749635198314\n",
      "Epoch 78 ; Train loss: 17.604265735407544 ; Test loss: 18.049844699300778\n",
      "Epoch 79 ; Train loss: 17.60474233262858 ; Test loss: 18.107112817703538\n",
      "Epoch 80 ; Train loss: 17.60157614300965 ; Test loss: 18.10657652472235\n",
      "Epoch 81 ; Train loss: 17.60620674509911 ; Test loss: 18.039839027793544\n",
      "Epoch 82 ; Train loss: 17.604236862462038 ; Test loss: 18.127223683011\n",
      "Epoch 83 ; Train loss: 17.601305501476215 ; Test loss: 18.154229929492733\n",
      "Epoch 84 ; Train loss: 17.603441926324443 ; Test loss: 18.122659640707028\n",
      "Epoch 85 ; Train loss: 17.60411688324752 ; Test loss: 18.141923303057435\n",
      "Epoch 86 ; Train loss: 17.603283520716772 ; Test loss: 18.144550949145273\n",
      "Epoch 87 ; Train loss: 17.601200330029627 ; Test loss: 18.0534220956693\n",
      "Epoch 88 ; Train loss: 17.606576073701216 ; Test loss: 18.021518403557454\n",
      "Epoch 89 ; Train loss: 17.602264575897507 ; Test loss: 18.003569912758604\n",
      "Epoch 90 ; Train loss: 17.603122102227182 ; Test loss: 18.08502740313293\n",
      "Epoch 91 ; Train loss: 17.601710409115835 ; Test loss: 18.10266516011232\n",
      "Epoch 92 ; Train loss: 17.60536312753228 ; Test loss: 18.100458345595438\n",
      "Epoch 93 ; Train loss: 17.601858154223983 ; Test loss: 18.1487297981408\n",
      "Epoch 94 ; Train loss: 17.600697812001417 ; Test loss: 18.131480806192776\n",
      "Epoch 95 ; Train loss: 17.6010009969116 ; Test loss: 18.162130197901636\n",
      "Epoch 96 ; Train loss: 17.60541891444261 ; Test loss: 18.221070101306697\n",
      "Epoch 97 ; Train loss: 17.60388937725383 ; Test loss: 18.113491926982903\n",
      "Epoch 98 ; Train loss: 17.602621616071957 ; Test loss: 18.125214467382733\n",
      "Epoch 99 ; Train loss: 17.6015914625423 ; Test loss: 18.086668719151977\n",
      "Epoch 100 ; Train loss: 17.602890209027915 ; Test loss: 18.150681161576774\n",
      "Epoch 101 ; Train loss: 17.604108356366492 ; Test loss: 18.094535602885447\n",
      "Epoch 102 ; Train loss: 17.60204529306691 ; Test loss: 18.037394298869334\n",
      "Epoch 103 ; Train loss: 17.601792221616027 ; Test loss: 18.243188766916845\n",
      "Epoch 104 ; Train loss: 17.6024054600175 ; Test loss: 18.2227939557118\n",
      "Epoch 105 ; Train loss: 17.603677937938908 ; Test loss: 18.120810581620333\n",
      "Epoch 106 ; Train loss: 17.603048500741362 ; Test loss: 18.036073162297534\n",
      "Epoch 107 ; Train loss: 17.602772638296624 ; Test loss: 18.087001496819173\n",
      "Epoch 108 ; Train loss: 17.604110319902944 ; Test loss: 18.085356305359277\n",
      "Epoch 109 ; Train loss: 17.60463491215068 ; Test loss: 18.073962163014016\n",
      "Epoch 110 ; Train loss: 17.60575855157937 ; Test loss: 18.12590334218019\n",
      "Epoch 111 ; Train loss: 17.606013126434036 ; Test loss: 18.111604599436376\n",
      "Epoch 112 ; Train loss: 17.602188863572042 ; Test loss: 18.091592855514236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113 ; Train loss: 17.605795306005295 ; Test loss: 18.098662667973027\n",
      "Epoch 114 ; Train loss: 17.602341088519733 ; Test loss: 18.109359571128895\n",
      "Epoch 115 ; Train loss: 17.604903895384187 ; Test loss: 18.184307353511738\n",
      "Epoch 116 ; Train loss: 17.606948776609578 ; Test loss: 18.107120088710907\n",
      "Epoch 117 ; Train loss: 17.602868622275675 ; Test loss: 18.073007565395088\n",
      "Epoch 118 ; Train loss: 17.604027734440603 ; Test loss: 18.159142579242683\n",
      "Epoch 119 ; Train loss: 17.606693393865207 ; Test loss: 18.087972574173264\n",
      "Epoch 120 ; Train loss: 17.603578930447814 ; Test loss: 18.095913079134217\n",
      "Epoch 121 ; Train loss: 17.600963493820966 ; Test loss: 18.10821439050565\n",
      "Epoch 122 ; Train loss: 17.604342337626562 ; Test loss: 18.094809787288593\n",
      "Epoch 123 ; Train loss: 17.601861335669355 ; Test loss: 18.20731197648747\n",
      "Epoch 124 ; Train loss: 17.606444366418632 ; Test loss: 18.040911188550815\n",
      "Epoch 125 ; Train loss: 17.604209178572248 ; Test loss: 18.13805108768925\n",
      "Epoch 126 ; Train loss: 17.603849735988934 ; Test loss: 18.19973897630242\n",
      "Epoch 127 ; Train loss: 17.604260810621223 ; Test loss: 18.05841190799786\n",
      "Epoch 128 ; Train loss: 17.603306741471503 ; Test loss: 18.083546808570812\n",
      "Epoch 129 ; Train loss: 17.603052193951456 ; Test loss: 18.121658689656833\n",
      "Epoch 130 ; Train loss: 17.601576721592313 ; Test loss: 18.06371540020985\n",
      "Epoch 131 ; Train loss: 17.607031807018693 ; Test loss: 18.088247736548162\n",
      "Epoch 132 ; Train loss: 17.603257990187142 ; Test loss: 18.098563923197947\n",
      "Epoch 133 ; Train loss: 17.603262702370905 ; Test loss: 18.064886518344757\n",
      "Epoch 134 ; Train loss: 17.602602059674112 ; Test loss: 18.21305325987992\n",
      "Epoch 135 ; Train loss: 17.60122814148095 ; Test loss: 18.086304427711827\n",
      "Epoch 136 ; Train loss: 17.602711422428204 ; Test loss: 18.129609369168616\n",
      "Epoch 137 ; Train loss: 17.60496809983709 ; Test loss: 18.111751732552886\n",
      "Epoch 138 ; Train loss: 17.603614482150714 ; Test loss: 18.01473869639597\n",
      "Epoch 139 ; Train loss: 17.604183001123417 ; Test loss: 18.15032864831815\n",
      "Epoch 140 ; Train loss: 17.602216348526586 ; Test loss: 18.076460777574283\n",
      "Epoch 141 ; Train loss: 17.605118593592554 ; Test loss: 18.107133689200044\n",
      "Epoch 142 ; Train loss: 17.60364308326867 ; Test loss: 18.077653550797965\n",
      "Epoch 143 ; Train loss: 17.60468596713558 ; Test loss: 18.140286779707406\n",
      "Epoch 144 ; Train loss: 17.60436375125958 ; Test loss: 18.060693182003725\n",
      "Epoch 145 ; Train loss: 17.60452455016458 ; Test loss: 18.12914804592254\n",
      "Epoch 146 ; Train loss: 17.604104353364107 ; Test loss: 18.068638710459325\n",
      "Epoch 147 ; Train loss: 17.602592679345683 ; Test loss: 18.070735451522147\n",
      "Epoch 148 ; Train loss: 17.602052964981954 ; Test loss: 18.014122039649138\n",
      "Epoch 149 ; Train loss: 17.601733506864804 ; Test loss: 18.15014676986986\n",
      "Epoch 150 ; Train loss: 17.604317624098176 ; Test loss: 18.10282137439509\n",
      "Epoch 151 ; Train loss: 17.60398296489837 ; Test loss: 18.086968282225786\n",
      "Epoch 152 ; Train loss: 17.60420068813737 ; Test loss: 18.134570030649755\n",
      "Epoch 153 ; Train loss: 17.604375335061626 ; Test loss: 18.06679357540835\n",
      "Epoch 154 ; Train loss: 17.604986344173454 ; Test loss: 18.087378617304907\n",
      "Epoch 155 ; Train loss: 17.603470117423186 ; Test loss: 18.106605171397995\n",
      "Epoch 156 ; Train loss: 17.601637330024865 ; Test loss: 18.099077564895534\n",
      "Epoch 157 ; Train loss: 17.605307923760385 ; Test loss: 18.022109560146454\n",
      "Epoch 158 ; Train loss: 17.601974983883512 ; Test loss: 18.118815835114496\n",
      "Epoch 159 ; Train loss: 17.606056646176963 ; Test loss: 18.02706776151232\n",
      "Epoch 160 ; Train loss: 17.602794006371955 ; Test loss: 18.15225112817849\n",
      "Epoch 161 ; Train loss: 17.60342615425207 ; Test loss: 18.09488185348025\n",
      "Epoch 162 ; Train loss: 17.602451257644944 ; Test loss: 18.063918915523846\n",
      "Epoch 163 ; Train loss: 17.605394108280255 ; Test loss: 18.017404683835945\n",
      "Epoch 164 ; Train loss: 17.601589687310966 ; Test loss: 18.123104599630757\n",
      "Epoch 165 ; Train loss: 17.601558566852738 ; Test loss: 18.07666049641409\n",
      "Epoch 166 ; Train loss: 17.606126227955908 ; Test loss: 18.207697473513853\n",
      "Epoch 167 ; Train loss: 17.604306951449935 ; Test loss: 18.005178281456043\n",
      "Epoch 168 ; Train loss: 17.605640727243607 ; Test loss: 18.15252937025325\n",
      "Epoch 169 ; Train loss: 17.602876640429162 ; Test loss: 18.0527788696775\n",
      "Epoch 170 ; Train loss: 17.606218562763967 ; Test loss: 18.03760018318322\n",
      "Epoch 171 ; Train loss: 17.60323912930337 ; Test loss: 18.0627052647293\n",
      "Epoch 172 ; Train loss: 17.605999905592316 ; Test loss: 18.1551970828111\n"
     ]
    }
   ],
   "source": [
    "model = LSTMPredictor(window_size,2,look_back)     \n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameter count: {pytorch_total_params}')\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1, rho=0.9, eps=1e-06, weight_decay=0.01)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = model.to(device)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inpt = batch['input'].to(device)\n",
    "        tget = batch['target'].to(device)\n",
    "        \n",
    "        inpt = rearrange(inpt, 'a b c d -> b (a c) d')\n",
    "        tget = rearrange(tget, 'a b c -> (a b) c')\n",
    "        \n",
    "        out = model(inpt)\n",
    "        loss = criterion(out,tget)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "    return np.array(losses).mean()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in test_loader:\n",
    "        inpt = batch['input'].to(device)\n",
    "        tget = batch['target'].to(device)\n",
    "        \n",
    "        inpt = rearrange(inpt, 'a b c d -> b (a c) d')\n",
    "        tget = rearrange(tget, 'a b c -> (a b) c')\n",
    "        \n",
    "        out = model(inpt)\n",
    "        loss = criterion(out,tget)\n",
    "        losses.append(loss.item())\n",
    "    return np.array(losses).mean()\n",
    "\n",
    "best_val_loss = np.inf\n",
    "esp = 0\n",
    "MAX_ESP = 20\n",
    "for epoch in range(1, 2000):\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "#     if test_loss < best_val_loss:\n",
    "#         best_val_loss = test_loss\n",
    "#         esp = 0\n",
    "#     else:\n",
    "#         esp += 1\n",
    "#         if esp >= MAX_ESP:\n",
    "#             break\n",
    "    print(f'Epoch {epoch} ; Train loss: {train_loss} ; Test loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
