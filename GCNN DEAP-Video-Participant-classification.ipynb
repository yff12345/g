{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skimage\n",
    "import pywt\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from einops import reduce, rearrange, repeat\n",
    "from npeet import entropy_estimators as ee\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from scipy.fft import rfft, rfftfreq, ifft\n",
    "from einops import rearrange\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from Electrodes import Electrodes\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDatasetEEGFeatures(InMemoryDataset):\n",
    "  def __init__(self, root, raw_dir, processed_dir, feature='de',target='participant_id', transform=None, pre_transform=None,include_edge_attr = True, undirected_graphs = True, add_global_connections=True, participant_from=1, participant_to=32, n_videos=40):\n",
    "      self._raw_dir = raw_dir\n",
    "      self._processed_dir = processed_dir\n",
    "      self.participant_from = participant_from\n",
    "      self.participant_to = participant_to\n",
    "      self.n_videos = n_videos\n",
    "      self.feature = feature\n",
    "      self.target = target\n",
    "      # Whether or not to include edge_attr in the dataset\n",
    "      self.include_edge_attr = include_edge_attr\n",
    "      # If true there will be 1024 links as opposed to 528\n",
    "      self.undirected_graphs = undirected_graphs\n",
    "      # Instantiate class to handle electrode positions\n",
    "      print('Using global connections' if add_global_connections else 'Not using global connections')\n",
    "      self.electrodes = Electrodes(add_global_connections, expand_3d = False)\n",
    "      super(DEAPDatasetEEGFeatures, self).__init__(root, transform, pre_transform)\n",
    "      self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "      \n",
    "  @property\n",
    "  def raw_dir(self):\n",
    "      return f'{self.root}/{self._raw_dir}'\n",
    "\n",
    "  @property\n",
    "  def processed_dir(self):\n",
    "      return f'{self.root}/{self._processed_dir}'\n",
    "\n",
    "  @property\n",
    "  def raw_file_names(self):\n",
    "      raw_names = [f for f in os.listdir(self.raw_dir)]\n",
    "      raw_names.sort()\n",
    "      return raw_names\n",
    "\n",
    "  @property\n",
    "  def processed_file_names(self):\n",
    "      if not os.path.exists(self.processed_dir):\n",
    "        os.makedirs(self.processed_dir)\n",
    "      file_name = f'{self.participant_from}-{self.participant_to}' if self.participant_from is not self.participant_to else f'{self.participant_from}'\n",
    "      return [f'deap_processed_graph.{file_name}_{self.feature}_{self.target}.dataset']\n",
    "\n",
    "  def process(self):\n",
    "        # Number of nodes per graph\n",
    "        n_nodes = len(self.electrodes.positions_3d)\n",
    "        \n",
    "\n",
    "        if self.undirected_graphs:\n",
    "            source_nodes, target_nodes = np.repeat(np.arange(0,n_nodes),n_nodes), np.tile(np.arange(0,n_nodes),n_nodes)\n",
    "        else:\n",
    "            source_nodes, target_nodes = np.tril_indices(n_nodes,n_nodes)\n",
    "        \n",
    "        edge_attr = self.electrodes.adjacency_matrix[source_nodes,target_nodes]\n",
    "        \n",
    "        # Remove zero weight links\n",
    "        mask = np.ma.masked_not_equal(edge_attr, 0).mask\n",
    "        edge_attr,source_nodes,target_nodes = edge_attr[mask], source_nodes[mask], target_nodes[mask]\n",
    "\n",
    "        edge_attr, edge_index = torch.FloatTensor(edge_attr), torch.tensor([source_nodes,target_nodes], dtype=torch.long)\n",
    "        \n",
    "        # Expand edge_index and edge_attr to match windows\n",
    "        e_edge_index = edge_index.clone()\n",
    "        e_edge_attr = edge_attr.clone()\n",
    "        \n",
    "        number_of_graphs = 4\n",
    "        for i in range(number_of_graphs-1):\n",
    "            a = edge_index + e_edge_index.max() + 1\n",
    "            e_edge_index = torch.cat([e_edge_index,a],dim=1)\n",
    "            e_edge_attr = torch.cat([e_edge_attr,edge_attr],dim=0)\n",
    "\n",
    "        print(f'Number of graphs per video: {number_of_graphs}')\n",
    "        # List of graphs that will be written to file\n",
    "        data_list = []\n",
    "        pbar = tqdm(range(self.participant_from,self.participant_to+1))\n",
    "        for participant_id in pbar:\n",
    "            raw_name = [e for e in self.raw_file_names if str(participant_id).zfill(2) in e][0]\n",
    "            pbar.set_description(raw_name)\n",
    "            # Load raw file as np array\n",
    "            participant_data = scipy.io.loadmat(f'{self.raw_dir}/{raw_name}')\n",
    "            signal_data = torch.FloatTensor(remove_baseline_mean(participant_data['data'][:,:32,:]))\n",
    "#             signal_data = torch.FloatTensor()\n",
    "            processed = []\n",
    "            for video_index, video in enumerate(signal_data[:self.n_videos,:,:]):\n",
    "                if self.feature == 'wav':\n",
    "                    node_features = process_video_wavelet(video)\n",
    "                elif self.feature =='wav-entropy':\n",
    "                    node_features = process_video_wavelet(video, feature='entropy')\n",
    "                else:\n",
    "                    node_features = process_video(video, feature=self.feature)\n",
    "                \n",
    "                if self.target == 'emotion_labels':\n",
    "                    target = [participant_data['labels'][video_index]]\n",
    "                if self.target == 'participant_id':\n",
    "                    target = participant_id-1\n",
    "                elif self.target == 'video_id':\n",
    "                    target = video_index\n",
    "                else:\n",
    "                    raise 'Invalid target'\n",
    "                data = Data(x=torch.FloatTensor(node_features),edge_attr=e_edge_attr,edge_index=e_edge_index, y=torch.LongTensor([target])) if self.include_edge_attr else Data(x=torch.FloatTensor(node_features), edge_index=e_edge_index, y=torch.LongTensor([target]))\n",
    "                data_list.append(data) \n",
    "               \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_de(window):\n",
    "    return ee.entropy(window.reshape(-1,1), k=2)\n",
    "# Input: Video with shape (32,7680)\n",
    "# Output: Graph node features with shape (5*32, 59) -> 5 graphs with 32 nodes each with 59 features each\n",
    "def process_video(video, feature='psd'):\n",
    "    # Transform to frequency domain\n",
    "    fft_vals = np.fft.rfft(video, axis=-1)\n",
    "     # Get frequencies for amplitudes in Hz\n",
    "    samplingFrequency = 128\n",
    "    fft_freq = np.fft.rfftfreq(video.shape[-1], 1.0/samplingFrequency)\n",
    "    # Delta, Theta, Alpha, Beta, Gamma\n",
    "    bands = [(0,4),(4,8),(8,12),(12,30),(30,45)]\n",
    "    \n",
    "    band_mask = np.array([np.logical_or(fft_freq < f, fft_freq > t) for f,t in bands])\n",
    "    band_mask = repeat(band_mask,'a b -> a c b', c=32)\n",
    "    band_data = np.array(fft_vals)\n",
    "    band_data = repeat(band_data,'a b -> c a b', c=5)\n",
    "     \n",
    "    band_data[band_mask] = 0\n",
    "    \n",
    "    band_data = np.fft.irfft(band_data)\n",
    "\n",
    "    windows = skimage.util.view_as_windows(band_data, (5,32,128), step=128).squeeze()\n",
    "    # (5, 32, 60, 128)\n",
    "    windows = rearrange(windows, 'a b c d -> b c a d')\n",
    "    \n",
    "    if feature == 'psd':\n",
    "        features = scipy.signal.periodogram(windows)[1]\n",
    "        features = np.mean(features, axis=-1)\n",
    "    elif feature == 'de':\n",
    "        features = np.apply_along_axis(calculate_de, -1, windows)\n",
    "\n",
    "    \n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baseline_mean(signal_data):\n",
    "    # Take first three senconds of data\n",
    "    signal_baseline = np.array(signal_data[:,:,:128*3]).reshape(40,32,128,-1)\n",
    "    # Mean of three senconds of baseline will be deducted from all windows\n",
    "    signal_noise = np.mean(signal_baseline,axis=-1)\n",
    "    # Expand mask\n",
    "    signal_noise = repeat(signal_noise,'a b c -> a b (d c)',d=60)\n",
    "    return signal_data[:,:,128*3:] - signal_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_wavelet(video, feature='energy', time_domain=False):\n",
    "    band_widths = [32,16,8,4]\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            # Highest frequencies (64-128Hz) are not used\n",
    "            cA, cD = pywt.dwt(video.numpy(), 'db4')\n",
    "        else:\n",
    "            cA, cD = pywt.dwt(cA, 'db4')\n",
    "            \n",
    "            cA_windows = skimage.util.view_as_windows(cA, (32,band_widths[i-1]*2), step=band_widths[i-1]).squeeze()\n",
    "            cA_windows = np.transpose(cA_windows[:59,:,:],(1,0,2))\n",
    "            if feature == 'energy':\n",
    "                cA_windows = np.square(cA_windows)\n",
    "                cA_windows = np.sum(cA_windows, axis=-1)\n",
    "                features.append(cA_windows)\n",
    "            elif feature == 'entropy':\n",
    "                cA_windows = np.square(cA_windows) * np.log(np.square(cA_windows))\n",
    "                cA_windows = -np.sum(cA_windows, axis=-1)\n",
    "                features.append(cA_windows)\n",
    "\n",
    "            else:\n",
    "                raise 'Error, invalid wavelet feature'\n",
    "                \n",
    "    if time_domain:\n",
    "        features = np.transpose(features,(2,1,0))\n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "    \n",
    "    # Normalization\n",
    "    m = features.mean(0, keepdim=True)\n",
    "    s = features.std(0, unbiased=False, keepdim=True)\n",
    "    features -= m\n",
    "    features /= s\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global connections\n"
     ]
    }
   ],
   "source": [
    "# Constants used to define data paths\n",
    "ROOT_DIR = './'\n",
    "RAW_DIR = 'data/matlabPREPROCESSED'\n",
    "PROCESSED_DIR = 'data/graphProcessedData'\n",
    "\n",
    "dataset = DEAPDatasetEEGFeatures(root= ROOT_DIR, raw_dir= RAW_DIR, processed_dir= PROCESSED_DIR, feature='wav',participant_to=32)\n",
    "# dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 180)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 880 used for training, 220 validation and 180 testing\n",
    "# test_participant = 1\n",
    "# \n",
    "splt_idx = 1100\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# 85% used for train/val\n",
    "train_dataset = dataset[:splt_idx]\n",
    "test_dataset = dataset[splt_idx:]\n",
    "\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = np.unique(np.array([d.y for d in dataset])).shape[0]\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCN2Conv, GraphConv, global_max_pool as gmp\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels,n_graphs, hidden_channels=16, n_classes = 32):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        self.gconv1 = GraphConv(in_channels,hidden_channels*2)\n",
    "        self.gconv2 = GraphConv(hidden_channels*2,hidden_channels)\n",
    "        \n",
    "#         self.gconv3 = GCNConv(in_channels,hidden_channels)\n",
    "        \n",
    "        # self.rnn = torch.nn.GRU(hidden_channels, rnn_hidden_dim, 2,dropout=0.2, batch_first=True)\n",
    "        self.cnn1 = torch.nn.Conv1d(n_graphs, 1, kernel_size=3, stride=2)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(224, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, n_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        bs = len(torch.unique(batch.batch))\n",
    "        x, edge_index, edge_attr = batch.x, batch.edge_index, batch.edge_attr\n",
    "#         print(x.shape)\n",
    "        x = self.gconv1(x, edge_index, edge_attr)\n",
    "        x = self.gconv2(x, edge_index, edge_attr)\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = x.relu()\n",
    "#         print(x.shape)\n",
    "        x = rearrange(x, '(bs g e) f -> (bs e) g f', bs=bs, e=32)\n",
    "#         print(x.shape)\n",
    "        x = self.cnn1(x).squeeze()\n",
    "        x = x.tanh()\n",
    "        x = rearrange(x, '(bs e) f -> bs (e f)', bs=bs)\n",
    "#         print(x.shape)\n",
    "#         x = torch.sum(x, dim=1)\n",
    "        \n",
    "        \n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "#         print(x.shape)\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "#         print(x.shape)\n",
    "#         x = x.view(-1)\n",
    "       \n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count: 9005\n",
      "Epoch 1;t loss: 3.46284 ;t acc: 0.06 ;v loss: 3.44581 ;v acc: 0.06\n",
      "Epoch 2;t loss: 3.42695 ;t acc: 0.16 ;v loss: 3.32504 ;v acc: 0.24\n",
      "Epoch 3;t loss: 3.29000 ;t acc: 0.29 ;v loss: 3.12330 ;v acc: 0.39\n",
      "Epoch 4;t loss: 3.15423 ;t acc: 0.42 ;v loss: 3.01743 ;v acc: 0.50\n",
      "Epoch 5;t loss: 3.02527 ;t acc: 0.57 ;v loss: 2.89537 ;v acc: 0.62\n",
      "Epoch 6;t loss: 2.93046 ;t acc: 0.64 ;v loss: 2.84290 ;v acc: 0.67\n",
      "Epoch 7;t loss: 2.87945 ;t acc: 0.69 ;v loss: 2.79903 ;v acc: 0.70\n",
      "Epoch 8;t loss: 2.81635 ;t acc: 0.75 ;v loss: 2.73755 ;v acc: 0.77\n",
      "Epoch 9;t loss: 2.77724 ;t acc: 0.77 ;v loss: 2.72939 ;v acc: 0.77\n",
      "Epoch 10;t loss: 2.76003 ;t acc: 0.78 ;v loss: 2.72539 ;v acc: 0.77\n",
      "Epoch 11;t loss: 2.75216 ;t acc: 0.78 ;v loss: 2.71803 ;v acc: 0.77\n",
      "Epoch 12;t loss: 2.73311 ;t acc: 0.81 ;v loss: 2.69530 ;v acc: 0.80\n",
      "Epoch 13;t loss: 2.71993 ;t acc: 0.81 ;v loss: 2.69294 ;v acc: 0.80\n",
      "Epoch 14;t loss: 2.71375 ;t acc: 0.81 ;v loss: 2.69250 ;v acc: 0.80\n",
      "Epoch 15;t loss: 2.71122 ;t acc: 0.81 ;v loss: 2.69061 ;v acc: 0.81\n",
      "Epoch 16;t loss: 2.70899 ;t acc: 0.82 ;v loss: 2.66941 ;v acc: 0.84\n",
      "Epoch 17;t loss: 2.68745 ;t acc: 0.84 ;v loss: 2.66296 ;v acc: 0.84\n",
      "Epoch 18;t loss: 2.68077 ;t acc: 0.85 ;v loss: 2.66370 ;v acc: 0.84\n",
      "Epoch 19;t loss: 2.66484 ;t acc: 0.87 ;v loss: 2.63844 ;v acc: 0.87\n",
      "Epoch 20;t loss: 2.65254 ;t acc: 0.88 ;v loss: 2.63413 ;v acc: 0.87\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27  5] [27  3]\n",
      "[26 27] [0 3]\n",
      "[1 5] [1 5]\n",
      "[ 4 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[27  4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[26 12] [0 7]\n",
      "[20 24] [20 29]\n",
      "[6 2] [6 2]\n",
      "[23 24] [29 24]\n",
      "[27 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 26] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[ 5 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[ 5 12] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 26] [9 0]\n",
      "[12  9] [12  9]\n",
      "[26 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 31] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 23] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[24  9] [29  9]\n",
      "[24 30] [ 7 30]\n",
      "[27 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 24] [19 29]\n",
      "[31 23] [29 23]\n",
      "[ 4 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 19] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17 27] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 26] [8 0]\n",
      "[27  2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n",
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 26] [21  0]\n",
      "Test loss: 2.6737924602296617 ; Test acc: 0.8444444537162781\n",
      "Epoch 21;t loss: 2.64966 ;t acc: 0.88 ;v loss: 2.63590 ;v acc: 0.87\n",
      "Epoch 22;t loss: 2.63637 ;t acc: 0.89 ;v loss: 2.61893 ;v acc: 0.89\n",
      "Epoch 23;t loss: 2.62143 ;t acc: 0.91 ;v loss: 2.61573 ;v acc: 0.89\n",
      "Epoch 24;t loss: 2.61540 ;t acc: 0.91 ;v loss: 2.61569 ;v acc: 0.89\n",
      "Epoch 25;t loss: 2.61741 ;t acc: 0.91 ;v loss: 2.61553 ;v acc: 0.89\n",
      "Epoch 26;t loss: 2.61838 ;t acc: 0.91 ;v loss: 2.61818 ;v acc: 0.89\n",
      "Epoch 27;t loss: 2.61351 ;t acc: 0.91 ;v loss: 2.61508 ;v acc: 0.89\n",
      "Epoch 28;t loss: 2.61189 ;t acc: 0.91 ;v loss: 2.61530 ;v acc: 0.89\n",
      "Epoch 29;t loss: 2.61029 ;t acc: 0.91 ;v loss: 2.61482 ;v acc: 0.89\n",
      "Epoch 30;t loss: 2.60991 ;t acc: 0.91 ;v loss: 2.61426 ;v acc: 0.89\n",
      "Epoch 31;t loss: 2.61132 ;t acc: 0.91 ;v loss: 2.61440 ;v acc: 0.89\n",
      "Epoch 32;t loss: 2.60951 ;t acc: 0.91 ;v loss: 2.61435 ;v acc: 0.89\n",
      "Epoch 33;t loss: 2.60993 ;t acc: 0.91 ;v loss: 2.61447 ;v acc: 0.89\n",
      "Epoch 34;t loss: 2.60942 ;t acc: 0.91 ;v loss: 2.61405 ;v acc: 0.89\n",
      "Epoch 35;t loss: 2.61098 ;t acc: 0.91 ;v loss: 2.61504 ;v acc: 0.89\n",
      "Epoch 36;t loss: 2.61082 ;t acc: 0.91 ;v loss: 2.61454 ;v acc: 0.89\n",
      "Epoch 37;t loss: 2.60905 ;t acc: 0.91 ;v loss: 2.61172 ;v acc: 0.90\n",
      "Epoch 38;t loss: 2.60900 ;t acc: 0.91 ;v loss: 2.61092 ;v acc: 0.90\n",
      "Epoch 39;t loss: 2.60836 ;t acc: 0.91 ;v loss: 2.61288 ;v acc: 0.89\n",
      "Epoch 40;t loss: 2.61037 ;t acc: 0.91 ;v loss: 2.61377 ;v acc: 0.89\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27 17] [27  3]\n",
      "[12 27] [0 3]\n",
      "[1 5] [1 5]\n",
      "[12 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[27  4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[12  8] [0 7]\n",
      "[20 29] [20 29]\n",
      "[6 2] [6 2]\n",
      "[29 24] [29 24]\n",
      "[27 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 26] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[ 5 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[ 5 26] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 26] [9 0]\n",
      "[12  9] [12  9]\n",
      "[26 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 29] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 29] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[29  9] [29  9]\n",
      "[12 30] [ 7 30]\n",
      "[27 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 29] [19 29]\n",
      "[29 23] [29 23]\n",
      "[ 4 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 19] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17 27] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 26] [8 0]\n",
      "[27  2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n",
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 26] [21  0]\n",
      "Test loss: 2.6330786519580416 ; Test acc: 0.8833333253860474\n",
      "Epoch 41;t loss: 2.61051 ;t acc: 0.91 ;v loss: 2.61412 ;v acc: 0.89\n",
      "Epoch 42;t loss: 2.60999 ;t acc: 0.91 ;v loss: 2.61303 ;v acc: 0.89\n",
      "Epoch 43;t loss: 2.60725 ;t acc: 0.91 ;v loss: 2.58027 ;v acc: 0.93\n",
      "Epoch 44;t loss: 2.58203 ;t acc: 0.94 ;v loss: 2.57873 ;v acc: 0.93\n",
      "Epoch 45;t loss: 2.58153 ;t acc: 0.94 ;v loss: 2.57835 ;v acc: 0.93\n",
      "Epoch 46;t loss: 2.57790 ;t acc: 0.94 ;v loss: 2.57768 ;v acc: 0.93\n",
      "Epoch 47;t loss: 2.57944 ;t acc: 0.94 ;v loss: 2.57855 ;v acc: 0.93\n",
      "Epoch 48;t loss: 2.57841 ;t acc: 0.94 ;v loss: 2.57636 ;v acc: 0.94\n",
      "Epoch 49;t loss: 2.57743 ;t acc: 0.94 ;v loss: 2.57880 ;v acc: 0.93\n",
      "Epoch 50;t loss: 2.57823 ;t acc: 0.94 ;v loss: 2.57625 ;v acc: 0.94\n",
      "Epoch 51;t loss: 2.57893 ;t acc: 0.94 ;v loss: 2.57973 ;v acc: 0.93\n",
      "Epoch 52;t loss: 2.57835 ;t acc: 0.94 ;v loss: 2.57209 ;v acc: 0.94\n",
      "Epoch 53;t loss: 2.57663 ;t acc: 0.94 ;v loss: 2.57723 ;v acc: 0.93\n",
      "Epoch 54;t loss: 2.57736 ;t acc: 0.94 ;v loss: 2.57662 ;v acc: 0.94\n",
      "Epoch 55;t loss: 2.57647 ;t acc: 0.94 ;v loss: 2.57844 ;v acc: 0.93\n",
      "Epoch 56;t loss: 2.57695 ;t acc: 0.94 ;v loss: 2.57586 ;v acc: 0.94\n",
      "Epoch 57;t loss: 2.57625 ;t acc: 0.94 ;v loss: 2.58010 ;v acc: 0.93\n",
      "Epoch 58;t loss: 2.57814 ;t acc: 0.94 ;v loss: 2.57991 ;v acc: 0.93\n",
      "Epoch 59;t loss: 2.57711 ;t acc: 0.94 ;v loss: 2.57985 ;v acc: 0.93\n",
      "Epoch 60;t loss: 2.57640 ;t acc: 0.94 ;v loss: 2.57945 ;v acc: 0.93\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27 17] [27  3]\n",
      "[12 27] [0 3]\n",
      "[1 5] [1 5]\n",
      "[ 7 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[27  4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[11  7] [0 7]\n",
      "[20 29] [20 29]\n",
      "[6 2] [6 2]\n",
      "[29 24] [29 24]\n",
      "[27 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 26] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[17 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[5 7] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 26] [9 0]\n",
      "[12  9] [12  9]\n",
      "[12 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 29] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 29] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[29  9] [29  9]\n",
      "[ 7 30] [ 7 30]\n",
      "[27 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 29] [19 29]\n",
      "[29 23] [29 23]\n",
      "[ 7 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 19] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17 27] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 26] [8 0]\n",
      "[17  2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n",
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 26] [21  0]\n",
      "Test loss: 2.605627491739061 ; Test acc: 0.9111111164093018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61;t loss: 2.57563 ;t acc: 0.94 ;v loss: 2.57872 ;v acc: 0.93\n",
      "Epoch 62;t loss: 2.57637 ;t acc: 0.94 ;v loss: 2.57847 ;v acc: 0.93\n",
      "Epoch 63;t loss: 2.57672 ;t acc: 0.94 ;v loss: 2.57475 ;v acc: 0.94\n",
      "Epoch 64;t loss: 2.57852 ;t acc: 0.94 ;v loss: 2.57538 ;v acc: 0.94\n",
      "Epoch 65;t loss: 2.57661 ;t acc: 0.94 ;v loss: 2.57742 ;v acc: 0.93\n",
      "Epoch 66;t loss: 2.57646 ;t acc: 0.94 ;v loss: 2.57873 ;v acc: 0.93\n",
      "Epoch 67;t loss: 2.57691 ;t acc: 0.94 ;v loss: 2.57985 ;v acc: 0.93\n",
      "Epoch 68;t loss: 2.57628 ;t acc: 0.94 ;v loss: 2.57704 ;v acc: 0.93\n",
      "Epoch 69;t loss: 2.57764 ;t acc: 0.94 ;v loss: 2.57918 ;v acc: 0.93\n",
      "Epoch 70;t loss: 2.57608 ;t acc: 0.94 ;v loss: 2.57835 ;v acc: 0.93\n",
      "Epoch 71;t loss: 2.57982 ;t acc: 0.94 ;v loss: 2.57841 ;v acc: 0.93\n",
      "Epoch 72;t loss: 2.57863 ;t acc: 0.94 ;v loss: 2.58008 ;v acc: 0.93\n",
      "Epoch 73;t loss: 2.57572 ;t acc: 0.94 ;v loss: 2.57978 ;v acc: 0.93\n",
      "Epoch 74;t loss: 2.57630 ;t acc: 0.94 ;v loss: 2.58018 ;v acc: 0.93\n",
      "Epoch 75;t loss: 2.57564 ;t acc: 0.94 ;v loss: 2.57940 ;v acc: 0.93\n",
      "Epoch 76;t loss: 2.57539 ;t acc: 0.94 ;v loss: 2.57827 ;v acc: 0.93\n",
      "Epoch 77;t loss: 2.57553 ;t acc: 0.94 ;v loss: 2.57819 ;v acc: 0.93\n",
      "Epoch 78;t loss: 2.57634 ;t acc: 0.94 ;v loss: 2.57552 ;v acc: 0.94\n",
      "Epoch 79;t loss: 2.57535 ;t acc: 0.94 ;v loss: 2.57883 ;v acc: 0.93\n",
      "Epoch 80;t loss: 2.57645 ;t acc: 0.94 ;v loss: 2.57897 ;v acc: 0.93\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27 27] [27  3]\n",
      "[24 27] [0 3]\n",
      "[1 5] [1 5]\n",
      "[ 7 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[21  4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[26  7] [0 7]\n",
      "[20 29] [20 29]\n",
      "[6 2] [6 2]\n",
      "[29 24] [29 24]\n",
      "[27 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 24] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[21 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[5 7] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 26] [9 0]\n",
      "[12  9] [12  9]\n",
      "[15 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 29] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 29] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[29  9] [29  9]\n",
      "[ 7 30] [ 7 30]\n",
      "[27 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 29] [19 29]\n",
      "[29 23] [29 23]\n",
      "[ 7 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 15] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17 27] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 21] [8 0]\n",
      "[27  2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n",
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 26] [21  0]\n",
      "Test loss: 2.6056468698713515 ; Test acc: 0.9111111164093018\n",
      "Epoch 81;t loss: 2.57638 ;t acc: 0.94 ;v loss: 2.57846 ;v acc: 0.93\n",
      "Epoch 82;t loss: 2.57621 ;t acc: 0.94 ;v loss: 2.57839 ;v acc: 0.93\n",
      "Epoch 83;t loss: 2.57521 ;t acc: 0.94 ;v loss: 2.57876 ;v acc: 0.93\n",
      "Epoch 84;t loss: 2.57607 ;t acc: 0.94 ;v loss: 2.57909 ;v acc: 0.93\n",
      "Epoch 85;t loss: 2.57642 ;t acc: 0.94 ;v loss: 2.57832 ;v acc: 0.93\n",
      "Epoch 86;t loss: 2.57546 ;t acc: 0.94 ;v loss: 2.57846 ;v acc: 0.93\n",
      "Epoch 87;t loss: 2.57648 ;t acc: 0.94 ;v loss: 2.57712 ;v acc: 0.93\n",
      "Epoch 88;t loss: 2.57533 ;t acc: 0.94 ;v loss: 2.57774 ;v acc: 0.93\n",
      "Epoch 89;t loss: 2.57622 ;t acc: 0.94 ;v loss: 2.57905 ;v acc: 0.93\n",
      "Epoch 90;t loss: 2.57557 ;t acc: 0.94 ;v loss: 2.57759 ;v acc: 0.93\n",
      "Epoch 91;t loss: 2.57626 ;t acc: 0.94 ;v loss: 2.57842 ;v acc: 0.93\n",
      "Epoch 92;t loss: 2.57641 ;t acc: 0.94 ;v loss: 2.57898 ;v acc: 0.93\n",
      "Epoch 93;t loss: 2.57537 ;t acc: 0.94 ;v loss: 2.57921 ;v acc: 0.93\n",
      "Epoch 94;t loss: 2.57655 ;t acc: 0.94 ;v loss: 2.57154 ;v acc: 0.94\n",
      "Epoch 95;t loss: 2.57715 ;t acc: 0.94 ;v loss: 2.57903 ;v acc: 0.93\n",
      "Epoch 96;t loss: 2.57563 ;t acc: 0.94 ;v loss: 2.57909 ;v acc: 0.93\n",
      "Epoch 97;t loss: 2.57613 ;t acc: 0.94 ;v loss: 2.57953 ;v acc: 0.93\n",
      "Epoch 98;t loss: 2.57597 ;t acc: 0.94 ;v loss: 2.57969 ;v acc: 0.93\n",
      "Epoch 99;t loss: 2.57527 ;t acc: 0.94 ;v loss: 2.57944 ;v acc: 0.93\n",
      "Epoch 100;t loss: 2.57599 ;t acc: 0.94 ;v loss: 2.57448 ;v acc: 0.94\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27 17] [27  3]\n",
      "[12  5] [0 3]\n",
      "[1 5] [1 5]\n",
      "[ 7 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[21  4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[11  7] [0 7]\n",
      "[20 29] [20 29]\n",
      "[6 2] [6 2]\n",
      "[29 24] [29 24]\n",
      "[18 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 12] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[ 5 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[5 7] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 12] [9 0]\n",
      "[12  9] [12  9]\n",
      "[12 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 29] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 29] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[29  9] [29  9]\n",
      "[ 7 30] [ 7 30]\n",
      "[27 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 29] [19 29]\n",
      "[29 23] [29 23]\n",
      "[ 7 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 12] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17 27] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 12] [8 0]\n",
      "[17  2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n",
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 11] [21  0]\n",
      "Test loss: 2.6056610027949016 ; Test acc: 0.9111111164093018\n",
      "Epoch 101;t loss: 2.57614 ;t acc: 0.94 ;v loss: 2.57848 ;v acc: 0.93\n",
      "Epoch 102;t loss: 2.57524 ;t acc: 0.94 ;v loss: 2.57814 ;v acc: 0.93\n",
      "Epoch 103;t loss: 2.57545 ;t acc: 0.94 ;v loss: 2.57970 ;v acc: 0.93\n",
      "Epoch 104;t loss: 2.57530 ;t acc: 0.94 ;v loss: 2.57966 ;v acc: 0.93\n",
      "Epoch 105;t loss: 2.57621 ;t acc: 0.94 ;v loss: 2.57968 ;v acc: 0.93\n",
      "Epoch 106;t loss: 2.57776 ;t acc: 0.94 ;v loss: 2.57969 ;v acc: 0.93\n",
      "Epoch 107;t loss: 2.57756 ;t acc: 0.94 ;v loss: 2.57989 ;v acc: 0.93\n",
      "Epoch 108;t loss: 2.57540 ;t acc: 0.94 ;v loss: 2.57970 ;v acc: 0.93\n",
      "Epoch 109;t loss: 2.57523 ;t acc: 0.94 ;v loss: 2.57957 ;v acc: 0.93\n",
      "Epoch 110;t loss: 2.57606 ;t acc: 0.94 ;v loss: 2.57940 ;v acc: 0.93\n",
      "Epoch 111;t loss: 2.57647 ;t acc: 0.94 ;v loss: 2.57968 ;v acc: 0.93\n",
      "Epoch 112;t loss: 2.57615 ;t acc: 0.94 ;v loss: 2.57969 ;v acc: 0.93\n",
      "Epoch 113;t loss: 2.57502 ;t acc: 0.94 ;v loss: 2.57939 ;v acc: 0.93\n",
      "Epoch 114;t loss: 2.57639 ;t acc: 0.94 ;v loss: 2.57960 ;v acc: 0.93\n",
      "Epoch 115;t loss: 2.57510 ;t acc: 0.94 ;v loss: 2.57950 ;v acc: 0.93\n",
      "Epoch 116;t loss: 2.57794 ;t acc: 0.94 ;v loss: 2.57703 ;v acc: 0.94\n",
      "Epoch 117;t loss: 2.57638 ;t acc: 0.94 ;v loss: 2.58006 ;v acc: 0.93\n",
      "Epoch 118;t loss: 2.57609 ;t acc: 0.94 ;v loss: 2.57961 ;v acc: 0.93\n",
      "Epoch 119;t loss: 2.57615 ;t acc: 0.94 ;v loss: 2.57960 ;v acc: 0.93\n",
      "Epoch 120;t loss: 2.57515 ;t acc: 0.94 ;v loss: 2.57922 ;v acc: 0.93\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27 17] [27  3]\n",
      "[12 17] [0 3]\n",
      "[1 5] [1 5]\n",
      "[ 7 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[21  4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[12  7] [0 7]\n",
      "[20 29] [20 29]\n",
      "[6 2] [6 2]\n",
      "[29 24] [29 24]\n",
      "[27 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 24] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[17 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[5 7] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 12] [9 0]\n",
      "[12  9] [12  9]\n",
      "[15 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 29] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 29] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[29  9] [29  9]\n",
      "[ 7 30] [ 7 30]\n",
      "[27 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 29] [19 29]\n",
      "[29 23] [29 23]\n",
      "[ 7 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 15] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17 27] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 21] [8 0]\n",
      "[17  2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 12] [21  0]\n",
      "Test loss: 2.6056704945034452 ; Test acc: 0.9111111164093018\n",
      "Epoch 121;t loss: 2.57636 ;t acc: 0.94 ;v loss: 2.57768 ;v acc: 0.93\n",
      "Epoch 122;t loss: 2.57630 ;t acc: 0.94 ;v loss: 2.57877 ;v acc: 0.93\n",
      "Epoch 123;t loss: 2.57511 ;t acc: 0.94 ;v loss: 2.57873 ;v acc: 0.93\n",
      "Epoch 124;t loss: 2.57510 ;t acc: 0.94 ;v loss: 2.57825 ;v acc: 0.93\n",
      "Epoch 125;t loss: 2.57611 ;t acc: 0.94 ;v loss: 2.57716 ;v acc: 0.93\n",
      "Epoch 126;t loss: 2.57509 ;t acc: 0.94 ;v loss: 2.57439 ;v acc: 0.94\n",
      "Epoch 127;t loss: 2.57707 ;t acc: 0.94 ;v loss: 2.57571 ;v acc: 0.94\n",
      "Epoch 128;t loss: 2.57503 ;t acc: 0.94 ;v loss: 2.57524 ;v acc: 0.94\n",
      "Epoch 129;t loss: 2.57497 ;t acc: 0.94 ;v loss: 2.57689 ;v acc: 0.93\n",
      "Epoch 130;t loss: 2.57598 ;t acc: 0.94 ;v loss: 2.57657 ;v acc: 0.93\n",
      "Epoch 131;t loss: 2.57499 ;t acc: 0.94 ;v loss: 2.57664 ;v acc: 0.93\n",
      "Epoch 132;t loss: 2.57505 ;t acc: 0.94 ;v loss: 2.57104 ;v acc: 0.94\n",
      "Epoch 133;t loss: 2.57609 ;t acc: 0.94 ;v loss: 2.57185 ;v acc: 0.94\n",
      "Epoch 134;t loss: 2.57497 ;t acc: 0.94 ;v loss: 2.57186 ;v acc: 0.94\n",
      "Epoch 135;t loss: 2.57502 ;t acc: 0.94 ;v loss: 2.57527 ;v acc: 0.94\n",
      "Epoch 136;t loss: 2.57504 ;t acc: 0.94 ;v loss: 2.57362 ;v acc: 0.94\n",
      "Epoch 137;t loss: 2.57492 ;t acc: 0.94 ;v loss: 2.57293 ;v acc: 0.94\n",
      "Epoch 138;t loss: 2.57773 ;t acc: 0.94 ;v loss: 2.57437 ;v acc: 0.94\n",
      "Epoch 139;t loss: 2.57792 ;t acc: 0.94 ;v loss: 2.57183 ;v acc: 0.94\n",
      "Epoch 140;t loss: 2.57582 ;t acc: 0.94 ;v loss: 2.57416 ;v acc: 0.94\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27 17] [27  3]\n",
      "[12 17] [0 3]\n",
      "[1 5] [1 5]\n",
      "[ 7 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[21  4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[11  7] [0 7]\n",
      "[20 29] [20 29]\n",
      "[6 2] [6 2]\n",
      "[29 24] [29 24]\n",
      "[18 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 12] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[17 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[5 7] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 12] [9 0]\n",
      "[12  9] [12  9]\n",
      "[12 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 29] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 29] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[29  9] [29  9]\n",
      "[ 7 30] [ 7 30]\n",
      "[27 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 29] [19 29]\n",
      "[29 23] [29 23]\n",
      "[ 7 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 11] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17 27] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 12] [8 0]\n",
      "[17  2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n",
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 12] [21  0]\n",
      "Test loss: 2.6058257791731094 ; Test acc: 0.9111111164093018\n",
      "Epoch 141;t loss: 2.56915 ;t acc: 0.95 ;v loss: 2.54286 ;v acc: 0.97\n",
      "Epoch 142;t loss: 2.55251 ;t acc: 0.97 ;v loss: 2.53128 ;v acc: 0.99\n",
      "Epoch 143;t loss: 2.55039 ;t acc: 0.97 ;v loss: 2.52832 ;v acc: 0.99\n",
      "Epoch 144;t loss: 2.54908 ;t acc: 0.97 ;v loss: 2.52795 ;v acc: 0.99\n",
      "Epoch 145;t loss: 2.54961 ;t acc: 0.97 ;v loss: 2.53587 ;v acc: 0.98\n",
      "Epoch 146;t loss: 2.55170 ;t acc: 0.97 ;v loss: 2.53481 ;v acc: 0.98\n",
      "Epoch 147;t loss: 2.54988 ;t acc: 0.97 ;v loss: 2.53455 ;v acc: 0.98\n",
      "Epoch 148;t loss: 2.54968 ;t acc: 0.97 ;v loss: 2.53278 ;v acc: 0.99\n",
      "Epoch 149;t loss: 2.54879 ;t acc: 0.97 ;v loss: 2.53107 ;v acc: 0.99\n",
      "Epoch 150;t loss: 2.54889 ;t acc: 0.97 ;v loss: 2.52845 ;v acc: 0.99\n",
      "Epoch 151;t loss: 2.54961 ;t acc: 0.97 ;v loss: 2.52987 ;v acc: 0.99\n",
      "Epoch 152;t loss: 2.54905 ;t acc: 0.97 ;v loss: 2.52930 ;v acc: 0.99\n",
      "Epoch 153;t loss: 2.55065 ;t acc: 0.97 ;v loss: 2.53141 ;v acc: 0.99\n",
      "Epoch 154;t loss: 2.54871 ;t acc: 0.97 ;v loss: 2.52831 ;v acc: 0.99\n",
      "Epoch 155;t loss: 2.54973 ;t acc: 0.97 ;v loss: 2.53130 ;v acc: 0.99\n",
      "Epoch 156;t loss: 2.54948 ;t acc: 0.97 ;v loss: 2.53208 ;v acc: 0.99\n",
      "Epoch 157;t loss: 2.54943 ;t acc: 0.97 ;v loss: 2.53231 ;v acc: 0.99\n",
      "Epoch 158;t loss: 2.55008 ;t acc: 0.97 ;v loss: 2.52778 ;v acc: 0.99\n",
      "Epoch 159;t loss: 2.54884 ;t acc: 0.97 ;v loss: 2.52812 ;v acc: 0.99\n",
      "Epoch 160;t loss: 2.54870 ;t acc: 0.97 ;v loss: 2.53148 ;v acc: 0.99\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27  3] [27  3]\n",
      "[24  3] [0 3]\n",
      "[1 5] [1 5]\n",
      "[ 7 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[3 4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[11  7] [0 7]\n",
      "[20 29] [20 29]\n",
      "[6 2] [6 2]\n",
      "[29 24] [29 24]\n",
      "[ 3 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 24] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[ 3 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[5 7] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 26] [9 0]\n",
      "[12  9] [12  9]\n",
      "[24 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 29] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 29] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[29  9] [29  9]\n",
      "[ 7 30] [ 7 30]\n",
      "[ 3 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 29] [19 29]\n",
      "[29 23] [29 23]\n",
      "[ 7 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 11] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17  3] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 26] [8 0]\n",
      "[3 2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n",
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 11] [21  0]\n",
      "Test loss: 2.5619635608461166 ; Test acc: 0.9555555582046509\n",
      "Epoch 161;t loss: 2.54987 ;t acc: 0.97 ;v loss: 2.52828 ;v acc: 0.99\n",
      "Epoch 162;t loss: 2.54962 ;t acc: 0.97 ;v loss: 2.53282 ;v acc: 0.99\n",
      "Epoch 163;t loss: 2.54869 ;t acc: 0.97 ;v loss: 2.53295 ;v acc: 0.98\n",
      "Epoch 164;t loss: 2.54860 ;t acc: 0.97 ;v loss: 2.53552 ;v acc: 0.98\n",
      "Epoch 165;t loss: 2.54882 ;t acc: 0.97 ;v loss: 2.52933 ;v acc: 0.99\n",
      "Epoch 166;t loss: 2.54924 ;t acc: 0.97 ;v loss: 2.54387 ;v acc: 0.97\n",
      "Epoch 167;t loss: 2.54898 ;t acc: 0.97 ;v loss: 2.53513 ;v acc: 0.98\n",
      "Epoch 168;t loss: 2.54860 ;t acc: 0.97 ;v loss: 2.53631 ;v acc: 0.98\n",
      "Epoch 169;t loss: 2.54867 ;t acc: 0.97 ;v loss: 2.53866 ;v acc: 0.98\n",
      "Epoch 170;t loss: 2.54891 ;t acc: 0.97 ;v loss: 2.52768 ;v acc: 0.99\n",
      "Epoch 171;t loss: 2.54872 ;t acc: 0.97 ;v loss: 2.52935 ;v acc: 0.99\n",
      "Epoch 172;t loss: 2.54864 ;t acc: 0.97 ;v loss: 2.52705 ;v acc: 0.99\n",
      "Epoch 173;t loss: 2.54963 ;t acc: 0.97 ;v loss: 2.52731 ;v acc: 0.99\n",
      "Epoch 174;t loss: 2.54863 ;t acc: 0.97 ;v loss: 2.53086 ;v acc: 0.99\n",
      "Epoch 175;t loss: 2.54847 ;t acc: 0.97 ;v loss: 2.52986 ;v acc: 0.99\n",
      "Epoch 176;t loss: 2.54915 ;t acc: 0.97 ;v loss: 2.52692 ;v acc: 0.99\n",
      "Epoch 177;t loss: 2.54854 ;t acc: 0.97 ;v loss: 2.52706 ;v acc: 0.99\n",
      "Epoch 178;t loss: 2.54853 ;t acc: 0.97 ;v loss: 2.52719 ;v acc: 0.99\n",
      "Epoch 179;t loss: 2.54856 ;t acc: 0.97 ;v loss: 2.52738 ;v acc: 0.99\n",
      "Epoch 180;t loss: 2.54851 ;t acc: 0.97 ;v loss: 2.52749 ;v acc: 0.99\n",
      "Predictions - Ground truth\n",
      "[ 1 27] [ 1 27]\n",
      "[31 14] [31 14]\n",
      "[25  2] [25  2]\n",
      "[27  3] [27  3]\n",
      "[26  3] [0 3]\n",
      "[1 5] [1 5]\n",
      "[ 7 21] [ 7 21]\n",
      "[12 20] [12 20]\n",
      "[23 16] [23 16]\n",
      "[19 14] [19 14]\n",
      "[31 19] [31 19]\n",
      "[3 4] [3 4]\n",
      "[13 31] [13 31]\n",
      "[23 27] [23 27]\n",
      "[ 4 27] [ 4 27]\n",
      "[ 6 14] [ 6 14]\n",
      "[24 14] [24 14]\n",
      "[26  7] [0 7]\n",
      "[20 29] [20 29]\n",
      "[6 2] [6 2]\n",
      "[29 24] [29 24]\n",
      "[ 3 12] [ 3 12]\n",
      "[ 5 26] [ 5 26]\n",
      "[13 27] [13 27]\n",
      "[ 1 10] [ 1 10]\n",
      "[20  5] [20  5]\n",
      "[10 11] [10 11]\n",
      "[ 6 26] [6 0]\n",
      "[20  6] [20  6]\n",
      "[13 22] [13 22]\n",
      "[24 24] [24 24]\n",
      "[ 8 26] [ 8 26]\n",
      "[ 3 31] [ 3 31]\n",
      "[ 2 24] [ 2 24]\n",
      "[19  9] [19  9]\n",
      "[28 12] [28 12]\n",
      "[ 2 10] [ 2 10]\n",
      "[5 7] [5 7]\n",
      "[16  1] [16  1]\n",
      "[15 17] [15 17]\n",
      "[ 9 26] [9 0]\n",
      "[12  9] [12  9]\n",
      "[15 17] [ 0 17]\n",
      "[25  5] [25  5]\n",
      "[26 18] [26 18]\n",
      "[22 29] [22 29]\n",
      "[10 31] [10 31]\n",
      "[11 21] [11 21]\n",
      "[16 29] [16 29]\n",
      "[24 25] [24 25]\n",
      "[27  6] [27  6]\n",
      "[24 15] [24 15]\n",
      "[13  4] [13  4]\n",
      "[29  9] [29  9]\n",
      "[ 7 30] [ 7 30]\n",
      "[ 3 19] [ 3 19]\n",
      "[17 16] [17 16]\n",
      "[9 5] [9 5]\n",
      "[21  9] [21  9]\n",
      "[19 29] [19 29]\n",
      "[29 23] [29 23]\n",
      "[ 7 28] [ 7 28]\n",
      "[18 15] [18 15]\n",
      "[12 15] [12 15]\n",
      "[23 15] [23 15]\n",
      "[18 13] [18 13]\n",
      "[28  1] [28  1]\n",
      "[25 11] [25 11]\n",
      "[17  1] [17  1]\n",
      "[11 28] [11 28]\n",
      "[14 11] [14  0]\n",
      "[19 25] [19 25]\n",
      "[17  3] [17  3]\n",
      "[15  6] [15  6]\n",
      "[21 12] [21 12]\n",
      "[22 17] [22 17]\n",
      "[21 28] [21 28]\n",
      "[ 8 26] [8 0]\n",
      "[3 2] [3 2]\n",
      "[24 26] [24 26]\n",
      "[ 5 20] [ 5 20]\n",
      "[ 8 10] [ 8 10]\n",
      "[12 17] [12 17]\n",
      "[10 20] [10 20]\n",
      "[23 20] [23 20]\n",
      "[14  2] [14  2]\n",
      "[ 9 16] [ 9 16]\n",
      "[20 26] [20 26]\n",
      "[ 8 31] [ 8 31]\n",
      "[21 26] [21  0]\n",
      "Test loss: 2.561894671122233 ; Test acc: 0.9555555582046509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181;t loss: 2.54852 ;t acc: 0.97 ;v loss: 2.52859 ;v acc: 0.99\n",
      "Epoch 182;t loss: 2.54858 ;t acc: 0.97 ;v loss: 2.52949 ;v acc: 0.99\n",
      "Epoch 183;t loss: 2.54867 ;t acc: 0.97 ;v loss: 2.53413 ;v acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "model = Model(train_dataset[0].x.shape[1],train_dataset[0].x.shape[0]//32, n_classes=n_classes).to(device)  \n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameter count: {pytorch_total_params}')\n",
    "\n",
    "# model = model.to(devic)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=.1, rho=0.9, eps=1e-06, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=1e-2, weight_decay=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3, weight_decay=0)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-5)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "# optimizer = torch.optim.Adagrad(model.parameters(), lr=5e-4, lr_decay=0, weight_decay=0)\n",
    "\n",
    "# Instantiate optimizer\n",
    "# scheduler = StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "def train(loader, target = 0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch)\n",
    "        loss = criterion(out,batch.y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        pred = torch.argmax(out,-1)\n",
    "        right += torch.sum((pred == batch.y).detach().cpu())\n",
    "        tot += batch.y.shape[0]\n",
    "        \n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "def test(loader,verbose=False, target = 0):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    if verbose:\n",
    "        print('Predictions - Ground truth')\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        # y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        pred = torch.argmax(out,-1)\n",
    "        if verbose:\n",
    "            print(pred.detach().cpu().numpy(),batch.y.detach().cpu().numpy())\n",
    "        loss = criterion(out,batch.y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        right += torch.sum((pred == batch.y).detach().cpu())\n",
    "        tot += batch.y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "best_val_loss = np.inf\n",
    "esp = 0\n",
    "MAX_ESP = 50\n",
    "\n",
    "BS = 16\n",
    "\n",
    "target = 0 # Valence-Arousal-Dominance-Liking\n",
    "\n",
    "splt_idx = 1000\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [splt_idx, len(train_dataset)-splt_idx])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BS, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BS)\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(1, 10000):    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Training and validation\n",
    "    train_loss, train_acc = train(train_loader, target = target)\n",
    "    val_loss, val_acc = test(val_loader , target = target)\n",
    "    print(f'Epoch {epoch};t loss: {train_loss:.5f} ;t acc: {train_acc:.2f} ;v loss: {val_loss:.5f} ;v acc: {val_acc:.2f}')\n",
    "\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/test', val_acc, epoch)\n",
    "    # Early stopping and checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        esp = 0\n",
    "        torch.save(model.state_dict(),'./best_params') \n",
    "    else:\n",
    "        esp += 1\n",
    "        if esp >= MAX_ESP:\n",
    "            break\n",
    "            \n",
    "    if epoch % 20 == 0:\n",
    "        test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "        loss, acc = test(test_loader, True)\n",
    "        print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./best_params'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "loss, acc = test(train_loader, False,target=target)\n",
    "print(f'Train loss: {loss} ; Train acc: {acc}')\n",
    "loss, acc = test(val_loader, False,target=target)\n",
    "\n",
    "print(f'Val loss: {loss} ; Val acc: {acc}')\n",
    "loss, acc = test(test_loader, True,target=target)\n",
    "print(f'Test loss: {loss} ; Test acc: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
