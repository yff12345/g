{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skimage\n",
    "import pywt\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from einops import reduce, rearrange, repeat\n",
    "from npeet import entropy_estimators as ee\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from scipy.fft import rfft, rfftfreq, ifft\n",
    "from einops import rearrange\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from Electrodes import Electrodes\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDatasetEEGFeatures(InMemoryDataset):\n",
    "  def __init__(self, root, raw_dir, processed_dir, feature='de', transform=None, pre_transform=None,include_edge_attr = True, undirected_graphs = True, add_global_connections=True, participant_from=1, participant_to=32, n_videos=40):\n",
    "      self._raw_dir = raw_dir\n",
    "      self._processed_dir = processed_dir\n",
    "      self.participant_from = participant_from\n",
    "      self.participant_to = participant_to\n",
    "      self.n_videos = n_videos\n",
    "      self.feature = feature\n",
    "      # Whether or not to include edge_attr in the dataset\n",
    "      self.include_edge_attr = include_edge_attr\n",
    "      # If true there will be 1024 links as opposed to 528\n",
    "      self.undirected_graphs = undirected_graphs\n",
    "      # Instantiate class to handle electrode positions\n",
    "      print('Using global connections' if add_global_connections else 'Not using global connections')\n",
    "      self.electrodes = Electrodes(add_global_connections, expand_3d = False)\n",
    "      super(DEAPDatasetEEGFeatures, self).__init__(root, transform, pre_transform)\n",
    "      self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "      \n",
    "  @property\n",
    "  def raw_dir(self):\n",
    "      return f'{self.root}/{self._raw_dir}'\n",
    "\n",
    "  @property\n",
    "  def processed_dir(self):\n",
    "      return f'{self.root}/{self._processed_dir}'\n",
    "\n",
    "  @property\n",
    "  def raw_file_names(self):\n",
    "      raw_names = [f for f in os.listdir(self.raw_dir)]\n",
    "      raw_names.sort()\n",
    "      return raw_names\n",
    "\n",
    "  @property\n",
    "  def processed_file_names(self):\n",
    "      if not os.path.exists(self.processed_dir):\n",
    "        os.makedirs(self.processed_dir)\n",
    "      file_name = f'{self.participant_from}-{self.participant_to}' if self.participant_from is not self.participant_to else f'{self.participant_from}'\n",
    "      return [f'deap_processed_graph.{file_name}_{self.feature}.dataset']\n",
    "\n",
    "  def process(self):\n",
    "        # Number of nodes per graph\n",
    "        n_nodes = len(self.electrodes.positions_3d)\n",
    "        \n",
    "\n",
    "        if self.undirected_graphs:\n",
    "            source_nodes, target_nodes = np.repeat(np.arange(0,n_nodes),n_nodes), np.tile(np.arange(0,n_nodes),n_nodes)\n",
    "        else:\n",
    "            source_nodes, target_nodes = np.tril_indices(n_nodes,n_nodes)\n",
    "        \n",
    "        edge_attr = self.electrodes.adjacency_matrix[source_nodes,target_nodes]\n",
    "        \n",
    "        # Remove zero weight links\n",
    "        mask = np.ma.masked_not_equal(edge_attr, 0).mask\n",
    "        edge_attr,source_nodes,target_nodes = edge_attr[mask], source_nodes[mask], target_nodes[mask]\n",
    "\n",
    "        edge_attr, edge_index = torch.FloatTensor(edge_attr), torch.tensor([source_nodes,target_nodes], dtype=torch.long)\n",
    "        \n",
    "        # Expand edge_index and edge_attr to match windows\n",
    "        e_edge_index = edge_index.clone()\n",
    "        e_edge_attr = edge_attr.clone()\n",
    "        number_of_graphs = 4\n",
    "        for i in range(number_of_graphs-1):\n",
    "            a = edge_index + e_edge_index.max() + 1\n",
    "            e_edge_index = torch.cat([e_edge_index,a],dim=1)\n",
    "            e_edge_attr = torch.cat([e_edge_attr,edge_attr],dim=0)\n",
    "\n",
    "        print(f'Number of graphs per video: {number_of_graphs}')\n",
    "        # List of graphs that will be written to file\n",
    "        data_list = []\n",
    "        pbar = tqdm(range(self.participant_from,self.participant_to+1))\n",
    "        for participant_id in pbar:\n",
    "            raw_name = [e for e in self.raw_file_names if str(participant_id).zfill(2) in e][0]\n",
    "            pbar.set_description(raw_name)\n",
    "            # Load raw file as np array\n",
    "            participant_data = scipy.io.loadmat(f'{self.raw_dir}/{raw_name}')\n",
    "            signal_data = torch.FloatTensor(remove_baseline_mean(participant_data['data'][:,:32,:]))\n",
    "#             signal_data = torch.FloatTensor()\n",
    "            processed = []\n",
    "            for i, video in enumerate(signal_data[:self.n_videos,:,:]):\n",
    "                if self.feature == 'wav':\n",
    "                    node_features = process_video_wavelet(video)\n",
    "                else:\n",
    "                    node_features = process_video(video, feature=self.feature)\n",
    "                data = Data(x=torch.FloatTensor(node_features),edge_attr=e_edge_attr,edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]])) if self.include_edge_attr else Data(x=torch.FloatTensor(node_features), edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]]))\n",
    "                data_list.append(data) \n",
    "               \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_de(window):\n",
    "    return ee.entropy(window.reshape(-1,1), k=2)\n",
    "# Input: Video with shape (32,7680)\n",
    "# Output: Graph node features with shape (5*32, 59) -> 5 graphs with 32 nodes each with 59 features each\n",
    "def process_video(video, feature='psd'):\n",
    "    # Transform to frequency domain\n",
    "    fft_vals = np.fft.rfft(video, axis=-1)\n",
    "     # Get frequencies for amplitudes in Hz\n",
    "    samplingFrequency = 128\n",
    "    fft_freq = np.fft.rfftfreq(video.shape[-1], 1.0/samplingFrequency)\n",
    "    # Delta, Theta, Alpha, Beta, Gamma\n",
    "    bands = [(0,4),(4,8),(8,12),(12,30),(30,45)]\n",
    "    \n",
    "    band_mask = np.array([np.logical_or(fft_freq < f, fft_freq > t) for f,t in bands])\n",
    "    band_mask = repeat(band_mask,'a b -> a c b', c=32)\n",
    "    band_data = np.array(fft_vals)\n",
    "    band_data = repeat(band_data,'a b -> c a b', c=5)\n",
    "     \n",
    "    band_data[band_mask] = 0\n",
    "    \n",
    "    band_data = np.fft.irfft(band_data)\n",
    "\n",
    "    windows = skimage.util.view_as_windows(band_data, (5,32,128), step=128).squeeze()\n",
    "    # (5, 32, 60, 128)\n",
    "    windows = rearrange(windows, 'a b c d -> b c a d')\n",
    "    \n",
    "    if feature == 'psd':\n",
    "        features = scipy.signal.periodogram(windows)[1]\n",
    "        features = np.mean(features, axis=-1)\n",
    "    elif feature == 'de':\n",
    "        features = np.apply_along_axis(calculate_de, -1, windows)\n",
    "\n",
    "    \n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_video(signal_data):\n",
    "    electrodes = Electrodes(expand_3d=False)\n",
    "    fig, axs = plt.subplots(32, sharex=True, figsize=(20,50))\n",
    "    fig.tight_layout()\n",
    "    video = signal_data[0]\n",
    "    for i in range(32):\n",
    "        c = [float(i)/float(32), 0.0, float(32-i)/float(32)] #R,G,B\n",
    "        axs[i].set_title(f'{electrodes.channel_names[i]}', loc='left', fontsize=20)\n",
    "        axs[i].plot(video[i],color=c)\n",
    "        axs[i].spines['top'].set_visible(False)\n",
    "        axs[i].spines['right'].set_visible(False)\n",
    "        axs[i].spines['left'].set_visible(False)\n",
    "        axs[i].spines['bottom'].set_visible(False)\n",
    "    plt.savefig('eeg.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baseline_mean(signal_data):\n",
    "    # Take first three senconds of data\n",
    "    signal_baseline = np.array(signal_data[:,:,:128*3]).reshape(40,32,128,-1)\n",
    "    # Mean of three senconds of baseline will be deducted from all windows\n",
    "    signal_noise = np.mean(signal_baseline,axis=-1)\n",
    "    # Expand mask\n",
    "    signal_noise = repeat(signal_noise,'a b c -> a b (d c)',d=60)\n",
    "    return signal_data[:,:,128*3:] - signal_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_wavelet(video, feature='energy', time_domain=False):\n",
    "    band_widths = [32,16,8,4]\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            # Highest frequencies (64-128Hz) are not used\n",
    "            cA, cD = pywt.dwt(video.numpy(), 'db4')\n",
    "        else:\n",
    "            cA, cD = pywt.dwt(cA, 'db4')\n",
    "            \n",
    "            cA_windows = skimage.util.view_as_windows(cA, (32,band_widths[i-1]*2), step=band_widths[i-1]).squeeze()\n",
    "            cA_windows = np.transpose(cA_windows[:59,:,:],(1,0,2))\n",
    "            if feature == 'energy':\n",
    "                cA_windows = np.square(cA_windows)\n",
    "                cA_windows = np.sum(cA_windows, axis=-1)\n",
    "                features.append(cA_windows)\n",
    "                \n",
    "    if time_domain:\n",
    "        features = np.transpose(features,(2,1,0))\n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "    \n",
    "    # Normalization\n",
    "    m = features.mean(0, keepdim=True)\n",
    "    s = features.std(0, unbiased=False, keepdim=True)\n",
    "    features -= m\n",
    "    features /= s\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global connections\n"
     ]
    }
   ],
   "source": [
    "# Constants used to define data paths\n",
    "ROOT_DIR = './'\n",
    "RAW_DIR = 'data/matlabPREPROCESSED'\n",
    "PROCESSED_DIR = 'data/graphProcessedData'\n",
    "\n",
    "dataset = DEAPDatasetEEGFeatures(root= ROOT_DIR, raw_dir= RAW_DIR, processed_dir= PROCESSED_DIR, feature='wav')\n",
    "# dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2128,  1.6996,  0.9838,  ...,  1.1742,  0.9820,  0.8368],\n",
       "        [ 1.4243,  1.9620,  1.3170,  ...,  1.1621,  0.9964,  1.0650],\n",
       "        [ 1.9055,  2.8486,  1.8405,  ...,  1.5020,  0.9923,  0.7060],\n",
       "        ...,\n",
       "        [-0.1919, -0.5199, -0.7240,  ..., -0.6899, -0.6883, -0.7674],\n",
       "        [-0.2252, -0.5211, -0.7225,  ..., -0.6893, -0.6871, -0.7652],\n",
       "        [-0.6994, -0.7078, -0.7248,  ..., -0.6892, -0.6866, -0.7649]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, 40)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 880 used for training, 220 validation and 180 testing\n",
    "test_participant = 1\n",
    "# \n",
    "# splt_idx = 35\n",
    "\n",
    "# 85% used for train/val\n",
    "train_dataset = dataset[0:test_participant*40] + dataset[test_participant*40+40:]\n",
    "test_dataset = dataset[test_participant*40:test_participant*40+40]\n",
    "\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dataset[0:30]\n",
    "# test_dataset = dataset[30:]\n",
    "\n",
    "# len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCN2Conv, GCNConv, global_max_pool as gmp\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=128, cnn_hidden_dim = 256):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        self.gconv1 = GCNConv(in_channels,32)\n",
    "        self.gconv2 = GCNConv(32,hidden_channels)\n",
    "        \n",
    "#         self.gconv3 = GCNConv(in_channels,hidden_channels)\n",
    "        \n",
    "        # self.rnn = torch.nn.GRU(hidden_channels, rnn_hidden_dim, 2,dropout=0.2, batch_first=True)\n",
    "        self.cnn1 = torch.nn.Conv1d(4, 1, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(hidden_channels, 1)\n",
    "#         self.lin2 = torch.nn.Linear(cnn_hidden_dim, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        bs = len(torch.unique(batch.batch))\n",
    "        x, edge_index, edge_attr = batch.x, batch.edge_index, batch.edge_attr\n",
    "        \n",
    "        x = self.gconv1(x, edge_index, edge_attr)\n",
    "        x = self.gconv2(x, edge_index, edge_attr)\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = x.relu()\n",
    "        x = rearrange(x, '(bs g e) f -> (bs e) g f', bs=bs, e=32)\n",
    "        x = self.cnn1(x).squeeze()\n",
    "        x = x.tanh()\n",
    "        x = rearrange(x, '(bs e) f -> bs e f', bs=bs)\n",
    "        x = torch.sum(x, dim=1)\n",
    "        \n",
    "        \n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        x = x.view(-1)\n",
    "        \n",
    "        return x.sigmoid()\n",
    "\n",
    "    \n",
    "#         x = rearrange(x,' (bs g e) f -> (bs g) e f', bs=bs, e=32)\n",
    "    \n",
    "#         print(x.shape)\n",
    "        \n",
    "#         x = torch.max(x, dim=1).values\n",
    "#         print(x.shape)\n",
    "\n",
    "#         raise 'err'\n",
    "#         \n",
    "        \n",
    "#         x = self.gconv3(x, edge_index, edge_attr )\n",
    "#         print('-0----------------------------------------------------')\n",
    "#         print(x)\n",
    "#         x = x.relu()\n",
    "#         print(x)\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # x = rearrange(x, '(bs a b) c -> (bs b) a c', bs=bs, b=32, c=self.hidden_channels)\n",
    "        # o, (h_n,c_n) = self.rnn(x)\n",
    "\n",
    "#         x = x.relu()\n",
    "#         x = self.cnn2(x)\n",
    "#         x = x.relu()\n",
    "#         print(x.shape)\n",
    "#         \n",
    "#         x = rearrange(x, 'bs a b -> bs (a b)', bs=bs)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         x = x.relu()\n",
    "#         x = self.lin2(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count: 6278\n",
      "Epoch 1;t loss: 1.40868 ;t acc: 0.49 ;v loss: 0.69540 ;v acc: 0.49\n",
      "Epoch 2;t loss: 1.25579 ;t acc: 0.51 ;v loss: 0.68473 ;v acc: 0.53\n",
      "Epoch 3;t loss: 1.17093 ;t acc: 0.49 ;v loss: 0.70040 ;v acc: 0.51\n",
      "Epoch 4;t loss: 1.11212 ;t acc: 0.51 ;v loss: 0.66948 ;v acc: 0.61\n",
      "Epoch 5;t loss: 1.02900 ;t acc: 0.52 ;v loss: 0.67098 ;v acc: 0.61\n",
      "Epoch 6;t loss: 1.04691 ;t acc: 0.51 ;v loss: 0.69757 ;v acc: 0.54\n",
      "Epoch 7;t loss: 1.01817 ;t acc: 0.50 ;v loss: 0.67169 ;v acc: 0.59\n",
      "Epoch 8;t loss: 0.93280 ;t acc: 0.52 ;v loss: 0.66974 ;v acc: 0.60\n",
      "Epoch 9;t loss: 0.92708 ;t acc: 0.52 ;v loss: 0.67604 ;v acc: 0.55\n",
      "Epoch 10;t loss: 0.92786 ;t acc: 0.52 ;v loss: 0.67154 ;v acc: 0.56\n",
      "Epoch 11;t loss: 0.88986 ;t acc: 0.53 ;v loss: 0.66349 ;v acc: 0.64\n",
      "Epoch 12;t loss: 0.90551 ;t acc: 0.51 ;v loss: 0.66405 ;v acc: 0.64\n",
      "Epoch 13;t loss: 0.87144 ;t acc: 0.51 ;v loss: 0.66313 ;v acc: 0.62\n",
      "Epoch 14;t loss: 0.85535 ;t acc: 0.52 ;v loss: 0.67156 ;v acc: 0.56\n",
      "Epoch 15;t loss: 0.86312 ;t acc: 0.53 ;v loss: 0.66693 ;v acc: 0.61\n",
      "Epoch 16;t loss: 0.85784 ;t acc: 0.53 ;v loss: 0.66695 ;v acc: 0.60\n",
      "Epoch 17;t loss: 0.87517 ;t acc: 0.51 ;v loss: 0.66770 ;v acc: 0.57\n",
      "Epoch 18;t loss: 0.87605 ;t acc: 0.49 ;v loss: 0.66281 ;v acc: 0.62\n",
      "Epoch 19;t loss: 0.84000 ;t acc: 0.54 ;v loss: 0.66412 ;v acc: 0.64\n",
      "Epoch 20;t loss: 0.83395 ;t acc: 0.50 ;v loss: 0.66166 ;v acc: 0.62\n",
      "tensor([0.6643], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4760], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6087], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5288], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6254], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5579], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5100], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5890], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4901], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6231], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4980], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5527], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6260], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6128], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6111], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5729], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5743], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5686], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5207], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6191], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6129], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6036], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6832], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5480], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6428], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6236], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5632], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5874], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4252], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5396], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6180], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5399], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5732], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6546], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5211], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5196], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5787], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5323], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6466], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6129], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.695805712044239 ; Test acc: 0.5\n",
      "Epoch 21;t loss: 0.84553 ;t acc: 0.52 ;v loss: 0.66406 ;v acc: 0.64\n",
      "Epoch 22;t loss: 0.85095 ;t acc: 0.49 ;v loss: 0.66456 ;v acc: 0.64\n",
      "Epoch 23;t loss: 0.85121 ;t acc: 0.49 ;v loss: 0.66673 ;v acc: 0.61\n",
      "Epoch 24;t loss: 0.84591 ;t acc: 0.52 ;v loss: 0.66842 ;v acc: 0.59\n",
      "Epoch 25;t loss: 0.84330 ;t acc: 0.52 ;v loss: 0.66662 ;v acc: 0.60\n",
      "Epoch 26;t loss: 0.82355 ;t acc: 0.52 ;v loss: 0.66582 ;v acc: 0.62\n",
      "Epoch 27;t loss: 0.80103 ;t acc: 0.52 ;v loss: 0.66674 ;v acc: 0.60\n",
      "Epoch 28;t loss: 0.80630 ;t acc: 0.51 ;v loss: 0.66537 ;v acc: 0.62\n",
      "Epoch 29;t loss: 0.83039 ;t acc: 0.50 ;v loss: 0.66495 ;v acc: 0.64\n",
      "Epoch 30;t loss: 0.80739 ;t acc: 0.52 ;v loss: 0.66622 ;v acc: 0.60\n",
      "Epoch 31;t loss: 0.78604 ;t acc: 0.54 ;v loss: 0.66501 ;v acc: 0.64\n",
      "Epoch 32;t loss: 0.81581 ;t acc: 0.51 ;v loss: 0.66485 ;v acc: 0.64\n",
      "Epoch 33;t loss: 0.80421 ;t acc: 0.54 ;v loss: 0.66495 ;v acc: 0.64\n",
      "Epoch 34;t loss: 0.79548 ;t acc: 0.53 ;v loss: 0.66358 ;v acc: 0.65\n",
      "Epoch 35;t loss: 0.79192 ;t acc: 0.53 ;v loss: 0.66659 ;v acc: 0.59\n",
      "Epoch 36;t loss: 0.81452 ;t acc: 0.51 ;v loss: 0.66477 ;v acc: 0.64\n",
      "Epoch 37;t loss: 0.80140 ;t acc: 0.52 ;v loss: 0.66527 ;v acc: 0.60\n",
      "Epoch 38;t loss: 0.80537 ;t acc: 0.52 ;v loss: 0.66357 ;v acc: 0.64\n",
      "Epoch 39;t loss: 0.79300 ;t acc: 0.53 ;v loss: 0.66338 ;v acc: 0.64\n",
      "Epoch 40;t loss: 0.80016 ;t acc: 0.52 ;v loss: 0.66464 ;v acc: 0.64\n",
      "tensor([0.6385], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4460], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5771], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5063], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6027], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5380], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4877], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5655], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4683], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5955], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4818], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5370], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5873], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5806], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5817], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5563], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5456], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5457], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5036], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5964], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5828], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5767], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6515], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5216], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6104], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6014], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5387], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5574], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4063], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5173], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5882], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5179], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5396], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6359], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4946], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4901], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5506], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5190], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6118], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5893], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6930557921528816 ; Test acc: 0.525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41;t loss: 0.78645 ;t acc: 0.52 ;v loss: 0.66417 ;v acc: 0.65\n",
      "Epoch 42;t loss: 0.80529 ;t acc: 0.52 ;v loss: 0.66298 ;v acc: 0.62\n",
      "Epoch 43;t loss: 0.81049 ;t acc: 0.49 ;v loss: 0.66579 ;v acc: 0.61\n",
      "Epoch 44;t loss: 0.80803 ;t acc: 0.50 ;v loss: 0.66609 ;v acc: 0.61\n",
      "Epoch 45;t loss: 0.81415 ;t acc: 0.50 ;v loss: 0.66657 ;v acc: 0.60\n",
      "Epoch 46;t loss: 0.79180 ;t acc: 0.51 ;v loss: 0.66563 ;v acc: 0.64\n",
      "Epoch 47;t loss: 0.80283 ;t acc: 0.51 ;v loss: 0.66735 ;v acc: 0.60\n",
      "Epoch 48;t loss: 0.80233 ;t acc: 0.51 ;v loss: 0.66757 ;v acc: 0.60\n",
      "Epoch 49;t loss: 0.82613 ;t acc: 0.50 ;v loss: 0.66808 ;v acc: 0.60\n",
      "Epoch 50;t loss: 0.76213 ;t acc: 0.54 ;v loss: 0.66872 ;v acc: 0.60\n",
      "Epoch 51;t loss: 0.81986 ;t acc: 0.49 ;v loss: 0.66777 ;v acc: 0.61\n",
      "Epoch 52;t loss: 0.79481 ;t acc: 0.52 ;v loss: 0.66697 ;v acc: 0.62\n",
      "Epoch 53;t loss: 0.77675 ;t acc: 0.50 ;v loss: 0.66620 ;v acc: 0.65\n",
      "Epoch 54;t loss: 0.81131 ;t acc: 0.50 ;v loss: 0.66581 ;v acc: 0.64\n",
      "Epoch 55;t loss: 0.79351 ;t acc: 0.50 ;v loss: 0.66432 ;v acc: 0.62\n",
      "Epoch 56;t loss: 0.77421 ;t acc: 0.53 ;v loss: 0.66483 ;v acc: 0.62\n",
      "Epoch 57;t loss: 0.79062 ;t acc: 0.52 ;v loss: 0.66670 ;v acc: 0.65\n",
      "Epoch 58;t loss: 0.79492 ;t acc: 0.52 ;v loss: 0.66893 ;v acc: 0.60\n",
      "Epoch 59;t loss: 0.78081 ;t acc: 0.52 ;v loss: 0.66831 ;v acc: 0.60\n",
      "Epoch 60;t loss: 0.79084 ;t acc: 0.53 ;v loss: 0.66653 ;v acc: 0.64\n",
      "tensor([0.6358], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4458], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5681], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5072], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6035], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5437], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4873], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5635], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4754], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5943], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4888], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5459], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5812], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5760], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5786], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5627], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5438], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5480], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5099], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6023], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5759], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5777], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6437], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5267], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6100], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6058], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5426], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5551], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4101], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5218], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5832], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5187], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5400], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6435], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4947], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4854], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5511], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5291], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6036], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5857], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6922624558210373 ; Test acc: 0.525\n",
      "Epoch 61;t loss: 0.77449 ;t acc: 0.52 ;v loss: 0.66733 ;v acc: 0.64\n",
      "Epoch 62;t loss: 0.78998 ;t acc: 0.48 ;v loss: 0.66728 ;v acc: 0.64\n",
      "Epoch 63;t loss: 0.78141 ;t acc: 0.52 ;v loss: 0.66769 ;v acc: 0.62\n",
      "Epoch 64;t loss: 0.80540 ;t acc: 0.48 ;v loss: 0.66807 ;v acc: 0.61\n",
      "Epoch 65;t loss: 0.77291 ;t acc: 0.54 ;v loss: 0.66756 ;v acc: 0.62\n",
      "Epoch 66;t loss: 0.75938 ;t acc: 0.54 ;v loss: 0.66918 ;v acc: 0.60\n",
      "Epoch 67;t loss: 0.78298 ;t acc: 0.51 ;v loss: 0.66827 ;v acc: 0.62\n",
      "Epoch 68;t loss: 0.78521 ;t acc: 0.50 ;v loss: 0.66715 ;v acc: 0.65\n",
      "Epoch 69;t loss: 0.76844 ;t acc: 0.52 ;v loss: 0.66739 ;v acc: 0.64\n",
      "Epoch 70;t loss: 0.79514 ;t acc: 0.51 ;v loss: 0.66976 ;v acc: 0.60\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "model = Model(train_dataset[0].x.shape[1]).to(device)  \n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameter count: {pytorch_total_params}')\n",
    "\n",
    "# model = model.to(device)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=.1, rho=0.9, eps=1e-06, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=1e-5, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=1e-4, weight_decay=1e-2)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-5)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-3, lr_decay=1e-4, weight_decay=1e-3)\n",
    "\n",
    "# Instantiate optimizer\n",
    "# scheduler = StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "def train(loader, target = 0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        y = (batch.y[:,target] > 5).float()\n",
    "#         y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        loss = criterion(out,y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        right += torch.eq(out > .5, y > .5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "        \n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "def test(loader,verbose=False, target = 0):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        y = (batch.y[:,target] > 5).float()\n",
    "#         y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        if verbose:\n",
    "            print(out,y)\n",
    "        loss = criterion(out,y)\n",
    "        losses.append(loss.item())\n",
    "        right += torch.eq(out > .5, y > .5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "best_val_loss = np.inf\n",
    "esp = 0\n",
    "MAX_ESP = 50\n",
    "\n",
    "BS = 8\n",
    "\n",
    "target = 0 # Valence-Arousal-Dominance-Liking\n",
    "\n",
    "splt_idx = 1160\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [splt_idx, len(train_dataset)-splt_idx])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BS, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BS)\n",
    "for epoch in range(1, 10000):    \n",
    "\n",
    "    # Training and validation\n",
    "    train_loss, train_acc = train(train_loader, target = target)\n",
    "    val_loss, val_acc = test(val_loader , target = target)\n",
    "    print(f'Epoch {epoch};t loss: {train_loss:.5f} ;t acc: {train_acc:.2f} ;v loss: {val_loss:.5f} ;v acc: {val_acc:.2f}')\n",
    "\n",
    "    # Early stopping and checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        esp = 0\n",
    "        torch.save(model.state_dict(),'./best_params') \n",
    "    else:\n",
    "        esp += 1\n",
    "        if esp >= MAX_ESP:\n",
    "            break\n",
    "            \n",
    "    if epoch % 20 == 0:\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "        loss, acc = test(test_loader, True)\n",
    "        print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6873049662030977 ; Train acc: 0.5594827586206896\n",
      "Val loss: 0.6616587221622467 ; Val acc: 0.625\n",
      "tensor([0.6643], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4760], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6087], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5288], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6254], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5579], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5100], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5890], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4901], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6231], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4980], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5527], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6260], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6128], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6111], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5729], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5743], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5686], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5207], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6191], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6129], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6036], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6832], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5480], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6428], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6236], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5632], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5874], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4252], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5396], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6180], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5399], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5732], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6546], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5211], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5196], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5787], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5323], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6466], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6129], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6958057075738907 ; Test acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_params'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "loss, acc = test(train_loader, False,target=target)\n",
    "print(f'Train loss: {loss} ; Train acc: {acc}')\n",
    "loss, acc = test(val_loader, False,target=target)\n",
    "\n",
    "print(f'Val loss: {loss} ; Val acc: {acc}')\n",
    "loss, acc = test(test_loader, True,target=target)\n",
    "print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "# TODO: scheduler(?) Loss/acc records"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
