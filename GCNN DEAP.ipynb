{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skimage\n",
    "import pywt\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from einops import reduce, rearrange, repeat\n",
    "from npeet import entropy_estimators as ee\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from scipy.fft import rfft, rfftfreq, ifft\n",
    "from einops import rearrange\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from Electrodes import Electrodes\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDatasetEEGFeatures(InMemoryDataset):\n",
    "  def __init__(self, root, raw_dir, processed_dir, feature='de', transform=None, pre_transform=None,include_edge_attr = True, undirected_graphs = True, add_global_connections=True, participant_from=1, participant_to=32, n_videos=40):\n",
    "      self._raw_dir = raw_dir\n",
    "      self._processed_dir = processed_dir\n",
    "      self.participant_from = participant_from\n",
    "      self.participant_to = participant_to\n",
    "      self.n_videos = n_videos\n",
    "      self.feature = feature\n",
    "      # Whether or not to include edge_attr in the dataset\n",
    "      self.include_edge_attr = include_edge_attr\n",
    "      # If true there will be 1024 links as opposed to 528\n",
    "      self.undirected_graphs = undirected_graphs\n",
    "      # Instantiate class to handle electrode positions\n",
    "      print('Using global connections' if add_global_connections else 'Not using global connections')\n",
    "      self.electrodes = Electrodes(add_global_connections, expand_3d = False)\n",
    "      super(DEAPDatasetEEGFeatures, self).__init__(root, transform, pre_transform)\n",
    "      self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "      \n",
    "  @property\n",
    "  def raw_dir(self):\n",
    "      return f'{self.root}/{self._raw_dir}'\n",
    "\n",
    "  @property\n",
    "  def processed_dir(self):\n",
    "      return f'{self.root}/{self._processed_dir}'\n",
    "\n",
    "  @property\n",
    "  def raw_file_names(self):\n",
    "      raw_names = [f for f in os.listdir(self.raw_dir)]\n",
    "      raw_names.sort()\n",
    "      return raw_names\n",
    "\n",
    "  @property\n",
    "  def processed_file_names(self):\n",
    "      if not os.path.exists(self.processed_dir):\n",
    "        os.makedirs(self.processed_dir)\n",
    "      file_name = f'{self.participant_from}-{self.participant_to}' if self.participant_from is not self.participant_to else f'{self.participant_from}'\n",
    "      return [f'deap_processed_graph.{file_name}_{self.feature}.dataset']\n",
    "\n",
    "  def process(self):\n",
    "        # Number of nodes per graph\n",
    "        n_nodes = len(self.electrodes.positions_3d)\n",
    "        \n",
    "\n",
    "        if self.undirected_graphs:\n",
    "            source_nodes, target_nodes = np.repeat(np.arange(0,n_nodes),n_nodes), np.tile(np.arange(0,n_nodes),n_nodes)\n",
    "        else:\n",
    "            source_nodes, target_nodes = np.tril_indices(n_nodes,n_nodes)\n",
    "        \n",
    "        edge_attr = self.electrodes.adjacency_matrix[source_nodes,target_nodes]\n",
    "        \n",
    "        # Remove zero weight links\n",
    "        mask = np.ma.masked_not_equal(edge_attr, 0).mask\n",
    "        edge_attr,source_nodes,target_nodes = edge_attr[mask], source_nodes[mask], target_nodes[mask]\n",
    "\n",
    "        edge_attr, edge_index = torch.FloatTensor(edge_attr), torch.tensor([source_nodes,target_nodes], dtype=torch.long)\n",
    "        \n",
    "        # Expand edge_index and edge_attr to match windows\n",
    "        e_edge_index = edge_index.clone()\n",
    "        e_edge_attr = edge_attr.clone()\n",
    "        number_of_graphs = 4\n",
    "        for i in range(number_of_graphs-1):\n",
    "            a = edge_index + e_edge_index.max() + 1\n",
    "            e_edge_index = torch.cat([e_edge_index,a],dim=1)\n",
    "            e_edge_attr = torch.cat([e_edge_attr,edge_attr],dim=0)\n",
    "\n",
    "        print(f'Number of graphs per video: {number_of_graphs}')\n",
    "        # List of graphs that will be written to file\n",
    "        data_list = []\n",
    "        pbar = tqdm(range(self.participant_from,self.participant_to+1))\n",
    "        for participant_id in pbar:\n",
    "            raw_name = [e for e in self.raw_file_names if str(participant_id).zfill(2) in e][0]\n",
    "            pbar.set_description(raw_name)\n",
    "            # Load raw file as np array\n",
    "            participant_data = scipy.io.loadmat(f'{self.raw_dir}/{raw_name}')\n",
    "            signal_data = torch.FloatTensor(remove_baseline_mean(participant_data['data'][:,:32,:]))\n",
    "#             signal_data = torch.FloatTensor()\n",
    "            processed = []\n",
    "            for i, video in enumerate(signal_data[:self.n_videos,:,:]):\n",
    "                if self.feature == 'wav':\n",
    "                    node_features = process_video_wavelet(video)\n",
    "                else:\n",
    "                    node_features = process_video(video, feature=self.feature)\n",
    "                data = Data(x=torch.FloatTensor(node_features),edge_attr=e_edge_attr,edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]])) if self.include_edge_attr else Data(x=torch.FloatTensor(node_features), edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]]))\n",
    "                data_list.append(data) \n",
    "               \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_de(window):\n",
    "    return ee.entropy(window.reshape(-1,1), k=2)\n",
    "# Input: Video with shape (32,7680)\n",
    "# Output: Graph node features with shape (5*32, 59) -> 5 graphs with 32 nodes each with 59 features each\n",
    "def process_video(video, feature='psd'):\n",
    "    # Transform to frequency domain\n",
    "    fft_vals = np.fft.rfft(video, axis=-1)\n",
    "     # Get frequencies for amplitudes in Hz\n",
    "    samplingFrequency = 128\n",
    "    fft_freq = np.fft.rfftfreq(video.shape[-1], 1.0/samplingFrequency)\n",
    "    # Delta, Theta, Alpha, Beta, Gamma\n",
    "    bands = [(0,4),(4,8),(8,12),(12,30),(30,45)]\n",
    "    \n",
    "    band_mask = np.array([np.logical_or(fft_freq < f, fft_freq > t) for f,t in bands])\n",
    "    band_mask = repeat(band_mask,'a b -> a c b', c=32)\n",
    "    band_data = np.array(fft_vals)\n",
    "    band_data = repeat(band_data,'a b -> c a b', c=5)\n",
    "     \n",
    "    band_data[band_mask] = 0\n",
    "    \n",
    "    band_data = np.fft.irfft(band_data)\n",
    "\n",
    "    windows = skimage.util.view_as_windows(band_data, (5,32,128), step=128).squeeze()\n",
    "    # (5, 32, 60, 128)\n",
    "    windows = rearrange(windows, 'a b c d -> b c a d')\n",
    "    \n",
    "    if feature == 'psd':\n",
    "        features = scipy.signal.periodogram(windows)[1]\n",
    "        features = np.mean(features, axis=-1)\n",
    "    elif feature == 'de':\n",
    "        features = np.apply_along_axis(calculate_de, -1, windows)\n",
    "\n",
    "    \n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_video(signal_data):\n",
    "    electrodes = Electrodes(expand_3d=False)\n",
    "    fig, axs = plt.subplots(32, sharex=True, figsize=(20,50))\n",
    "    fig.tight_layout()\n",
    "    video = signal_data[0]\n",
    "    for i in range(32):\n",
    "        c = [float(i)/float(32), 0.0, float(32-i)/float(32)] #R,G,B\n",
    "        axs[i].set_title(f'{electrodes.channel_names[i]}', loc='left', fontsize=20)\n",
    "        axs[i].plot(video[i],color=c)\n",
    "        axs[i].spines['top'].set_visible(False)\n",
    "        axs[i].spines['right'].set_visible(False)\n",
    "        axs[i].spines['left'].set_visible(False)\n",
    "        axs[i].spines['bottom'].set_visible(False)\n",
    "    plt.savefig('eeg.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baseline_mean(signal_data):\n",
    "    # Take first three senconds of data\n",
    "    signal_baseline = np.array(signal_data[:,:,:128*3]).reshape(40,32,128,-1)\n",
    "    # Mean of three senconds of baseline will be deducted from all windows\n",
    "    signal_noise = np.mean(signal_baseline,axis=-1)\n",
    "    # Expand mask\n",
    "    signal_noise = repeat(signal_noise,'a b c -> a b (d c)',d=60)\n",
    "    return signal_data[:,:,128*3:] - signal_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_wavelet(video, feature='energy', time_domain=False):\n",
    "    band_widths = [32,16,8,4]\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            # Highest frequencies (64-128Hz) are not used\n",
    "            cA, cD = pywt.dwt(video.numpy(), 'db4')\n",
    "        else:\n",
    "            cA, cD = pywt.dwt(cA, 'db4')\n",
    "            cA_windows = skimage.util.view_as_windows(cA, (32,band_widths[i-1]*2), step=band_widths[i-1]).squeeze()\n",
    "            cA_windows = np.transpose(cA_windows[:59,:,:],(1,0,2))\n",
    "            if feature == 'energy':\n",
    "                cA_windows = np.square(cA_windows)\n",
    "                cA_windows = np.sum(cA_windows, axis=-1)\n",
    "                features.append(cA_windows)\n",
    "                \n",
    "    if time_domain:\n",
    "        features = np.transpose(features,(2,1,0))\n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global connections\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[776], edge_index=[2, 776], x=[128, 59], y=[1, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants used to define data paths\n",
    "ROOT_DIR = './'\n",
    "RAW_DIR = 'data/matlabPREPROCESSED'\n",
    "PROCESSED_DIR = 'data/graphProcessedData'\n",
    "\n",
    "dataset = DEAPDatasetEEGFeatures(root= ROOT_DIR, raw_dir= RAW_DIR, processed_dir= PROCESSED_DIR, feature='wav')\n",
    "# dataset = dataset.shuffle()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 880 used for training, 220 validation and 180 testing\n",
    "test_participant = 0\n",
    "# \n",
    "# splt_idx = 35\n",
    "\n",
    "# 85% used for train/val\n",
    "train_dataset = dataset[0:test_participant*40] + dataset[test_participant*40+40:]\n",
    "test_dataset = dataset[test_participant*40:test_participant*40+40]\n",
    "\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCN2Conv, GCNConv\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=256, cnn_hidden_dim = 128):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        self.gconv1 = GCN2Conv(in_channels,1)\n",
    "        self.gconv2 = GCNConv(in_channels,hidden_channels)\n",
    "        \n",
    "#         self.gconv3 = GCNConv(in_channels,hidden_channels)\n",
    "        \n",
    "        # self.rnn = torch.nn.GRU(hidden_channels, rnn_hidden_dim, 2,dropout=0.2, batch_first=True)\n",
    "        self.cnn1 = torch.nn.Conv1d(4*hidden_channels, hidden_channels, kernel_size=1, stride=1)\n",
    "        self.cnn2 = torch.nn.Conv1d(hidden_channels, cnn_hidden_dim, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(32*cnn_hidden_dim, 1)\n",
    "#         self.lin2 = torch.nn.Linear(cnn_hidden_dim, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        bs = len(torch.unique(batch.batch))\n",
    "        x_, edge_index, edge_attr = batch.x, batch.edge_index, batch.edge_attr\n",
    "#         print(x.shape)\n",
    "        x = self.gconv1(x_,x_, edge_index, edge_attr)\n",
    "#         x = x.relu()\n",
    "        x = self.gconv2(x, edge_index, edge_attr)\n",
    "        x = x.tanh()\n",
    "        \n",
    "#         x = self.gconv3(x, edge_index, edge_attr )\n",
    "#         print('-0----------------------------------------------------')\n",
    "#         print(x)\n",
    "#         x = x.relu()\n",
    "#         print(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # x = rearrange(x, '(bs a b) c -> (bs b) a c', bs=bs, b=32, c=self.hidden_channels)\n",
    "        # o, (h_n,c_n) = self.rnn(x)\n",
    "        x = rearrange(x, '(bs g e) f -> bs (g f) e', bs=bs, e=32)\n",
    "        x = self.cnn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.cnn2(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = rearrange(x, 'bs a b -> bs (a b)', bs=bs)\n",
    "        \n",
    "        \n",
    "        x = self.lin1(x)\n",
    "#         x = x.relu()\n",
    "#         x = self.lin2(x)\n",
    "        x = x.view(-1)\n",
    "        return x.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count: 318234\n",
      "Epoch 1;t loss: 0.69492 ;t acc: 0.54 ;v loss: 0.68118 ;v acc: 0.56\n",
      "Epoch 2;t loss: 0.68235 ;t acc: 0.57 ;v loss: 0.68105 ;v acc: 0.57\n",
      "Epoch 3;t loss: 0.67860 ;t acc: 0.56 ;v loss: 0.68000 ;v acc: 0.56\n",
      "Epoch 4;t loss: 0.66745 ;t acc: 0.59 ;v loss: 0.67667 ;v acc: 0.60\n",
      "Epoch 5;t loss: 0.66688 ;t acc: 0.60 ;v loss: 0.67681 ;v acc: 0.60\n",
      "Epoch 6;t loss: 0.66019 ;t acc: 0.60 ;v loss: 0.67681 ;v acc: 0.60\n",
      "Epoch 7;t loss: 0.65829 ;t acc: 0.59 ;v loss: 0.68103 ;v acc: 0.54\n",
      "Epoch 8;t loss: 0.65161 ;t acc: 0.63 ;v loss: 0.68109 ;v acc: 0.59\n",
      "Epoch 9;t loss: 0.64829 ;t acc: 0.63 ;v loss: 0.68228 ;v acc: 0.55\n",
      "Epoch 10;t loss: 0.64653 ;t acc: 0.64 ;v loss: 0.68295 ;v acc: 0.55\n",
      "Epoch 11;t loss: 0.63656 ;t acc: 0.65 ;v loss: 0.68335 ;v acc: 0.55\n",
      "Epoch 12;t loss: 0.63136 ;t acc: 0.65 ;v loss: 0.68875 ;v acc: 0.59\n",
      "Epoch 13;t loss: 0.62425 ;t acc: 0.67 ;v loss: 0.68712 ;v acc: 0.61\n",
      "Epoch 14;t loss: 0.61761 ;t acc: 0.66 ;v loss: 0.68266 ;v acc: 0.57\n",
      "Epoch 15;t loss: 0.61310 ;t acc: 0.68 ;v loss: 0.70101 ;v acc: 0.56\n",
      "Epoch 16;t loss: 0.60652 ;t acc: 0.68 ;v loss: 0.68855 ;v acc: 0.64\n",
      "Epoch 17;t loss: 0.60779 ;t acc: 0.68 ;v loss: 0.69298 ;v acc: 0.62\n",
      "Epoch 18;t loss: 0.59775 ;t acc: 0.68 ;v loss: 0.69057 ;v acc: 0.57\n",
      "Epoch 19;t loss: 0.59570 ;t acc: 0.69 ;v loss: 0.70277 ;v acc: 0.56\n",
      "Epoch 20;t loss: 0.58589 ;t acc: 0.70 ;v loss: 0.70043 ;v acc: 0.57\n",
      "tensor([0.5406], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5824], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5902], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5077], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5410], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4773], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5221], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5423], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6896], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3233], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4517], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4181], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4859], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2901], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3539], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3337], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6406], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6688], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4828], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5602], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4038], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5125], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4769], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4277], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3090], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5020], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2567], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3114], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4564], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7177], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4082], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4853], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3680], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3407], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5128], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5613], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4029], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4214], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4619], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3758], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6972517043352127 ; Test acc: 0.625\n",
      "Epoch 21;t loss: 0.58295 ;t acc: 0.70 ;v loss: 0.70872 ;v acc: 0.56\n",
      "Epoch 22;t loss: 0.57089 ;t acc: 0.72 ;v loss: 0.70611 ;v acc: 0.57\n",
      "Epoch 23;t loss: 0.56694 ;t acc: 0.73 ;v loss: 0.70686 ;v acc: 0.56\n",
      "Epoch 24;t loss: 0.57159 ;t acc: 0.71 ;v loss: 0.75250 ;v acc: 0.51\n",
      "Epoch 25;t loss: 0.55497 ;t acc: 0.73 ;v loss: 0.71089 ;v acc: 0.54\n",
      "Epoch 26;t loss: 0.54590 ;t acc: 0.74 ;v loss: 0.71652 ;v acc: 0.55\n",
      "Epoch 27;t loss: 0.54684 ;t acc: 0.74 ;v loss: 0.71775 ;v acc: 0.57\n",
      "Epoch 28;t loss: 0.53528 ;t acc: 0.75 ;v loss: 0.71414 ;v acc: 0.54\n",
      "Epoch 29;t loss: 0.53662 ;t acc: 0.75 ;v loss: 0.72079 ;v acc: 0.53\n",
      "Epoch 30;t loss: 0.53173 ;t acc: 0.74 ;v loss: 0.72493 ;v acc: 0.53\n",
      "Epoch 31;t loss: 0.52888 ;t acc: 0.75 ;v loss: 0.74740 ;v acc: 0.62\n",
      "Epoch 32;t loss: 0.51119 ;t acc: 0.77 ;v loss: 0.72748 ;v acc: 0.57\n",
      "Epoch 33;t loss: 0.51315 ;t acc: 0.77 ;v loss: 0.72789 ;v acc: 0.55\n",
      "Epoch 34;t loss: 0.51420 ;t acc: 0.76 ;v loss: 0.74705 ;v acc: 0.59\n",
      "Epoch 35;t loss: 0.49405 ;t acc: 0.79 ;v loss: 0.77975 ;v acc: 0.57\n",
      "Epoch 36;t loss: 0.49211 ;t acc: 0.77 ;v loss: 0.75573 ;v acc: 0.55\n",
      "Epoch 37;t loss: 0.48808 ;t acc: 0.78 ;v loss: 0.77533 ;v acc: 0.57\n",
      "Epoch 38;t loss: 0.49186 ;t acc: 0.78 ;v loss: 0.76458 ;v acc: 0.53\n",
      "Epoch 39;t loss: 0.47935 ;t acc: 0.79 ;v loss: 0.80235 ;v acc: 0.56\n",
      "Epoch 40;t loss: 0.47571 ;t acc: 0.79 ;v loss: 0.76834 ;v acc: 0.54\n",
      "tensor([0.6230], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5533], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7780], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4991], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6472], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4779], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6114], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5571], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.8074], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3512], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5279], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3545], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4933], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2289], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3439], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2809], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.8022], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7977], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6624], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6941], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5710], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5299], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4707], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5114], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2761], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5557], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.1803], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3113], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3289], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7655], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3631], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4801], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3146], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2499], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5630], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6374], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3754], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3864], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4578], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2380], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6957443784922361 ; Test acc: 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41;t loss: 0.47170 ;t acc: 0.79 ;v loss: 0.77383 ;v acc: 0.56\n",
      "Epoch 42;t loss: 0.45327 ;t acc: 0.81 ;v loss: 0.84373 ;v acc: 0.54\n",
      "Epoch 43;t loss: 0.45611 ;t acc: 0.79 ;v loss: 0.84300 ;v acc: 0.53\n",
      "Epoch 44;t loss: 0.46010 ;t acc: 0.81 ;v loss: 0.79265 ;v acc: 0.59\n",
      "Epoch 45;t loss: 0.44292 ;t acc: 0.80 ;v loss: 0.80730 ;v acc: 0.60\n",
      "Epoch 46;t loss: 0.44196 ;t acc: 0.81 ;v loss: 0.82650 ;v acc: 0.56\n",
      "Epoch 47;t loss: 0.44171 ;t acc: 0.81 ;v loss: 0.83651 ;v acc: 0.51\n",
      "Epoch 48;t loss: 0.43203 ;t acc: 0.80 ;v loss: 0.82179 ;v acc: 0.51\n",
      "Epoch 49;t loss: 0.43040 ;t acc: 0.82 ;v loss: 0.89438 ;v acc: 0.51\n",
      "Epoch 50;t loss: 0.42760 ;t acc: 0.82 ;v loss: 0.83793 ;v acc: 0.53\n",
      "Epoch 51;t loss: 0.41405 ;t acc: 0.82 ;v loss: 0.85221 ;v acc: 0.54\n",
      "Epoch 52;t loss: 0.41119 ;t acc: 0.83 ;v loss: 0.84742 ;v acc: 0.53\n",
      "Epoch 53;t loss: 0.41110 ;t acc: 0.83 ;v loss: 0.86082 ;v acc: 0.55\n",
      "Epoch 54;t loss: 0.39306 ;t acc: 0.84 ;v loss: 0.86782 ;v acc: 0.49\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "model = Model(train_dataset[0].x.shape[1]).to(device)  \n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameter count: {pytorch_total_params}')\n",
    "\n",
    "# model = model.to(device)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=.1, rho=0.9, eps=1e-06, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=1e-4, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=1e-4, weight_decay=1e-2)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-5)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-3, lr_decay=0, weight_decay=0)\n",
    "\n",
    "# Instantiate optimizer\n",
    "# scheduler = StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "def train(loader, target = 0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        y = (batch.y[:,target] > 5).float()\n",
    "#         y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        loss = criterion(out,y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        right += torch.eq(out > .5, y > .5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "def test(loader,verbose=False, target = 0):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        y = (batch.y[:,target] > 5).float()\n",
    "#         y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        if verbose:\n",
    "            print(out,y)\n",
    "        loss = criterion(out,y)\n",
    "        losses.append(loss.item())\n",
    "        right += torch.eq(out > .5, y > .5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "best_val_loss = np.inf\n",
    "esp = 0\n",
    "MAX_ESP = 50\n",
    "\n",
    "BS = 8\n",
    "\n",
    "target = 0 # Valence-Arousal-Dominance-Liking\n",
    "\n",
    "splt_idx = 1160\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [splt_idx, len(train_dataset)-splt_idx])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BS, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BS)\n",
    "for epoch in range(1, 10000):    \n",
    "\n",
    "    # Training and validation\n",
    "    train_loss, train_acc = train(train_loader, target = target)\n",
    "    val_loss, val_acc = test(val_loader , target = target)\n",
    "    print(f'Epoch {epoch};t loss: {train_loss:.5f} ;t acc: {train_acc:.2f} ;v loss: {val_loss:.5f} ;v acc: {val_acc:.2f}')\n",
    "\n",
    "    # Early stopping and checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        esp = 0\n",
    "        torch.save(model.state_dict(),'./best_params') \n",
    "    else:\n",
    "        esp += 1\n",
    "        if esp >= MAX_ESP:\n",
    "            break\n",
    "            \n",
    "    if epoch % 20 == 0:\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "        loss, acc = test(test_loader, True)\n",
    "        print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.656625260566843 ; Train acc: 0.6008620689655172\n",
      "Val loss: 0.6766686975955963 ; Val acc: 0.6\n",
      "tensor([0.5964], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5896], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5569], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5922], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5587], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5795], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5209], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5599], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6175], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5006], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5694], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5371], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5463], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5232], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5173], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5256], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6032], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6136], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5116], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5845], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5412], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5651], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5621], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5417], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5015], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5564], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5338], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4776], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5286], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6095], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5878], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5530], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5461], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5406], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5428], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5644], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5459], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5328], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5265], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5322], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.7025002911686897 ; Test acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_params'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "loss, acc = test(train_loader, False,target=target)\n",
    "print(f'Train loss: {loss} ; Train acc: {acc}')\n",
    "loss, acc = test(val_loader, False,target=target)\n",
    "\n",
    "print(f'Val loss: {loss} ; Val acc: {acc}')\n",
    "loss, acc = test(test_loader, True,target=target)\n",
    "print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "# TODO: scheduler(?) Loss/acc records"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
