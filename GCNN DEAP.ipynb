{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skimage\n",
    "import pywt\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from einops import reduce, rearrange, repeat\n",
    "from npeet import entropy_estimators as ee\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from scipy.fft import rfft, rfftfreq, ifft\n",
    "from einops import rearrange\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from Electrodes import Electrodes\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDatasetEEGFeatures(InMemoryDataset):\n",
    "  def __init__(self, root, raw_dir, processed_dir, feature='de', transform=None, pre_transform=None,include_edge_attr = True, undirected_graphs = True, add_global_connections=True, participant_from=1, participant_to=32, n_videos=40):\n",
    "      self._raw_dir = raw_dir\n",
    "      self._processed_dir = processed_dir\n",
    "      self.participant_from = participant_from\n",
    "      self.participant_to = participant_to\n",
    "      self.n_videos = n_videos\n",
    "      self.feature = feature\n",
    "      # Whether or not to include edge_attr in the dataset\n",
    "      self.include_edge_attr = include_edge_attr\n",
    "      # If true there will be 1024 links as opposed to 528\n",
    "      self.undirected_graphs = undirected_graphs\n",
    "      # Instantiate class to handle electrode positions\n",
    "      print('Using global connections' if add_global_connections else 'Not using global connections')\n",
    "      self.electrodes = Electrodes(add_global_connections, expand_3d = False)\n",
    "      super(DEAPDatasetEEGFeatures, self).__init__(root, transform, pre_transform)\n",
    "      self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "      \n",
    "  @property\n",
    "  def raw_dir(self):\n",
    "      return f'{self.root}/{self._raw_dir}'\n",
    "\n",
    "  @property\n",
    "  def processed_dir(self):\n",
    "      return f'{self.root}/{self._processed_dir}'\n",
    "\n",
    "  @property\n",
    "  def raw_file_names(self):\n",
    "      raw_names = [f for f in os.listdir(self.raw_dir)]\n",
    "      raw_names.sort()\n",
    "      return raw_names\n",
    "\n",
    "  @property\n",
    "  def processed_file_names(self):\n",
    "      if not os.path.exists(self.processed_dir):\n",
    "        os.makedirs(self.processed_dir)\n",
    "      file_name = f'{self.participant_from}-{self.participant_to}' if self.participant_from is not self.participant_to else f'{self.participant_from}'\n",
    "      return [f'deap_processed_graph.{file_name}_{self.feature}.dataset']\n",
    "\n",
    "  def process(self):\n",
    "        # Number of nodes per graph\n",
    "        n_nodes = len(self.electrodes.positions_3d)\n",
    "        \n",
    "\n",
    "        if self.undirected_graphs:\n",
    "            source_nodes, target_nodes = np.repeat(np.arange(0,n_nodes),n_nodes), np.tile(np.arange(0,n_nodes),n_nodes)\n",
    "        else:\n",
    "            source_nodes, target_nodes = np.tril_indices(n_nodes,n_nodes)\n",
    "        \n",
    "        edge_attr = self.electrodes.adjacency_matrix[source_nodes,target_nodes]\n",
    "        \n",
    "        # Remove zero weight links\n",
    "        mask = np.ma.masked_not_equal(edge_attr, 0).mask\n",
    "        edge_attr,source_nodes,target_nodes = edge_attr[mask], source_nodes[mask], target_nodes[mask]\n",
    "\n",
    "        edge_attr, edge_index = torch.FloatTensor(edge_attr), torch.tensor([source_nodes,target_nodes], dtype=torch.long)\n",
    "        \n",
    "        # Expand edge_index and edge_attr to match windows\n",
    "        e_edge_index = edge_index.clone()\n",
    "        e_edge_attr = edge_attr.clone()\n",
    "        number_of_graphs = 4\n",
    "        for i in range(number_of_graphs-1):\n",
    "            a = edge_index + e_edge_index.max() + 1\n",
    "            e_edge_index = torch.cat([e_edge_index,a],dim=1)\n",
    "            e_edge_attr = torch.cat([e_edge_attr,edge_attr],dim=0)\n",
    "\n",
    "        print(f'Number of graphs per video: {number_of_graphs}')\n",
    "        # List of graphs that will be written to file\n",
    "        data_list = []\n",
    "        pbar = tqdm(range(self.participant_from,self.participant_to+1))\n",
    "        for participant_id in pbar:\n",
    "            raw_name = [e for e in self.raw_file_names if str(participant_id).zfill(2) in e][0]\n",
    "            pbar.set_description(raw_name)\n",
    "            # Load raw file as np array\n",
    "            participant_data = scipy.io.loadmat(f'{self.raw_dir}/{raw_name}')\n",
    "            signal_data = torch.FloatTensor(remove_baseline_mean(participant_data['data'][:,:32,:]))\n",
    "#             signal_data = torch.FloatTensor()\n",
    "            processed = []\n",
    "            for i, video in enumerate(signal_data[:self.n_videos,:,:]):\n",
    "                if self.feature == 'wav':\n",
    "                    node_features = process_video_wavelet(video)\n",
    "                else:\n",
    "                    node_features = process_video(video, feature=self.feature)\n",
    "                data = Data(x=torch.FloatTensor(node_features),edge_attr=e_edge_attr,edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]])) if self.include_edge_attr else Data(x=torch.FloatTensor(node_features), edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]]))\n",
    "                data_list.append(data) \n",
    "               \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_de(window):\n",
    "    return ee.entropy(window.reshape(-1,1), k=2)\n",
    "# Input: Video with shape (32,7680)\n",
    "# Output: Graph node features with shape (5*32, 59) -> 5 graphs with 32 nodes each with 59 features each\n",
    "def process_video(video, feature='psd'):\n",
    "    # Transform to frequency domain\n",
    "    fft_vals = np.fft.rfft(video, axis=-1)\n",
    "     # Get frequencies for amplitudes in Hz\n",
    "    samplingFrequency = 128\n",
    "    fft_freq = np.fft.rfftfreq(video.shape[-1], 1.0/samplingFrequency)\n",
    "    # Delta, Theta, Alpha, Beta, Gamma\n",
    "    bands = [(0,4),(4,8),(8,12),(12,30),(30,45)]\n",
    "    \n",
    "    band_mask = np.array([np.logical_or(fft_freq < f, fft_freq > t) for f,t in bands])\n",
    "    band_mask = repeat(band_mask,'a b -> a c b', c=32)\n",
    "    band_data = np.array(fft_vals)\n",
    "    band_data = repeat(band_data,'a b -> c a b', c=5)\n",
    "     \n",
    "    band_data[band_mask] = 0\n",
    "    \n",
    "    band_data = np.fft.irfft(band_data)\n",
    "\n",
    "    windows = skimage.util.view_as_windows(band_data, (5,32,128), step=128).squeeze()\n",
    "    # (5, 32, 60, 128)\n",
    "    windows = rearrange(windows, 'a b c d -> b c a d')\n",
    "    \n",
    "    if feature == 'psd':\n",
    "        features = scipy.signal.periodogram(windows)[1]\n",
    "        features = np.mean(features, axis=-1)\n",
    "    elif feature == 'de':\n",
    "        features = np.apply_along_axis(calculate_de, -1, windows)\n",
    "\n",
    "    \n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_video(signal_data):\n",
    "    electrodes = Electrodes(expand_3d=False)\n",
    "    fig, axs = plt.subplots(32, sharex=True, figsize=(20,50))\n",
    "    fig.tight_layout()\n",
    "    video = signal_data[0]\n",
    "    for i in range(32):\n",
    "        c = [float(i)/float(32), 0.0, float(32-i)/float(32)] #R,G,B\n",
    "        axs[i].set_title(f'{electrodes.channel_names[i]}', loc='left', fontsize=20)\n",
    "        axs[i].plot(video[i],color=c)\n",
    "        axs[i].spines['top'].set_visible(False)\n",
    "        axs[i].spines['right'].set_visible(False)\n",
    "        axs[i].spines['left'].set_visible(False)\n",
    "        axs[i].spines['bottom'].set_visible(False)\n",
    "    plt.savefig('eeg.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baseline_mean(signal_data):\n",
    "    # Take first three senconds of data\n",
    "    signal_baseline = np.array(signal_data[:,:,:128*3]).reshape(40,32,128,-1)\n",
    "    # Mean of three senconds of baseline will be deducted from all windows\n",
    "    signal_noise = np.mean(signal_baseline,axis=-1)\n",
    "    # Expand mask\n",
    "    signal_noise = repeat(signal_noise,'a b c -> a b (d c)',d=60)\n",
    "    return signal_data[:,:,128*3:] - signal_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_wavelet(video, feature='energy', time_domain=False):\n",
    "    band_widths = [32,16,8,4]\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            # Highest frequencies (64-128Hz) are not used\n",
    "            cA, cD = pywt.dwt(video.numpy(), 'db4')\n",
    "        else:\n",
    "            cA, cD = pywt.dwt(cA, 'db4')\n",
    "            cA_windows = skimage.util.view_as_windows(cA, (32,band_widths[i-1]*2), step=band_widths[i-1]).squeeze()\n",
    "            cA_windows = np.transpose(cA_windows[:59,:,:],(1,0,2))\n",
    "            if feature == 'energy':\n",
    "                cA_windows = np.square(cA_windows)\n",
    "                cA_windows = np.sum(cA_windows, axis=-1)\n",
    "                features.append(cA_windows)\n",
    "                \n",
    "    if time_domain:\n",
    "        features = np.transpose(features,(2,1,0))\n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global connections\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[776], edge_index=[2, 776], x=[128, 59], y=[1, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants used to define data paths\n",
    "ROOT_DIR = './'\n",
    "RAW_DIR = 'data/matlabPREPROCESSED'\n",
    "PROCESSED_DIR = 'data/graphProcessedData'\n",
    "\n",
    "dataset = DEAPDatasetEEGFeatures(root= ROOT_DIR, raw_dir= RAW_DIR, processed_dir= PROCESSED_DIR, feature='wav')\n",
    "# dataset = dataset.shuffle()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 880 used for training, 220 validation and 180 testing\n",
    "test_participant = 0\n",
    "# \n",
    "# splt_idx = 35\n",
    "\n",
    "# 85% used for train/val\n",
    "train_dataset = dataset[0:test_participant*40] + dataset[test_participant*40+40:]\n",
    "test_dataset = dataset[test_participant*40:test_participant*40+40]\n",
    "\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCN2Conv, GCNConv\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=512, cnn_hidden_dim = 256):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        self.gconv1 = GCN2Conv(in_channels,1)\n",
    "        self.gconv2 = GCNConv(in_channels,hidden_channels)\n",
    "        \n",
    "#         self.gconv3 = GCNConv(in_channels,hidden_channels)\n",
    "        \n",
    "        # self.rnn = torch.nn.GRU(hidden_channels, rnn_hidden_dim, 2,dropout=0.2, batch_first=True)\n",
    "        self.cnn1 = torch.nn.Conv1d(4*hidden_channels, hidden_channels, kernel_size=1, stride=1)\n",
    "        self.cnn2 = torch.nn.Conv1d(hidden_channels, cnn_hidden_dim, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(32*cnn_hidden_dim, 1)\n",
    "#         self.lin2 = torch.nn.Linear(cnn_hidden_dim, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        bs = len(torch.unique(batch.batch))\n",
    "        x_, edge_index, edge_attr = batch.x, batch.edge_index, batch.edge_attr\n",
    "#         print(x.shape)\n",
    "        x = self.gconv1(x_,x_, edge_index, edge_attr)\n",
    "#         x = x.relu()\n",
    "        x = self.gconv2(x, edge_index, edge_attr)\n",
    "        x = x.tanh()\n",
    "        \n",
    "#         x = self.gconv3(x, edge_index, edge_attr )\n",
    "#         print('-0----------------------------------------------------')\n",
    "#         print(x)\n",
    "#         x = x.relu()\n",
    "#         print(x)\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # x = rearrange(x, '(bs a b) c -> (bs b) a c', bs=bs, b=32, c=self.hidden_channels)\n",
    "        # o, (h_n,c_n) = self.rnn(x)\n",
    "        x = rearrange(x, '(bs g e) f -> bs (g f) e', bs=bs, e=32)\n",
    "        x = self.cnn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.cnn2(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = rearrange(x, 'bs a b -> bs (a b)', bs=bs)\n",
    "        \n",
    "        \n",
    "        x = self.lin1(x)\n",
    "#         x = x.relu()\n",
    "#         x = self.lin2(x)\n",
    "        x = x.view(-1)\n",
    "        return x.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count: 1222810\n",
      "Epoch 1;t loss: 0.68884 ;t acc: 0.53 ;v loss: 0.67046 ;v acc: 0.68\n",
      "Epoch 2;t loss: 0.67960 ;t acc: 0.57 ;v loss: 0.66599 ;v acc: 0.66\n",
      "Epoch 3;t loss: 0.67678 ;t acc: 0.58 ;v loss: 0.66223 ;v acc: 0.66\n",
      "Epoch 4;t loss: 0.67245 ;t acc: 0.60 ;v loss: 0.65808 ;v acc: 0.66\n",
      "Epoch 5;t loss: 0.66831 ;t acc: 0.58 ;v loss: 0.66561 ;v acc: 0.66\n",
      "Epoch 6;t loss: 0.66621 ;t acc: 0.61 ;v loss: 0.65902 ;v acc: 0.66\n",
      "Epoch 7;t loss: 0.66478 ;t acc: 0.61 ;v loss: 0.66442 ;v acc: 0.65\n",
      "Epoch 8;t loss: 0.66428 ;t acc: 0.61 ;v loss: 0.66695 ;v acc: 0.62\n",
      "Epoch 9;t loss: 0.66279 ;t acc: 0.62 ;v loss: 0.66779 ;v acc: 0.61\n",
      "Epoch 10;t loss: 0.65935 ;t acc: 0.64 ;v loss: 0.66226 ;v acc: 0.61\n",
      "Epoch 11;t loss: 0.65740 ;t acc: 0.63 ;v loss: 0.66133 ;v acc: 0.61\n",
      "Epoch 12;t loss: 0.65719 ;t acc: 0.64 ;v loss: 0.65996 ;v acc: 0.62\n",
      "Epoch 13;t loss: 0.65629 ;t acc: 0.64 ;v loss: 0.65723 ;v acc: 0.64\n",
      "Epoch 14;t loss: 0.65227 ;t acc: 0.64 ;v loss: 0.66089 ;v acc: 0.62\n",
      "Epoch 15;t loss: 0.65336 ;t acc: 0.64 ;v loss: 0.66244 ;v acc: 0.61\n",
      "Epoch 16;t loss: 0.65152 ;t acc: 0.66 ;v loss: 0.66374 ;v acc: 0.59\n",
      "Epoch 17;t loss: 0.64861 ;t acc: 0.66 ;v loss: 0.66518 ;v acc: 0.59\n",
      "Epoch 18;t loss: 0.64619 ;t acc: 0.67 ;v loss: 0.65906 ;v acc: 0.61\n",
      "Epoch 19;t loss: 0.64426 ;t acc: 0.66 ;v loss: 0.65799 ;v acc: 0.61\n",
      "Epoch 20;t loss: 0.64564 ;t acc: 0.65 ;v loss: 0.66322 ;v acc: 0.60\n",
      "tensor([0.5754], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5876], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5636], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5821], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4619], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4796], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4980], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5173], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5954], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4745], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5717], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4353], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4833], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4485], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4420], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5390], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6066], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6209], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5144], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5744], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5603], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5061], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5160], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4576], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4629], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5324], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4556], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4633], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4655], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6378], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4848], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5264], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5513], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4931], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5297], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5362], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5206], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5189], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4709], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5137], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.694631777703762 ; Test acc: 0.575\n",
      "Epoch 21;t loss: 0.64461 ;t acc: 0.65 ;v loss: 0.65996 ;v acc: 0.59\n",
      "Epoch 22;t loss: 0.64249 ;t acc: 0.66 ;v loss: 0.66034 ;v acc: 0.60\n",
      "Epoch 23;t loss: 0.64116 ;t acc: 0.66 ;v loss: 0.65985 ;v acc: 0.60\n",
      "Epoch 24;t loss: 0.63894 ;t acc: 0.67 ;v loss: 0.66044 ;v acc: 0.60\n",
      "Epoch 25;t loss: 0.63826 ;t acc: 0.67 ;v loss: 0.66317 ;v acc: 0.60\n",
      "Epoch 26;t loss: 0.63606 ;t acc: 0.68 ;v loss: 0.66045 ;v acc: 0.59\n",
      "Epoch 27;t loss: 0.64011 ;t acc: 0.66 ;v loss: 0.65979 ;v acc: 0.59\n",
      "Epoch 28;t loss: 0.63630 ;t acc: 0.67 ;v loss: 0.66230 ;v acc: 0.60\n",
      "Epoch 29;t loss: 0.63562 ;t acc: 0.66 ;v loss: 0.66080 ;v acc: 0.60\n",
      "Epoch 30;t loss: 0.63277 ;t acc: 0.68 ;v loss: 0.65839 ;v acc: 0.60\n",
      "Epoch 31;t loss: 0.63200 ;t acc: 0.68 ;v loss: 0.65897 ;v acc: 0.60\n",
      "Epoch 32;t loss: 0.63321 ;t acc: 0.66 ;v loss: 0.66040 ;v acc: 0.59\n",
      "Epoch 33;t loss: 0.63163 ;t acc: 0.67 ;v loss: 0.65566 ;v acc: 0.60\n",
      "Epoch 34;t loss: 0.63136 ;t acc: 0.67 ;v loss: 0.66078 ;v acc: 0.57\n",
      "Epoch 35;t loss: 0.62747 ;t acc: 0.68 ;v loss: 0.65596 ;v acc: 0.61\n",
      "Epoch 36;t loss: 0.63078 ;t acc: 0.67 ;v loss: 0.65815 ;v acc: 0.60\n",
      "Epoch 37;t loss: 0.62812 ;t acc: 0.67 ;v loss: 0.66189 ;v acc: 0.59\n",
      "Epoch 38;t loss: 0.62621 ;t acc: 0.68 ;v loss: 0.66358 ;v acc: 0.60\n",
      "Epoch 39;t loss: 0.62486 ;t acc: 0.69 ;v loss: 0.66352 ;v acc: 0.60\n",
      "Epoch 40;t loss: 0.62326 ;t acc: 0.69 ;v loss: 0.66408 ;v acc: 0.60\n",
      "tensor([0.5850], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6052], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5761], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5935], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4209], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4462], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4808], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5159], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6186], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4465], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5725], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3808], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4624], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3931], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3948], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5254], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6341], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6505], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4922], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5770], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5567], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4725], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4971], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4047], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4102], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5272], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4116], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4123], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4289], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6728], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4412], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5126], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5479], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4627], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5262], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5236], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4948], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4935], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4394], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4911], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6956159748136997 ; Test acc: 0.525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41;t loss: 0.62409 ;t acc: 0.67 ;v loss: 0.66485 ;v acc: 0.61\n",
      "Epoch 42;t loss: 0.62405 ;t acc: 0.68 ;v loss: 0.66263 ;v acc: 0.60\n",
      "Epoch 43;t loss: 0.62269 ;t acc: 0.68 ;v loss: 0.66418 ;v acc: 0.61\n",
      "Epoch 44;t loss: 0.62189 ;t acc: 0.68 ;v loss: 0.66286 ;v acc: 0.60\n",
      "Epoch 45;t loss: 0.61995 ;t acc: 0.69 ;v loss: 0.66317 ;v acc: 0.60\n",
      "Epoch 46;t loss: 0.61871 ;t acc: 0.68 ;v loss: 0.66474 ;v acc: 0.60\n",
      "Epoch 47;t loss: 0.61916 ;t acc: 0.69 ;v loss: 0.67263 ;v acc: 0.57\n",
      "Epoch 48;t loss: 0.61747 ;t acc: 0.68 ;v loss: 0.66411 ;v acc: 0.59\n",
      "Epoch 49;t loss: 0.61789 ;t acc: 0.69 ;v loss: 0.66972 ;v acc: 0.59\n",
      "Epoch 50;t loss: 0.61551 ;t acc: 0.68 ;v loss: 0.65979 ;v acc: 0.57\n",
      "Epoch 51;t loss: 0.61524 ;t acc: 0.69 ;v loss: 0.66168 ;v acc: 0.59\n",
      "Epoch 52;t loss: 0.61432 ;t acc: 0.70 ;v loss: 0.66072 ;v acc: 0.57\n",
      "Epoch 53;t loss: 0.61466 ;t acc: 0.68 ;v loss: 0.66226 ;v acc: 0.60\n",
      "Epoch 54;t loss: 0.61239 ;t acc: 0.70 ;v loss: 0.67003 ;v acc: 0.57\n",
      "Epoch 55;t loss: 0.61383 ;t acc: 0.69 ;v loss: 0.66494 ;v acc: 0.57\n",
      "Epoch 56;t loss: 0.61149 ;t acc: 0.67 ;v loss: 0.66539 ;v acc: 0.57\n",
      "Epoch 57;t loss: 0.61246 ;t acc: 0.70 ;v loss: 0.66211 ;v acc: 0.59\n",
      "Epoch 58;t loss: 0.60902 ;t acc: 0.69 ;v loss: 0.66204 ;v acc: 0.59\n",
      "Epoch 59;t loss: 0.61033 ;t acc: 0.69 ;v loss: 0.66214 ;v acc: 0.59\n",
      "Epoch 60;t loss: 0.60860 ;t acc: 0.69 ;v loss: 0.66273 ;v acc: 0.59\n",
      "tensor([0.6002], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6161], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5921], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6067], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4049], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4371], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4825], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5291], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6385], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4387], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5738], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3631], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4626], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3627], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3783], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5213], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6535], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6694], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4914], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5884], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5599], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4596], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4929], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3838], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3828], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5366], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3911], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3957], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4140], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6918], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4222], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5170], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5521], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4481], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5338], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5291], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4799], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4900], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4382], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4896], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6978997580707074 ; Test acc: 0.525\n",
      "Epoch 61;t loss: 0.60702 ;t acc: 0.70 ;v loss: 0.66093 ;v acc: 0.59\n",
      "Epoch 62;t loss: 0.60495 ;t acc: 0.70 ;v loss: 0.66633 ;v acc: 0.59\n",
      "Epoch 63;t loss: 0.60563 ;t acc: 0.70 ;v loss: 0.66266 ;v acc: 0.59\n",
      "Epoch 64;t loss: 0.60437 ;t acc: 0.70 ;v loss: 0.66202 ;v acc: 0.59\n",
      "Epoch 65;t loss: 0.60484 ;t acc: 0.69 ;v loss: 0.66265 ;v acc: 0.57\n",
      "Epoch 66;t loss: 0.60376 ;t acc: 0.70 ;v loss: 0.66051 ;v acc: 0.60\n",
      "Epoch 67;t loss: 0.60170 ;t acc: 0.70 ;v loss: 0.66102 ;v acc: 0.60\n",
      "Epoch 68;t loss: 0.60023 ;t acc: 0.71 ;v loss: 0.65864 ;v acc: 0.60\n",
      "Epoch 69;t loss: 0.60406 ;t acc: 0.70 ;v loss: 0.66179 ;v acc: 0.60\n",
      "Epoch 70;t loss: 0.59732 ;t acc: 0.72 ;v loss: 0.66502 ;v acc: 0.57\n",
      "Epoch 71;t loss: 0.59988 ;t acc: 0.70 ;v loss: 0.66288 ;v acc: 0.59\n",
      "Epoch 72;t loss: 0.60047 ;t acc: 0.70 ;v loss: 0.66547 ;v acc: 0.57\n",
      "Epoch 73;t loss: 0.59804 ;t acc: 0.70 ;v loss: 0.66873 ;v acc: 0.56\n",
      "Epoch 74;t loss: 0.59696 ;t acc: 0.71 ;v loss: 0.66357 ;v acc: 0.57\n",
      "Epoch 75;t loss: 0.59247 ;t acc: 0.72 ;v loss: 0.66671 ;v acc: 0.57\n",
      "Epoch 76;t loss: 0.59343 ;t acc: 0.71 ;v loss: 0.67327 ;v acc: 0.56\n",
      "Epoch 77;t loss: 0.59560 ;t acc: 0.70 ;v loss: 0.66695 ;v acc: 0.57\n",
      "Epoch 78;t loss: 0.59408 ;t acc: 0.71 ;v loss: 0.65857 ;v acc: 0.61\n",
      "Epoch 79;t loss: 0.59490 ;t acc: 0.71 ;v loss: 0.66416 ;v acc: 0.57\n",
      "Epoch 80;t loss: 0.59432 ;t acc: 0.72 ;v loss: 0.66286 ;v acc: 0.57\n",
      "tensor([0.6075], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6195], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6035], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6165], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3935], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4349], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4772], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5454], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6507], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4389], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5689], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3456], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4605], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3366], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3683], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5162], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6680], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6812], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4919], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5990], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5631], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4514], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4838], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3651], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3591], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5379], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3779], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3838], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4033], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6997], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4084], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5191], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5539], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4403], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5386], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5332], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4630], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4805], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4432], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4907], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6995421305298806 ; Test acc: 0.525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81;t loss: 0.59060 ;t acc: 0.72 ;v loss: 0.66282 ;v acc: 0.57\n",
      "Epoch 82;t loss: 0.58834 ;t acc: 0.72 ;v loss: 0.66213 ;v acc: 0.57\n",
      "Epoch 83;t loss: 0.58779 ;t acc: 0.73 ;v loss: 0.66826 ;v acc: 0.56\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "model = Model(train_dataset[0].x.shape[1]).to(device)  \n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameter count: {pytorch_total_params}')\n",
    "\n",
    "# model = model.to(device)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=.1, rho=0.9, eps=1e-06, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=1e-4, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=1e-4, weight_decay=1e-2)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-5)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-4, lr_decay=0, weight_decay=5e-2)\n",
    "\n",
    "# Instantiate optimizer\n",
    "# scheduler = StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "def train(loader, target = 0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        y = (batch.y[:,target] > 5).float()\n",
    "#         y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        loss = criterion(out,y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        right += torch.eq(out > .5, y > .5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "def test(loader,verbose=False, target = 0):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        y = (batch.y[:,target] > 5).float()\n",
    "#         y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        if verbose:\n",
    "            print(out,y)\n",
    "        loss = criterion(out,y)\n",
    "        losses.append(loss.item())\n",
    "        right += torch.eq(out > .5, y > .5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "best_val_loss = np.inf\n",
    "esp = 0\n",
    "MAX_ESP = 50\n",
    "\n",
    "BS = 8\n",
    "\n",
    "target = 0 # Valence-Arousal-Dominance-Liking\n",
    "\n",
    "splt_idx = 1160\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [splt_idx, len(train_dataset)-splt_idx])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BS, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BS)\n",
    "for epoch in range(1, 10000):    \n",
    "\n",
    "    # Training and validation\n",
    "    train_loss, train_acc = train(train_loader, target = target)\n",
    "    val_loss, val_acc = test(val_loader , target = target)\n",
    "    print(f'Epoch {epoch};t loss: {train_loss:.5f} ;t acc: {train_acc:.2f} ;v loss: {val_loss:.5f} ;v acc: {val_acc:.2f}')\n",
    "\n",
    "    # Early stopping and checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        esp = 0\n",
    "        torch.save(model.state_dict(),'./best_params') \n",
    "    else:\n",
    "        esp += 1\n",
    "        if esp >= MAX_ESP:\n",
    "            break\n",
    "            \n",
    "    if epoch % 20 == 0:\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "        loss, acc = test(test_loader, True)\n",
    "        print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.628433863253429 ; Train acc: 0.6732758620689655\n",
      "Val loss: 0.6556640923023224 ; Val acc: 0.6\n",
      "tensor([0.6001], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6186], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5911], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6067], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4521], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4755], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5078], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5390], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6292], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4726], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5848], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4191], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4900], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4264], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4299], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5481], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6424], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6576], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5180], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5985], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5796], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5028], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5206], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4402], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4472], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5469], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4433], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4471], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4590], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6790], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4781], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5379], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5649], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4917], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5462], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5504], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5200], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5256], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4680], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5200], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6976296819746495 ; Test acc: 0.6\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_params'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "loss, acc = test(train_loader, False,target=target)\n",
    "print(f'Train loss: {loss} ; Train acc: {acc}')\n",
    "loss, acc = test(val_loader, False,target=target)\n",
    "\n",
    "print(f'Val loss: {loss} ; Val acc: {acc}')\n",
    "loss, acc = test(test_loader, True,target=target)\n",
    "print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "# TODO: scheduler(?) Loss/acc records"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
