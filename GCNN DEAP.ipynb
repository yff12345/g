{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skimage\n",
    "import pywt\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from einops import reduce, rearrange, repeat\n",
    "from npeet import entropy_estimators as ee\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from scipy.fft import rfft, rfftfreq, ifft\n",
    "from einops import rearrange\n",
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from Electrodes import Electrodes\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAPDatasetEEGFeatures(InMemoryDataset):\n",
    "  def __init__(self, root, raw_dir, processed_dir, feature='de', transform=None, pre_transform=None,include_edge_attr = True, undirected_graphs = True, add_global_connections=True, participant_from=1, participant_to=32, n_videos=40):\n",
    "      self._raw_dir = raw_dir\n",
    "      self._processed_dir = processed_dir\n",
    "      self.participant_from = participant_from\n",
    "      self.participant_to = participant_to\n",
    "      self.n_videos = n_videos\n",
    "      self.feature = feature\n",
    "      # Whether or not to include edge_attr in the dataset\n",
    "      self.include_edge_attr = include_edge_attr\n",
    "      # If true there will be 1024 links as opposed to 528\n",
    "      self.undirected_graphs = undirected_graphs\n",
    "      # Instantiate class to handle electrode positions\n",
    "      print('Using global connections' if add_global_connections else 'Not using global connections')\n",
    "      self.electrodes = Electrodes(add_global_connections, expand_3d = False)\n",
    "      super(DEAPDatasetEEGFeatures, self).__init__(root, transform, pre_transform)\n",
    "      self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "      \n",
    "  @property\n",
    "  def raw_dir(self):\n",
    "      return f'{self.root}/{self._raw_dir}'\n",
    "\n",
    "  @property\n",
    "  def processed_dir(self):\n",
    "      return f'{self.root}/{self._processed_dir}'\n",
    "\n",
    "  @property\n",
    "  def raw_file_names(self):\n",
    "      raw_names = [f for f in os.listdir(self.raw_dir)]\n",
    "      raw_names.sort()\n",
    "      return raw_names\n",
    "\n",
    "  @property\n",
    "  def processed_file_names(self):\n",
    "      if not os.path.exists(self.processed_dir):\n",
    "        os.makedirs(self.processed_dir)\n",
    "      file_name = f'{self.participant_from}-{self.participant_to}' if self.participant_from is not self.participant_to else f'{self.participant_from}'\n",
    "      return [f'deap_processed_graph.{file_name}_{self.feature}.dataset']\n",
    "\n",
    "  def process(self):\n",
    "        # Number of nodes per graph\n",
    "        n_nodes = len(self.electrodes.positions_3d)\n",
    "        \n",
    "\n",
    "        if self.undirected_graphs:\n",
    "            source_nodes, target_nodes = np.repeat(np.arange(0,n_nodes),n_nodes), np.tile(np.arange(0,n_nodes),n_nodes)\n",
    "        else:\n",
    "            source_nodes, target_nodes = np.tril_indices(n_nodes,n_nodes)\n",
    "        \n",
    "        edge_attr = self.electrodes.adjacency_matrix[source_nodes,target_nodes]\n",
    "        \n",
    "        # Remove zero weight links\n",
    "        mask = np.ma.masked_not_equal(edge_attr, 0).mask\n",
    "        edge_attr,source_nodes,target_nodes = edge_attr[mask], source_nodes[mask], target_nodes[mask]\n",
    "\n",
    "        edge_attr, edge_index = torch.FloatTensor(edge_attr), torch.tensor([source_nodes,target_nodes], dtype=torch.long)\n",
    "        \n",
    "        # Expand edge_index and edge_attr to match windows\n",
    "        e_edge_index = edge_index.clone()\n",
    "        e_edge_attr = edge_attr.clone()\n",
    "        number_of_graphs = 4\n",
    "        for i in range(number_of_graphs-1):\n",
    "            a = edge_index + e_edge_index.max() + 1\n",
    "            e_edge_index = torch.cat([e_edge_index,a],dim=1)\n",
    "            e_edge_attr = torch.cat([e_edge_attr,edge_attr],dim=0)\n",
    "\n",
    "        print(f'Number of graphs per video: {number_of_graphs}')\n",
    "        # List of graphs that will be written to file\n",
    "        data_list = []\n",
    "        pbar = tqdm(range(self.participant_from,self.participant_to+1))\n",
    "        for participant_id in pbar:\n",
    "            raw_name = [e for e in self.raw_file_names if str(participant_id).zfill(2) in e][0]\n",
    "            pbar.set_description(raw_name)\n",
    "            # Load raw file as np array\n",
    "            participant_data = scipy.io.loadmat(f'{self.raw_dir}/{raw_name}')\n",
    "            signal_data = torch.FloatTensor(remove_baseline_mean(participant_data['data'][:,:32,:]))\n",
    "#             signal_data = torch.FloatTensor()\n",
    "            processed = []\n",
    "            for i, video in enumerate(signal_data[:self.n_videos,:,:]):\n",
    "                if self.feature == 'wav':\n",
    "                    node_features = process_video_wavelet(video)\n",
    "                else:\n",
    "                    node_features = process_video(video, feature=self.feature)\n",
    "                data = Data(x=torch.FloatTensor(node_features),edge_attr=e_edge_attr,edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]])) if self.include_edge_attr else Data(x=torch.FloatTensor(node_features), edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]]))\n",
    "                data_list.append(data) \n",
    "               \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_de(window):\n",
    "    return ee.entropy(window.reshape(-1,1), k=2)\n",
    "# Input: Video with shape (32,7680)\n",
    "# Output: Graph node features with shape (5*32, 59) -> 5 graphs with 32 nodes each with 59 features each\n",
    "def process_video(video, feature='psd'):\n",
    "    # Transform to frequency domain\n",
    "    fft_vals = np.fft.rfft(video, axis=-1)\n",
    "     # Get frequencies for amplitudes in Hz\n",
    "    samplingFrequency = 128\n",
    "    fft_freq = np.fft.rfftfreq(video.shape[-1], 1.0/samplingFrequency)\n",
    "    # Delta, Theta, Alpha, Beta, Gamma\n",
    "    bands = [(0,4),(4,8),(8,12),(12,30),(30,45)]\n",
    "    \n",
    "    band_mask = np.array([np.logical_or(fft_freq < f, fft_freq > t) for f,t in bands])\n",
    "    band_mask = repeat(band_mask,'a b -> a c b', c=32)\n",
    "    band_data = np.array(fft_vals)\n",
    "    band_data = repeat(band_data,'a b -> c a b', c=5)\n",
    "     \n",
    "    band_data[band_mask] = 0\n",
    "    \n",
    "    band_data = np.fft.irfft(band_data)\n",
    "\n",
    "    windows = skimage.util.view_as_windows(band_data, (5,32,128), step=128).squeeze()\n",
    "    # (5, 32, 60, 128)\n",
    "    windows = rearrange(windows, 'a b c d -> b c a d')\n",
    "    \n",
    "    if feature == 'psd':\n",
    "        features = scipy.signal.periodogram(windows)[1]\n",
    "        features = np.mean(features, axis=-1)\n",
    "    elif feature == 'de':\n",
    "        features = np.apply_along_axis(calculate_de, -1, windows)\n",
    "\n",
    "    \n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_video(signal_data):\n",
    "    electrodes = Electrodes(expand_3d=False)\n",
    "    fig, axs = plt.subplots(32, sharex=True, figsize=(20,50))\n",
    "    fig.tight_layout()\n",
    "    video = signal_data[0]\n",
    "    for i in range(32):\n",
    "        c = [float(i)/float(32), 0.0, float(32-i)/float(32)] #R,G,B\n",
    "        axs[i].set_title(f'{electrodes.channel_names[i]}', loc='left', fontsize=20)\n",
    "        axs[i].plot(video[i],color=c)\n",
    "        axs[i].spines['top'].set_visible(False)\n",
    "        axs[i].spines['right'].set_visible(False)\n",
    "        axs[i].spines['left'].set_visible(False)\n",
    "        axs[i].spines['bottom'].set_visible(False)\n",
    "    plt.savefig('eeg.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baseline_mean(signal_data):\n",
    "    # Take first three senconds of data\n",
    "    signal_baseline = np.array(signal_data[:,:,:128*3]).reshape(40,32,128,-1)\n",
    "    # Mean of three senconds of baseline will be deducted from all windows\n",
    "    signal_noise = np.mean(signal_baseline,axis=-1)\n",
    "    # Expand mask\n",
    "    signal_noise = repeat(signal_noise,'a b c -> a b (d c)',d=60)\n",
    "    return signal_data[:,:,128*3:] - signal_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_wavelet(video, feature='energy', time_domain=False):\n",
    "    band_widths = [32,16,8,4]\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            # Highest frequencies (64-128Hz) are not used\n",
    "            cA, cD = pywt.dwt(video.numpy(), 'db4')\n",
    "        else:\n",
    "            cA, cD = pywt.dwt(cA, 'db4')\n",
    "            cA_windows = skimage.util.view_as_windows(cA, (32,band_widths[i-1]*2), step=band_widths[i-1]).squeeze()\n",
    "            cA_windows = np.transpose(cA_windows[:59,:,:],(1,0,2))\n",
    "            if feature == 'energy':\n",
    "                cA_windows = np.square(cA_windows)\n",
    "                cA_windows = np.sum(cA_windows, axis=-1)\n",
    "                features.append(cA_windows)\n",
    "                \n",
    "    if time_domain:\n",
    "        features = np.transpose(features,(2,1,0))\n",
    "    features = rearrange(features, 'a b c -> (a b) c')\n",
    "    features = torch.FloatTensor(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global connections\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[776], edge_index=[2, 776], x=[128, 59], y=[1, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants used to define data paths\n",
    "ROOT_DIR = './'\n",
    "RAW_DIR = 'data/matlabPREPROCESSED'\n",
    "PROCESSED_DIR = 'data/graphProcessedData'\n",
    "\n",
    "dataset = DEAPDatasetEEGFeatures(root= ROOT_DIR, raw_dir= RAW_DIR, processed_dir= PROCESSED_DIR, feature='wav')\n",
    "# dataset = dataset.shuffle()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1240, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 880 used for training, 220 validation and 180 testing\n",
    "test_participant = 1\n",
    "# \n",
    "# splt_idx = 35\n",
    "\n",
    "# 85% used for train/val\n",
    "train_dataset = dataset[0:test_participant*40] + dataset[test_participant*40+40:]\n",
    "test_dataset = dataset[test_participant*40:test_participant*40+40]\n",
    "\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCN2Conv, GCNConv\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=512, cnn_hidden_dim = 256):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        self.gconv1 = GCN2Conv(in_channels,1)\n",
    "        self.gconv2 = GCNConv(in_channels,hidden_channels)\n",
    "        \n",
    "#         self.gconv3 = GCNConv(in_channels,hidden_channels)\n",
    "        \n",
    "        # self.rnn = torch.nn.GRU(hidden_channels, rnn_hidden_dim, 2,dropout=0.2, batch_first=True)\n",
    "        self.cnn1 = torch.nn.Conv1d(4*hidden_channels, hidden_channels, kernel_size=1, stride=1)\n",
    "        self.cnn2 = torch.nn.Conv1d(hidden_channels, cnn_hidden_dim, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(32*cnn_hidden_dim, 1)\n",
    "#         self.lin2 = torch.nn.Linear(cnn_hidden_dim, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        bs = len(torch.unique(batch.batch))\n",
    "        x_, edge_index, edge_attr = batch.x, batch.edge_index, batch.edge_attr\n",
    "#         print(x.shape)\n",
    "        x = self.gconv1(x_,x_, edge_index, edge_attr)\n",
    "#         x = x.relu()\n",
    "        x = self.gconv2(x, edge_index, edge_attr)\n",
    "        x = x.tanh()\n",
    "        \n",
    "#         x = self.gconv3(x, edge_index, edge_attr )\n",
    "#         print('-0----------------------------------------------------')\n",
    "#         print(x)\n",
    "#         x = x.relu()\n",
    "#         print(x)\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # x = rearrange(x, '(bs a b) c -> (bs b) a c', bs=bs, b=32, c=self.hidden_channels)\n",
    "        # o, (h_n,c_n) = self.rnn(x)\n",
    "        x = rearrange(x, '(bs g e) f -> bs (g f) e', bs=bs, e=32)\n",
    "        x = self.cnn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.cnn2(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = rearrange(x, 'bs a b -> bs (a b)', bs=bs)\n",
    "        \n",
    "        \n",
    "        x = self.lin1(x)\n",
    "#         x = x.relu()\n",
    "#         x = self.lin2(x)\n",
    "        x = x.view(-1)\n",
    "        return x.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count: 1222810\n",
      "Epoch 1;t loss: 0.69028 ;t acc: 0.53 ;v loss: 0.65962 ;v acc: 0.68\n",
      "Epoch 2;t loss: 0.68474 ;t acc: 0.54 ;v loss: 0.66483 ;v acc: 0.68\n",
      "Epoch 3;t loss: 0.67899 ;t acc: 0.56 ;v loss: 0.66622 ;v acc: 0.68\n",
      "Epoch 4;t loss: 0.67583 ;t acc: 0.57 ;v loss: 0.66557 ;v acc: 0.68\n",
      "Epoch 5;t loss: 0.67444 ;t acc: 0.57 ;v loss: 0.66449 ;v acc: 0.68\n",
      "Epoch 6;t loss: 0.67077 ;t acc: 0.58 ;v loss: 0.66245 ;v acc: 0.68\n",
      "Epoch 7;t loss: 0.66990 ;t acc: 0.59 ;v loss: 0.65995 ;v acc: 0.68\n",
      "Epoch 8;t loss: 0.67003 ;t acc: 0.58 ;v loss: 0.66236 ;v acc: 0.68\n",
      "Epoch 9;t loss: 0.67031 ;t acc: 0.59 ;v loss: 0.66186 ;v acc: 0.68\n",
      "Epoch 10;t loss: 0.66607 ;t acc: 0.60 ;v loss: 0.66045 ;v acc: 0.66\n",
      "Epoch 11;t loss: 0.66198 ;t acc: 0.61 ;v loss: 0.65771 ;v acc: 0.68\n",
      "Epoch 12;t loss: 0.66201 ;t acc: 0.61 ;v loss: 0.65930 ;v acc: 0.68\n",
      "Epoch 13;t loss: 0.66247 ;t acc: 0.59 ;v loss: 0.66114 ;v acc: 0.68\n",
      "Epoch 14;t loss: 0.66044 ;t acc: 0.62 ;v loss: 0.65713 ;v acc: 0.68\n",
      "Epoch 15;t loss: 0.66108 ;t acc: 0.60 ;v loss: 0.65886 ;v acc: 0.68\n",
      "Epoch 16;t loss: 0.65984 ;t acc: 0.62 ;v loss: 0.66086 ;v acc: 0.66\n",
      "Epoch 17;t loss: 0.65978 ;t acc: 0.62 ;v loss: 0.65793 ;v acc: 0.68\n",
      "Epoch 18;t loss: 0.65622 ;t acc: 0.63 ;v loss: 0.65565 ;v acc: 0.68\n",
      "Epoch 19;t loss: 0.65581 ;t acc: 0.62 ;v loss: 0.65871 ;v acc: 0.66\n",
      "Epoch 20;t loss: 0.65802 ;t acc: 0.63 ;v loss: 0.65814 ;v acc: 0.66\n",
      "tensor([0.6138], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4623], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5121], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5133], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5652], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5280], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5545], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5620], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5426], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5573], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5412], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5182], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5533], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5166], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5824], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6019], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5459], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5627], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5711], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4996], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4821], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4976], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5181], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5613], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5983], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5893], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5466], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5766], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4841], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6032], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5451], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5724], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5330], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6030], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5539], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5104], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5498], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4948], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5656], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5587], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.684160728007555 ; Test acc: 0.6\n",
      "Epoch 21;t loss: 0.65457 ;t acc: 0.63 ;v loss: 0.65893 ;v acc: 0.66\n",
      "Epoch 22;t loss: 0.65390 ;t acc: 0.63 ;v loss: 0.65785 ;v acc: 0.66\n",
      "Epoch 23;t loss: 0.65212 ;t acc: 0.64 ;v loss: 0.65831 ;v acc: 0.66\n",
      "Epoch 24;t loss: 0.65051 ;t acc: 0.64 ;v loss: 0.65793 ;v acc: 0.65\n",
      "Epoch 25;t loss: 0.65021 ;t acc: 0.65 ;v loss: 0.65799 ;v acc: 0.65\n",
      "Epoch 26;t loss: 0.64930 ;t acc: 0.66 ;v loss: 0.65458 ;v acc: 0.66\n",
      "Epoch 27;t loss: 0.64984 ;t acc: 0.66 ;v loss: 0.65389 ;v acc: 0.65\n",
      "Epoch 28;t loss: 0.64687 ;t acc: 0.66 ;v loss: 0.65335 ;v acc: 0.66\n",
      "Epoch 29;t loss: 0.64741 ;t acc: 0.66 ;v loss: 0.65681 ;v acc: 0.65\n",
      "Epoch 30;t loss: 0.64729 ;t acc: 0.66 ;v loss: 0.65575 ;v acc: 0.65\n",
      "Epoch 31;t loss: 0.64583 ;t acc: 0.66 ;v loss: 0.65653 ;v acc: 0.65\n",
      "Epoch 32;t loss: 0.64248 ;t acc: 0.67 ;v loss: 0.65270 ;v acc: 0.65\n",
      "Epoch 33;t loss: 0.64369 ;t acc: 0.66 ;v loss: 0.65626 ;v acc: 0.65\n",
      "Epoch 34;t loss: 0.64224 ;t acc: 0.68 ;v loss: 0.65509 ;v acc: 0.65\n",
      "Epoch 35;t loss: 0.64207 ;t acc: 0.67 ;v loss: 0.65568 ;v acc: 0.65\n",
      "Epoch 36;t loss: 0.64086 ;t acc: 0.67 ;v loss: 0.65556 ;v acc: 0.68\n",
      "Epoch 37;t loss: 0.63765 ;t acc: 0.68 ;v loss: 0.65546 ;v acc: 0.65\n",
      "Epoch 38;t loss: 0.63873 ;t acc: 0.68 ;v loss: 0.65580 ;v acc: 0.66\n",
      "Epoch 39;t loss: 0.63750 ;t acc: 0.69 ;v loss: 0.65414 ;v acc: 0.68\n",
      "Epoch 40;t loss: 0.63571 ;t acc: 0.68 ;v loss: 0.65430 ;v acc: 0.66\n",
      "tensor([0.6595], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4073], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4865], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5009], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5816], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5220], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5408], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5762], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5287], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5582], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5405], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5110], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5628], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5119], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6103], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6455], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5482], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5713], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5846], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4549], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4512], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4760], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5084], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5697], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6437], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6144], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5485], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5884], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4555], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6345], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5490], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5882], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4967], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6373], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5469], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4971], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5652], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4584], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5859], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5736], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6810328289866447 ; Test acc: 0.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41;t loss: 0.63452 ;t acc: 0.69 ;v loss: 0.65116 ;v acc: 0.68\n",
      "Epoch 42;t loss: 0.63432 ;t acc: 0.68 ;v loss: 0.65270 ;v acc: 0.69\n",
      "Epoch 43;t loss: 0.63442 ;t acc: 0.69 ;v loss: 0.65157 ;v acc: 0.66\n",
      "Epoch 44;t loss: 0.63163 ;t acc: 0.70 ;v loss: 0.65232 ;v acc: 0.69\n",
      "Epoch 45;t loss: 0.63425 ;t acc: 0.69 ;v loss: 0.65523 ;v acc: 0.64\n",
      "Epoch 46;t loss: 0.63180 ;t acc: 0.69 ;v loss: 0.65422 ;v acc: 0.65\n",
      "Epoch 47;t loss: 0.62882 ;t acc: 0.70 ;v loss: 0.65542 ;v acc: 0.62\n",
      "Epoch 48;t loss: 0.62693 ;t acc: 0.70 ;v loss: 0.65992 ;v acc: 0.60\n",
      "Epoch 49;t loss: 0.62667 ;t acc: 0.71 ;v loss: 0.66016 ;v acc: 0.61\n",
      "Epoch 50;t loss: 0.62562 ;t acc: 0.73 ;v loss: 0.65504 ;v acc: 0.61\n",
      "Epoch 51;t loss: 0.62537 ;t acc: 0.71 ;v loss: 0.66118 ;v acc: 0.61\n",
      "Epoch 52;t loss: 0.62756 ;t acc: 0.72 ;v loss: 0.65102 ;v acc: 0.65\n",
      "Epoch 53;t loss: 0.62591 ;t acc: 0.71 ;v loss: 0.64939 ;v acc: 0.66\n",
      "Epoch 54;t loss: 0.62113 ;t acc: 0.71 ;v loss: 0.64608 ;v acc: 0.66\n",
      "Epoch 55;t loss: 0.62156 ;t acc: 0.69 ;v loss: 0.65282 ;v acc: 0.60\n",
      "Epoch 56;t loss: 0.62020 ;t acc: 0.71 ;v loss: 0.65293 ;v acc: 0.60\n",
      "Epoch 57;t loss: 0.62268 ;t acc: 0.71 ;v loss: 0.65474 ;v acc: 0.60\n",
      "Epoch 58;t loss: 0.62076 ;t acc: 0.71 ;v loss: 0.65022 ;v acc: 0.62\n",
      "Epoch 59;t loss: 0.61767 ;t acc: 0.72 ;v loss: 0.65457 ;v acc: 0.60\n",
      "Epoch 60;t loss: 0.61708 ;t acc: 0.72 ;v loss: 0.64862 ;v acc: 0.65\n",
      "tensor([0.7034], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3599], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4650], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5091], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6054], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5179], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5255], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5848], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5051], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5553], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5437], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5152], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5812], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5196], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6341], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6861], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5552], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5932], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5935], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4051], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4269], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4598], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5137], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5833], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6903], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6379], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5474], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6074], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4384], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6752], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5579], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5980], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4544], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6778], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5545], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4966], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5871], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4319], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6120], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5878], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6810936190187931 ; Test acc: 0.575\n",
      "Epoch 61;t loss: 0.61513 ;t acc: 0.72 ;v loss: 0.65031 ;v acc: 0.62\n",
      "Epoch 62;t loss: 0.61469 ;t acc: 0.71 ;v loss: 0.65453 ;v acc: 0.61\n",
      "Epoch 63;t loss: 0.61685 ;t acc: 0.71 ;v loss: 0.65040 ;v acc: 0.61\n",
      "Epoch 64;t loss: 0.61314 ;t acc: 0.71 ;v loss: 0.65971 ;v acc: 0.59\n",
      "Epoch 65;t loss: 0.61391 ;t acc: 0.72 ;v loss: 0.65097 ;v acc: 0.61\n",
      "Epoch 66;t loss: 0.61073 ;t acc: 0.73 ;v loss: 0.65303 ;v acc: 0.62\n",
      "Epoch 67;t loss: 0.61033 ;t acc: 0.73 ;v loss: 0.64922 ;v acc: 0.62\n",
      "Epoch 68;t loss: 0.60888 ;t acc: 0.72 ;v loss: 0.65264 ;v acc: 0.61\n",
      "Epoch 69;t loss: 0.60864 ;t acc: 0.73 ;v loss: 0.64829 ;v acc: 0.62\n",
      "Epoch 70;t loss: 0.60802 ;t acc: 0.73 ;v loss: 0.64644 ;v acc: 0.61\n",
      "Epoch 71;t loss: 0.60777 ;t acc: 0.72 ;v loss: 0.64872 ;v acc: 0.62\n",
      "Epoch 72;t loss: 0.60559 ;t acc: 0.72 ;v loss: 0.65278 ;v acc: 0.61\n",
      "Epoch 73;t loss: 0.60396 ;t acc: 0.74 ;v loss: 0.65184 ;v acc: 0.61\n",
      "Epoch 74;t loss: 0.60074 ;t acc: 0.75 ;v loss: 0.65326 ;v acc: 0.61\n",
      "Epoch 75;t loss: 0.60466 ;t acc: 0.72 ;v loss: 0.65802 ;v acc: 0.61\n",
      "Epoch 76;t loss: 0.60127 ;t acc: 0.73 ;v loss: 0.65242 ;v acc: 0.62\n",
      "Epoch 77;t loss: 0.60117 ;t acc: 0.73 ;v loss: 0.65305 ;v acc: 0.62\n",
      "Epoch 78;t loss: 0.59948 ;t acc: 0.73 ;v loss: 0.65811 ;v acc: 0.61\n",
      "Epoch 79;t loss: 0.60107 ;t acc: 0.73 ;v loss: 0.65413 ;v acc: 0.61\n",
      "Epoch 80;t loss: 0.59729 ;t acc: 0.74 ;v loss: 0.65301 ;v acc: 0.64\n",
      "tensor([0.7282], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3023], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4295], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5041], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6150], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4941], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4924], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5768], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4664], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5285], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5264], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5043], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5955], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5180], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6477], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7080], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5363], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5962], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5814], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3485], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3955], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4180], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5050], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5788], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7189], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6418], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5249], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6031], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4138], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7033], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5548], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5814], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3959], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7040], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5463], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4787], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5927], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3874], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6156], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5827], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6817542403936386 ; Test acc: 0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81;t loss: 0.59647 ;t acc: 0.74 ;v loss: 0.65115 ;v acc: 0.64\n",
      "Epoch 82;t loss: 0.59547 ;t acc: 0.73 ;v loss: 0.65024 ;v acc: 0.64\n",
      "Epoch 83;t loss: 0.59644 ;t acc: 0.73 ;v loss: 0.65230 ;v acc: 0.62\n",
      "Epoch 84;t loss: 0.59345 ;t acc: 0.75 ;v loss: 0.65272 ;v acc: 0.62\n",
      "Epoch 85;t loss: 0.59120 ;t acc: 0.74 ;v loss: 0.66038 ;v acc: 0.60\n",
      "Epoch 86;t loss: 0.59104 ;t acc: 0.74 ;v loss: 0.65813 ;v acc: 0.60\n",
      "Epoch 87;t loss: 0.59096 ;t acc: 0.74 ;v loss: 0.65300 ;v acc: 0.64\n",
      "Epoch 88;t loss: 0.59015 ;t acc: 0.74 ;v loss: 0.65743 ;v acc: 0.60\n",
      "Epoch 89;t loss: 0.58993 ;t acc: 0.75 ;v loss: 0.65370 ;v acc: 0.62\n",
      "Epoch 90;t loss: 0.58677 ;t acc: 0.75 ;v loss: 0.65925 ;v acc: 0.60\n",
      "Epoch 91;t loss: 0.58452 ;t acc: 0.74 ;v loss: 0.64164 ;v acc: 0.64\n",
      "Epoch 92;t loss: 0.58546 ;t acc: 0.74 ;v loss: 0.65594 ;v acc: 0.60\n",
      "Epoch 93;t loss: 0.58472 ;t acc: 0.76 ;v loss: 0.65595 ;v acc: 0.60\n",
      "Epoch 94;t loss: 0.58218 ;t acc: 0.74 ;v loss: 0.66106 ;v acc: 0.59\n",
      "Epoch 95;t loss: 0.58111 ;t acc: 0.75 ;v loss: 0.65037 ;v acc: 0.65\n",
      "Epoch 96;t loss: 0.57957 ;t acc: 0.74 ;v loss: 0.65346 ;v acc: 0.62\n",
      "Epoch 97;t loss: 0.57987 ;t acc: 0.74 ;v loss: 0.65080 ;v acc: 0.62\n",
      "Epoch 98;t loss: 0.57991 ;t acc: 0.75 ;v loss: 0.64774 ;v acc: 0.62\n",
      "Epoch 99;t loss: 0.57740 ;t acc: 0.76 ;v loss: 0.65686 ;v acc: 0.59\n",
      "Epoch 100;t loss: 0.57603 ;t acc: 0.76 ;v loss: 0.66198 ;v acc: 0.59\n",
      "tensor([0.7402], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2486], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3884], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4980], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6234], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4634], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4556], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5525], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4167], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4877], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4925], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4827], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6105], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5034], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6548], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7198], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5122], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5944], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5510], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2946], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3560], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3780], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4816], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5604], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7277], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6320], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4844], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5905], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3790], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7212], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5490], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5484], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3427], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7210], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5280], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4587], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5918], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3537], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6143], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5618], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6866994604468346 ; Test acc: 0.6\n",
      "Epoch 101;t loss: 0.57533 ;t acc: 0.75 ;v loss: 0.65017 ;v acc: 0.62\n",
      "Epoch 102;t loss: 0.57704 ;t acc: 0.75 ;v loss: 0.65915 ;v acc: 0.59\n",
      "Epoch 103;t loss: 0.57419 ;t acc: 0.76 ;v loss: 0.65103 ;v acc: 0.64\n",
      "Epoch 104;t loss: 0.57158 ;t acc: 0.75 ;v loss: 0.65382 ;v acc: 0.59\n",
      "Epoch 105;t loss: 0.57223 ;t acc: 0.75 ;v loss: 0.65040 ;v acc: 0.62\n",
      "Epoch 106;t loss: 0.56806 ;t acc: 0.75 ;v loss: 0.65014 ;v acc: 0.62\n",
      "Epoch 107;t loss: 0.56863 ;t acc: 0.76 ;v loss: 0.65184 ;v acc: 0.62\n",
      "Epoch 108;t loss: 0.56642 ;t acc: 0.76 ;v loss: 0.64950 ;v acc: 0.62\n",
      "Epoch 109;t loss: 0.56700 ;t acc: 0.75 ;v loss: 0.65520 ;v acc: 0.59\n",
      "Epoch 110;t loss: 0.56480 ;t acc: 0.76 ;v loss: 0.66509 ;v acc: 0.60\n",
      "Epoch 111;t loss: 0.56450 ;t acc: 0.76 ;v loss: 0.65154 ;v acc: 0.62\n",
      "Epoch 112;t loss: 0.56375 ;t acc: 0.76 ;v loss: 0.65477 ;v acc: 0.60\n",
      "Epoch 113;t loss: 0.56198 ;t acc: 0.76 ;v loss: 0.65661 ;v acc: 0.59\n",
      "Epoch 114;t loss: 0.55960 ;t acc: 0.77 ;v loss: 0.66274 ;v acc: 0.59\n",
      "Epoch 115;t loss: 0.55846 ;t acc: 0.77 ;v loss: 0.65581 ;v acc: 0.59\n",
      "Epoch 116;t loss: 0.56025 ;t acc: 0.76 ;v loss: 0.66657 ;v acc: 0.60\n",
      "Epoch 117;t loss: 0.55826 ;t acc: 0.77 ;v loss: 0.65627 ;v acc: 0.59\n",
      "Epoch 118;t loss: 0.55511 ;t acc: 0.76 ;v loss: 0.64366 ;v acc: 0.62\n",
      "Epoch 119;t loss: 0.55406 ;t acc: 0.77 ;v loss: 0.65733 ;v acc: 0.59\n",
      "Epoch 120;t loss: 0.55293 ;t acc: 0.76 ;v loss: 0.64941 ;v acc: 0.62\n",
      "tensor([0.7817], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2250], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3810], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5403], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6617], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4842], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4514], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5607], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3945], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4932], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4939], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5036], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6498], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5419], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6866], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7552], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5273], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6406], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5624], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2696], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3424], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3641], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5030], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5906], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7756], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6694], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4926], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6177], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3938], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7761], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5878], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5562], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3186], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7577], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5535], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4778], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6355], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3541], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6631], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5612], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6948182705789805 ; Test acc: 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121;t loss: 0.55103 ;t acc: 0.78 ;v loss: 0.65733 ;v acc: 0.59\n",
      "Epoch 122;t loss: 0.55143 ;t acc: 0.78 ;v loss: 0.65831 ;v acc: 0.60\n",
      "Epoch 123;t loss: 0.54752 ;t acc: 0.78 ;v loss: 0.64987 ;v acc: 0.61\n",
      "Epoch 124;t loss: 0.54899 ;t acc: 0.78 ;v loss: 0.65320 ;v acc: 0.60\n",
      "Epoch 125;t loss: 0.54723 ;t acc: 0.78 ;v loss: 0.65347 ;v acc: 0.61\n",
      "Epoch 126;t loss: 0.54516 ;t acc: 0.78 ;v loss: 0.65727 ;v acc: 0.60\n",
      "Epoch 127;t loss: 0.54417 ;t acc: 0.77 ;v loss: 0.66130 ;v acc: 0.61\n",
      "Epoch 128;t loss: 0.54197 ;t acc: 0.78 ;v loss: 0.65797 ;v acc: 0.60\n",
      "Epoch 129;t loss: 0.54298 ;t acc: 0.77 ;v loss: 0.65988 ;v acc: 0.60\n",
      "Epoch 130;t loss: 0.54083 ;t acc: 0.79 ;v loss: 0.64791 ;v acc: 0.62\n",
      "Epoch 131;t loss: 0.54030 ;t acc: 0.78 ;v loss: 0.65556 ;v acc: 0.60\n",
      "Epoch 132;t loss: 0.53732 ;t acc: 0.78 ;v loss: 0.65195 ;v acc: 0.62\n",
      "Epoch 133;t loss: 0.53534 ;t acc: 0.78 ;v loss: 0.64814 ;v acc: 0.62\n",
      "Epoch 134;t loss: 0.53436 ;t acc: 0.79 ;v loss: 0.66270 ;v acc: 0.61\n",
      "Epoch 135;t loss: 0.53456 ;t acc: 0.79 ;v loss: 0.66448 ;v acc: 0.60\n",
      "Epoch 136;t loss: 0.53649 ;t acc: 0.78 ;v loss: 0.65614 ;v acc: 0.62\n",
      "Epoch 137;t loss: 0.53416 ;t acc: 0.79 ;v loss: 0.66611 ;v acc: 0.59\n",
      "Epoch 138;t loss: 0.53079 ;t acc: 0.79 ;v loss: 0.65686 ;v acc: 0.62\n",
      "Epoch 139;t loss: 0.52852 ;t acc: 0.79 ;v loss: 0.67070 ;v acc: 0.59\n",
      "Epoch 140;t loss: 0.52846 ;t acc: 0.79 ;v loss: 0.66963 ;v acc: 0.59\n",
      "tensor([0.7674], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.1825], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3242], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5214], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6648], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4370], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4021], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5161], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3325], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4266], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4346], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4428], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6580], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5147], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6764], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7270], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4777], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6198], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5114], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2293], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2974], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2992], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4620], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5532], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7711], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6376], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4225], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5847], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3501], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7787], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5526], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4876], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.2612], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7559], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5096], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4382], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6086], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3120], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6596], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5027], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.7039595618844032 ; Test acc: 0.625\n",
      "Epoch 141;t loss: 0.52660 ;t acc: 0.80 ;v loss: 0.66289 ;v acc: 0.61\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "model = Model(train_dataset[0].x.shape[1]).to(device)  \n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameter count: {pytorch_total_params}')\n",
    "\n",
    "# model = model.to(device)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=.1, rho=0.9, eps=1e-06, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=1e-4, weight_decay=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=1e-4, weight_decay=1e-2)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-5)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-4, lr_decay=0, weight_decay=5e-2)\n",
    "\n",
    "# Instantiate optimizer\n",
    "# scheduler = StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "def train(loader, target = 0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        y = (batch.y[:,target] > 5).float()\n",
    "#         y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        loss = criterion(out,y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        right += torch.eq(out > .5, y > .5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "def test(loader,verbose=False, target = 0):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        y = (batch.y[:,target] > 5).float()\n",
    "#         y = batch.y[:,target]\n",
    "        out = model(batch)\n",
    "        if verbose:\n",
    "            print(out,y)\n",
    "        loss = criterion(out,y)\n",
    "        losses.append(loss.item())\n",
    "        right += torch.eq(out > .5, y > .5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "best_val_loss = np.inf\n",
    "esp = 0\n",
    "MAX_ESP = 50\n",
    "\n",
    "BS = 8\n",
    "\n",
    "target = 0 # Valence-Arousal-Dominance-Liking\n",
    "\n",
    "splt_idx = 1160\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [splt_idx, len(train_dataset)-splt_idx])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BS, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BS)\n",
    "for epoch in range(1, 10000):    \n",
    "\n",
    "    # Training and validation\n",
    "    train_loss, train_acc = train(train_loader, target = target)\n",
    "    val_loss, val_acc = test(val_loader , target = target)\n",
    "    print(f'Epoch {epoch};t loss: {train_loss:.5f} ;t acc: {train_acc:.2f} ;v loss: {val_loss:.5f} ;v acc: {val_acc:.2f}')\n",
    "\n",
    "    # Early stopping and checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        esp = 0\n",
    "        torch.save(model.state_dict(),'./best_params') \n",
    "    else:\n",
    "        esp += 1\n",
    "        if esp >= MAX_ESP:\n",
    "            break\n",
    "            \n",
    "    if epoch % 20 == 0:\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "        loss, acc = test(test_loader, True)\n",
    "        print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5842350984441823 ; Train acc: 0.7387931034482759\n",
      "Val loss: 0.6416360288858414 ; Val acc: 0.6375\n",
      "tensor([0.7650], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.2958], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4349], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5382], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6427], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5136], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4985], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5887], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4608], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5371], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5424], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5316], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6189], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5471], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6737], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7404], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5590], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6330], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5959], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.3332], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.4004], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4281], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5237], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6033], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.7590], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6740], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.5429], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6325], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.4288], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7439], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5893], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6036], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3873], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.7345], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5778], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5048], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "tensor([0.6288], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.3930], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.6493], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([0.], device='cuda:0')\n",
      "tensor([0.5942], device='cuda:0', grad_fn=<SigmoidBackward>) tensor([1.], device='cuda:0')\n",
      "Test loss: 0.6868231624364853 ; Test acc: 0.6\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_params'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "loss, acc = test(train_loader, False,target=target)\n",
    "print(f'Train loss: {loss} ; Train acc: {acc}')\n",
    "loss, acc = test(val_loader, False,target=target)\n",
    "\n",
    "print(f'Val loss: {loss} ; Val acc: {acc}')\n",
    "loss, acc = test(test_loader, True,target=target)\n",
    "print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "# TODO: scheduler(?) Loss/acc records"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
