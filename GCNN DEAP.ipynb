{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "from einops import reduce, rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data, DataLoader\n",
    "from Electrodes import Electrodes\n",
    "from tqdm import tqdm\n",
    "class DEAPDatasetEEGFeatures(InMemoryDataset):\n",
    "    \n",
    "  def __init__(self, root, raw_dir,processed_dir, transform=None, pre_transform=None,include_edge_attr = False, undirected_graphs = True, add_global_connections=True, participant_from=1, participant_to=32,window_size=128, n_videos=40):\n",
    "      self._raw_dir = raw_dir\n",
    "      self._processed_dir = processed_dir\n",
    "      self.participant_from = participant_from\n",
    "      self.participant_to = participant_to\n",
    "      self.n_videos = n_videos\n",
    "      self.window_size = window_size\n",
    "      # Whether or not to include edge_attr in the dataset\n",
    "      self.include_edge_attr = include_edge_attr\n",
    "      # If true there will be 1024 links as opposed to 528\n",
    "      self.undirected_graphs = undirected_graphs\n",
    "      # Instantiate class to handle electrode positions\n",
    "      print('Using global connections' if add_global_connections else 'Not using global connections')\n",
    "      self.electrodes = Electrodes(add_global_connections, expand_3d = False)\n",
    "      super(DEAPDatasetEEGFeatures, self).__init__(root, transform, pre_transform)\n",
    "      self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "      \n",
    "\n",
    "  @property\n",
    "  def raw_dir(self):\n",
    "      return f'{self.root}/{self._raw_dir}'\n",
    "\n",
    "  @property\n",
    "  def processed_dir(self):\n",
    "      return f'{self.root}/{self._processed_dir}'\n",
    "\n",
    "  @property\n",
    "  def raw_file_names(self):\n",
    "      raw_names = [f for f in os.listdir(self.raw_dir)]\n",
    "      raw_names.sort()\n",
    "      return raw_names\n",
    "\n",
    "  @property\n",
    "  def processed_file_names(self):\n",
    "      if not os.path.exists(self.processed_dir):\n",
    "        os.makedirs(self.processed_dir)\n",
    "      file_name = f'{self.participant_from}-{self.participant_to}' if self.participant_from is not self.participant_to else f'{self.participant_from}'\n",
    "      return [f'deap_processed_graph.{file_name}.dataset']\n",
    "\n",
    "  def process(self):\n",
    "        # Number of nodes per graph\n",
    "        n_nodes = len(self.electrodes.positions_3d)\n",
    "\n",
    "        if self.undirected_graphs:\n",
    "            source_nodes, target_nodes = np.repeat(np.arange(0,n_nodes),n_nodes), np.tile(np.arange(0,n_nodes),n_nodes)\n",
    "        else:\n",
    "            source_nodes, target_nodes = np.tril_indices(n_nodes,n_nodes)\n",
    "        \n",
    "        edge_attr = self.electrodes.adjacency_matrix[source_nodes,target_nodes]\n",
    "        \n",
    "        # Remove zero weight links\n",
    "        mask = np.ma.masked_not_equal(edge_attr, 0).mask\n",
    "        edge_attr,source_nodes,target_nodes = edge_attr[mask], source_nodes[mask], target_nodes[mask]\n",
    "\n",
    "        edge_attr, edge_index = torch.FloatTensor(edge_attr), torch.tensor([source_nodes,target_nodes], dtype=torch.long)\n",
    "        \n",
    "        # Expand edge_index and edge_attr to match windows\n",
    "        e_edge_index = edge_index.clone()\n",
    "        e_edge_attr = edge_attr.clone()\n",
    "        for i in range(128*60//self.window_size-1):\n",
    "            a = edge_index + e_edge_index.max() + 1\n",
    "            e_edge_index = torch.cat([e_edge_index,a],dim=1)\n",
    "            e_edge_attr = torch.cat([e_edge_attr,edge_attr],dim=0)\n",
    "\n",
    "        # List of graphs that will be written to file\n",
    "        data_list = []\n",
    "        pbar = tqdm(range(self.participant_from,self.participant_to+1))\n",
    "        for participant_id in pbar:\n",
    "            raw_name = [e for e in self.raw_file_names if str(participant_id).zfill(2) in e][0]\n",
    "            pbar.set_description(raw_name)\n",
    "            # Load raw file as np array\n",
    "            participant_data = scipy.io.loadmat(f'{self.raw_dir}/{raw_name}')\n",
    "            signal_data = torch.FloatTensor(participant_data['data'][:,:32,128*3:])\n",
    "            processed = []\n",
    "            for i, video in enumerate(signal_data[:self.n_videos,:,:]):\n",
    "                video = video.reshape(-1,128)\n",
    "                n = video[0].shape[-1]\n",
    "                \n",
    "                # Differential entropy features\n",
    "#                 de_features = []\n",
    "#                 for window in video:\n",
    "                    \n",
    "#                     fourier = np.fft.rfft(window)\n",
    "#                     real_absolute_fft = 2.0/n * np.abs(fourier[:n//2])\n",
    "#                     freq = np.fft.rfftfreq(n, d=1./128)\n",
    "\n",
    "#                     delta_mask = np.logical_and(freq > 0.5 ,freq < 4)[:64]\n",
    "#                     delta_values = real_absolute_fft[delta_mask]\n",
    "#                     delta_entropy = scipy.stats.entropy(delta_values)\n",
    "\n",
    "#                     theta_mask = np.logical_and(freq > 4 ,freq < 8)[:64]\n",
    "#                     theta_values = real_absolute_fft[theta_mask]\n",
    "#                     theta_entropy = scipy.stats.entropy(theta_values)\n",
    "\n",
    "#                     alpha_mask = np.logical_and(freq > 8 ,freq < 12)[:64]\n",
    "#                     alpha_values = real_absolute_fft[alpha_mask]\n",
    "#                     alpha_entropy = scipy.stats.entropy(alpha_values)\n",
    "\n",
    "#                     beta_mask = np.logical_and(freq > 12 ,freq < 30)[:64]\n",
    "#                     beta_values = real_absolute_fft[beta_mask]\n",
    "#                     beta_entropy = scipy.stats.entropy(beta_values)\n",
    "\n",
    "#                     gamma_mask = np.logical_and(freq > 30 ,freq < 45)[:64]\n",
    "#                     gamma_values = real_absolute_fft[gamma_mask]\n",
    "#                     gamma_entropy = scipy.stats.entropy(gamma_values)\n",
    "                \n",
    "#                     window_features = torch.FloatTensor([delta_entropy, theta_entropy, alpha_entropy, beta_entropy, gamma_entropy])\n",
    "#                     de_features.append(window_features)\n",
    "                \n",
    "#                 de_features = torch.stack(de_features)\n",
    "#                 node_features = de_features\n",
    "            \n",
    "                # Power spectral density for each channel\n",
    "                psd = scipy.signal.periodogram(video)[1]\n",
    "                node_features = psd\n",
    "                \n",
    "                # Raw signals \n",
    "                # node_features = video\n",
    "                \n",
    "                # Should we add MinMax/Z scaler?\n",
    "                data = Data(x=torch.FloatTensor(node_features),edge_attr=e_edge_attr,edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]])) if self.include_edge_attr else Data(x=torch.FloatTensor(node_features), edge_index=e_edge_index, y=torch.FloatTensor([participant_data['labels'][i]]))\n",
    "                data_list.append(data) \n",
    "               \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using global connections\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 11640], x=[1920, 65], y=[1, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants used to define data paths\n",
    "ROOT_DIR = './'\n",
    "RAW_DIR = 'data/matlabPREPROCESSED'\n",
    "PROCESSED_DIR = 'data/graphProcessedData'\n",
    "\n",
    "dataset = DEAPDatasetEEGFeatures(root= ROOT_DIR, raw_dir= RAW_DIR, processed_dir= PROCESSED_DIR)\n",
    "# Subject-independent classification\n",
    "# DEPENDING ON WHAT DATA IS USED THE NETWORK LEARNS BETTER OR WORSE.\n",
    "# SHOULD WE TRY TO HAVE A BALANCE TRAINING SET?\n",
    "dataset = dataset.shuffle()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DEAPDatasetEEGFeatures(1100), DEAPDatasetEEGFeatures(180))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 880 used for training, 220 validation and 180 testing\n",
    "splt_idx = 1100\n",
    "# splt_idx = 35\n",
    "\n",
    "# 85% used for train/val\n",
    "train_dataset = dataset[:splt_idx]\n",
    "test_dataset = dataset[splt_idx:]\n",
    "\n",
    "train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Model, self).__init__()\n",
    "        self.gconv1 = GCNConv(in_channels, 256, aggr='add')\n",
    "        self.gconv2 = GCNConv(256, 128, aggr='add')\n",
    "        self.gconv3 = GCNConv(128, 1, aggr='add')\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(60, 8, 1, stride=1)\n",
    "        \n",
    "        self.lin1 = nn.Linear(8*32,32)\n",
    "        self.lin2 = nn.Linear(32,1)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        bs = len(torch.unique(batch.batch))\n",
    "        x, edge_index = batch.x, batch.edge_index\n",
    "        \n",
    "        x = self.gconv1(x, edge_index)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.gconv2(x, edge_index)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.gconv3(x, edge_index)\n",
    "        x = torch.tanh(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = x.reshape(bs,-1,32)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.reshape(bs,-1)\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x.sigmoid()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameter count: 58666\n",
      "Epoch 1 - Kfold:0 ;t loss: 24.98012 ;t acc: 0.44 ;v loss: 19.26181 ;v acc: 0.48\n",
      "Epoch 2 - Kfold:0 ;t loss: 14.39674 ;t acc: 0.44 ;v loss: 9.03530 ;v acc: 0.48\n",
      "Epoch 3 - Kfold:0 ;t loss: 6.52481 ;t acc: 0.47 ;v loss: 4.56312 ;v acc: 0.48\n",
      "Epoch 4 - Kfold:0 ;t loss: 4.94089 ;t acc: 0.51 ;v loss: 4.37789 ;v acc: 0.52\n",
      "Epoch 5 - Kfold:0 ;t loss: 4.93372 ;t acc: 0.53 ;v loss: 4.37732 ;v acc: 0.52\n",
      "Epoch 6 - Kfold:0 ;t loss: 4.89964 ;t acc: 0.52 ;v loss: 4.37830 ;v acc: 0.52\n",
      "Epoch 7 - Kfold:0 ;t loss: 4.89751 ;t acc: 0.52 ;v loss: 4.38123 ;v acc: 0.54\n",
      "Epoch 8 - Kfold:0 ;t loss: 4.78496 ;t acc: 0.54 ;v loss: 4.38649 ;v acc: 0.52\n",
      "Epoch 9 - Kfold:0 ;t loss: 4.93338 ;t acc: 0.49 ;v loss: 4.39819 ;v acc: 0.52\n",
      "Epoch 10 - Kfold:1 ;t loss: 4.79636 ;t acc: 0.50 ;v loss: 5.02512 ;v acc: 0.49\n",
      "Epoch 11 - Kfold:1 ;t loss: 4.77461 ;t acc: 0.53 ;v loss: 5.05100 ;v acc: 0.49\n",
      "Epoch 12 - Kfold:1 ;t loss: 4.66230 ;t acc: 0.54 ;v loss: 5.03992 ;v acc: 0.50\n",
      "Epoch 13 - Kfold:1 ;t loss: 4.72670 ;t acc: 0.54 ;v loss: 5.07104 ;v acc: 0.52\n",
      "Epoch 14 - Kfold:1 ;t loss: 4.78323 ;t acc: 0.51 ;v loss: 5.06189 ;v acc: 0.51\n",
      "Epoch 15 - Kfold:1 ;t loss: 4.69686 ;t acc: 0.55 ;v loss: 5.07277 ;v acc: 0.46\n",
      "Epoch 16 - Kfold:1 ;t loss: 4.53917 ;t acc: 0.55 ;v loss: 5.07670 ;v acc: 0.49\n",
      "Epoch 17 - Kfold:1 ;t loss: 4.65414 ;t acc: 0.54 ;v loss: 5.09545 ;v acc: 0.47\n",
      "Epoch 18 - Kfold:1 ;t loss: 4.47607 ;t acc: 0.58 ;v loss: 5.10681 ;v acc: 0.49\n",
      "Epoch 19 - Kfold:1 ;t loss: 4.42984 ;t acc: 0.56 ;v loss: 5.10296 ;v acc: 0.51\n",
      "Epoch 20 - Kfold:2 ;t loss: 4.69644 ;t acc: 0.53 ;v loss: 4.33654 ;v acc: 0.59\n",
      "Epoch 21 - Kfold:2 ;t loss: 4.89254 ;t acc: 0.51 ;v loss: 4.30424 ;v acc: 0.63\n",
      "Epoch 22 - Kfold:2 ;t loss: 4.68199 ;t acc: 0.55 ;v loss: 4.28771 ;v acc: 0.62\n",
      "Epoch 23 - Kfold:2 ;t loss: 4.84564 ;t acc: 0.53 ;v loss: 4.27870 ;v acc: 0.62\n",
      "Epoch 24 - Kfold:2 ;t loss: 4.69071 ;t acc: 0.54 ;v loss: 4.28841 ;v acc: 0.60\n",
      "Epoch 25 - Kfold:2 ;t loss: 4.58812 ;t acc: 0.56 ;v loss: 4.30114 ;v acc: 0.60\n",
      "Epoch 26 - Kfold:2 ;t loss: 4.70334 ;t acc: 0.54 ;v loss: 4.27055 ;v acc: 0.60\n",
      "Epoch 27 - Kfold:2 ;t loss: 4.75833 ;t acc: 0.54 ;v loss: 4.27484 ;v acc: 0.60\n",
      "Epoch 28 - Kfold:2 ;t loss: 4.57856 ;t acc: 0.54 ;v loss: 4.27850 ;v acc: 0.60\n",
      "Epoch 29 - Kfold:2 ;t loss: 4.60015 ;t acc: 0.56 ;v loss: 4.27489 ;v acc: 0.60\n",
      "Epoch 30 - Kfold:3 ;t loss: 4.76357 ;t acc: 0.56 ;v loss: 3.94424 ;v acc: 0.55\n",
      "Epoch 31 - Kfold:3 ;t loss: 4.66515 ;t acc: 0.57 ;v loss: 3.96080 ;v acc: 0.55\n",
      "Epoch 32 - Kfold:3 ;t loss: 4.76969 ;t acc: 0.55 ;v loss: 3.95788 ;v acc: 0.54\n",
      "Epoch 33 - Kfold:3 ;t loss: 4.68749 ;t acc: 0.56 ;v loss: 3.95285 ;v acc: 0.55\n",
      "Epoch 34 - Kfold:3 ;t loss: 4.73010 ;t acc: 0.56 ;v loss: 3.96199 ;v acc: 0.55\n",
      "Epoch 35 - Kfold:3 ;t loss: 4.68272 ;t acc: 0.57 ;v loss: 3.97094 ;v acc: 0.55\n",
      "Epoch 36 - Kfold:3 ;t loss: 4.66625 ;t acc: 0.57 ;v loss: 3.99465 ;v acc: 0.56\n",
      "Epoch 37 - Kfold:3 ;t loss: 4.76721 ;t acc: 0.56 ;v loss: 4.00736 ;v acc: 0.55\n",
      "Epoch 38 - Kfold:3 ;t loss: 4.68351 ;t acc: 0.57 ;v loss: 4.00988 ;v acc: 0.55\n",
      "Epoch 39 - Kfold:3 ;t loss: 4.55806 ;t acc: 0.59 ;v loss: 4.00344 ;v acc: 0.55\n",
      "Epoch 40 - Kfold:4 ;t loss: 4.62137 ;t acc: 0.53 ;v loss: 4.28830 ;v acc: 0.68\n",
      "Epoch 41 - Kfold:4 ;t loss: 4.55541 ;t acc: 0.56 ;v loss: 4.30033 ;v acc: 0.68\n",
      "Epoch 42 - Kfold:4 ;t loss: 4.43507 ;t acc: 0.56 ;v loss: 4.30404 ;v acc: 0.69\n",
      "Epoch 43 - Kfold:4 ;t loss: 4.53934 ;t acc: 0.54 ;v loss: 4.31121 ;v acc: 0.67\n",
      "Epoch 44 - Kfold:4 ;t loss: 4.39883 ;t acc: 0.58 ;v loss: 4.30970 ;v acc: 0.67\n",
      "Epoch 45 - Kfold:4 ;t loss: 4.53217 ;t acc: 0.58 ;v loss: 4.31001 ;v acc: 0.66\n",
      "Epoch 46 - Kfold:4 ;t loss: 4.46207 ;t acc: 0.57 ;v loss: 4.32102 ;v acc: 0.66\n",
      "Epoch 47 - Kfold:4 ;t loss: 4.61350 ;t acc: 0.54 ;v loss: 4.30550 ;v acc: 0.68\n",
      "Epoch 48 - Kfold:4 ;t loss: 4.50591 ;t acc: 0.56 ;v loss: 4.31699 ;v acc: 0.64\n",
      "Epoch 49 - Kfold:4 ;t loss: 4.43436 ;t acc: 0.56 ;v loss: 4.31164 ;v acc: 0.66\n",
      "Epoch 50 - Kfold:0 ;t loss: 4.43665 ;t acc: 0.56 ;v loss: 4.02355 ;v acc: 0.60\n",
      "Epoch 51 - Kfold:0 ;t loss: 4.61181 ;t acc: 0.57 ;v loss: 4.03213 ;v acc: 0.59\n",
      "Epoch 52 - Kfold:0 ;t loss: 4.53910 ;t acc: 0.57 ;v loss: 4.14652 ;v acc: 0.60\n",
      "Epoch 53 - Kfold:0 ;t loss: 4.48621 ;t acc: 0.56 ;v loss: 4.10503 ;v acc: 0.59\n",
      "Epoch 54 - Kfold:0 ;t loss: 4.56951 ;t acc: 0.57 ;v loss: 4.11608 ;v acc: 0.58\n",
      "Epoch 55 - Kfold:0 ;t loss: 4.47040 ;t acc: 0.59 ;v loss: 4.18391 ;v acc: 0.59\n",
      "Epoch 56 - Kfold:0 ;t loss: 4.41868 ;t acc: 0.58 ;v loss: 4.20969 ;v acc: 0.58\n",
      "Epoch 57 - Kfold:0 ;t loss: 4.45604 ;t acc: 0.59 ;v loss: 4.12009 ;v acc: 0.57\n",
      "Epoch 58 - Kfold:0 ;t loss: 4.39446 ;t acc: 0.58 ;v loss: 4.16603 ;v acc: 0.58\n",
      "Epoch 59 - Kfold:0 ;t loss: 4.44673 ;t acc: 0.57 ;v loss: 4.20407 ;v acc: 0.57\n",
      "Epoch 60 - Kfold:1 ;t loss: 4.32116 ;t acc: 0.57 ;v loss: 4.71473 ;v acc: 0.56\n",
      "Epoch 61 - Kfold:1 ;t loss: 4.07109 ;t acc: 0.61 ;v loss: 4.76627 ;v acc: 0.56\n",
      "Epoch 62 - Kfold:1 ;t loss: 4.13308 ;t acc: 0.62 ;v loss: 4.64892 ;v acc: 0.60\n",
      "Epoch 63 - Kfold:1 ;t loss: 4.28864 ;t acc: 0.60 ;v loss: 4.74640 ;v acc: 0.56\n",
      "Epoch 64 - Kfold:1 ;t loss: 4.17590 ;t acc: 0.60 ;v loss: 4.66416 ;v acc: 0.58\n",
      "Epoch 65 - Kfold:1 ;t loss: 4.21250 ;t acc: 0.59 ;v loss: 4.76392 ;v acc: 0.58\n",
      "Epoch 66 - Kfold:1 ;t loss: 4.22404 ;t acc: 0.58 ;v loss: 4.80916 ;v acc: 0.57\n",
      "Epoch 67 - Kfold:1 ;t loss: 4.17002 ;t acc: 0.62 ;v loss: 4.74127 ;v acc: 0.57\n",
      "Epoch 68 - Kfold:1 ;t loss: 4.04414 ;t acc: 0.61 ;v loss: 4.82813 ;v acc: 0.57\n",
      "Epoch 69 - Kfold:1 ;t loss: 4.19376 ;t acc: 0.59 ;v loss: 4.90150 ;v acc: 0.56\n",
      "Epoch 70 - Kfold:2 ;t loss: 4.49344 ;t acc: 0.58 ;v loss: 3.78407 ;v acc: 0.63\n",
      "Epoch 71 - Kfold:2 ;t loss: 4.32674 ;t acc: 0.57 ;v loss: 3.72315 ;v acc: 0.66\n",
      "Epoch 72 - Kfold:2 ;t loss: 4.34666 ;t acc: 0.58 ;v loss: 3.80326 ;v acc: 0.63\n",
      "Epoch 73 - Kfold:2 ;t loss: 4.45829 ;t acc: 0.57 ;v loss: 3.78515 ;v acc: 0.65\n",
      "Epoch 74 - Kfold:2 ;t loss: 4.45496 ;t acc: 0.59 ;v loss: 3.80369 ;v acc: 0.65\n",
      "Epoch 75 - Kfold:2 ;t loss: 4.30155 ;t acc: 0.59 ;v loss: 3.82768 ;v acc: 0.64\n",
      "Epoch 76 - Kfold:2 ;t loss: 4.45843 ;t acc: 0.56 ;v loss: 3.89425 ;v acc: 0.62\n",
      "Epoch 77 - Kfold:2 ;t loss: 4.36878 ;t acc: 0.59 ;v loss: 3.79566 ;v acc: 0.66\n",
      "Epoch 78 - Kfold:2 ;t loss: 4.35477 ;t acc: 0.57 ;v loss: 3.87617 ;v acc: 0.64\n",
      "Epoch 79 - Kfold:2 ;t loss: 4.28166 ;t acc: 0.59 ;v loss: 3.82048 ;v acc: 0.65\n",
      "Epoch 80 - Kfold:3 ;t loss: 4.35565 ;t acc: 0.61 ;v loss: 3.73805 ;v acc: 0.60\n",
      "Epoch 81 - Kfold:3 ;t loss: 4.25417 ;t acc: 0.60 ;v loss: 3.88493 ;v acc: 0.58\n",
      "Epoch 82 - Kfold:3 ;t loss: 4.31272 ;t acc: 0.59 ;v loss: 3.88451 ;v acc: 0.58\n",
      "Epoch 83 - Kfold:3 ;t loss: 4.34776 ;t acc: 0.62 ;v loss: 3.85661 ;v acc: 0.58\n",
      "Epoch 84 - Kfold:3 ;t loss: 4.35715 ;t acc: 0.59 ;v loss: 3.85573 ;v acc: 0.59\n",
      "Epoch 85 - Kfold:3 ;t loss: 4.39140 ;t acc: 0.58 ;v loss: 3.92063 ;v acc: 0.59\n",
      "Epoch 86 - Kfold:3 ;t loss: 4.10345 ;t acc: 0.61 ;v loss: 3.92588 ;v acc: 0.59\n",
      "Epoch 87 - Kfold:3 ;t loss: 4.12905 ;t acc: 0.61 ;v loss: 3.90738 ;v acc: 0.59\n",
      "Epoch 88 - Kfold:3 ;t loss: 4.30925 ;t acc: 0.60 ;v loss: 3.89316 ;v acc: 0.59\n",
      "Epoch 89 - Kfold:3 ;t loss: 4.23479 ;t acc: 0.61 ;v loss: 3.89800 ;v acc: 0.58\n",
      "Epoch 90 - Kfold:4 ;t loss: 4.13605 ;t acc: 0.60 ;v loss: 3.86210 ;v acc: 0.67\n",
      "Epoch 91 - Kfold:4 ;t loss: 4.08901 ;t acc: 0.61 ;v loss: 3.86802 ;v acc: 0.68\n",
      "Epoch 92 - Kfold:4 ;t loss: 4.21711 ;t acc: 0.58 ;v loss: 3.99049 ;v acc: 0.69\n",
      "Epoch 93 - Kfold:4 ;t loss: 4.07875 ;t acc: 0.60 ;v loss: 3.90385 ;v acc: 0.67\n",
      "Epoch 94 - Kfold:4 ;t loss: 4.11362 ;t acc: 0.60 ;v loss: 3.90965 ;v acc: 0.66\n",
      "Epoch 95 - Kfold:4 ;t loss: 4.21348 ;t acc: 0.60 ;v loss: 3.90427 ;v acc: 0.66\n",
      "Epoch 96 - Kfold:4 ;t loss: 4.06133 ;t acc: 0.62 ;v loss: 3.91685 ;v acc: 0.66\n",
      "Epoch 97 - Kfold:4 ;t loss: 4.09212 ;t acc: 0.61 ;v loss: 3.92495 ;v acc: 0.66\n",
      "Epoch 98 - Kfold:4 ;t loss: 4.10256 ;t acc: 0.59 ;v loss: 3.93868 ;v acc: 0.66\n",
      "Epoch 99 - Kfold:4 ;t loss: 4.10477 ;t acc: 0.59 ;v loss: 3.97958 ;v acc: 0.68\n",
      "Epoch 100 - Kfold:0 ;t loss: 4.17489 ;t acc: 0.62 ;v loss: 3.87809 ;v acc: 0.57\n",
      "Epoch 101 - Kfold:0 ;t loss: 4.35732 ;t acc: 0.59 ;v loss: 3.87603 ;v acc: 0.58\n",
      "Epoch 102 - Kfold:0 ;t loss: 4.16443 ;t acc: 0.61 ;v loss: 3.91618 ;v acc: 0.58\n",
      "Epoch 103 - Kfold:0 ;t loss: 4.09303 ;t acc: 0.61 ;v loss: 3.92690 ;v acc: 0.58\n",
      "Epoch 104 - Kfold:0 ;t loss: 4.23214 ;t acc: 0.60 ;v loss: 3.86258 ;v acc: 0.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105 - Kfold:0 ;t loss: 4.22951 ;t acc: 0.59 ;v loss: 4.02789 ;v acc: 0.57\n",
      "Epoch 106 - Kfold:0 ;t loss: 4.12524 ;t acc: 0.61 ;v loss: 3.87939 ;v acc: 0.60\n",
      "Epoch 107 - Kfold:0 ;t loss: 4.08284 ;t acc: 0.61 ;v loss: 3.91212 ;v acc: 0.57\n",
      "Epoch 108 - Kfold:0 ;t loss: 4.11837 ;t acc: 0.60 ;v loss: 4.00846 ;v acc: 0.57\n",
      "Epoch 109 - Kfold:0 ;t loss: 4.02578 ;t acc: 0.63 ;v loss: 3.95189 ;v acc: 0.59\n",
      "Epoch 110 - Kfold:1 ;t loss: 4.00637 ;t acc: 0.64 ;v loss: 4.43183 ;v acc: 0.62\n",
      "Epoch 111 - Kfold:1 ;t loss: 3.93885 ;t acc: 0.62 ;v loss: 4.35586 ;v acc: 0.61\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "\n",
    "model = Model(dataset[0].x[1].shape[0])     \n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameter count: {pytorch_total_params}')\n",
    "\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=.1, rho=0.9, eps=1e-06, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=1e-1, weight_decay=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-1)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def train(loader, target = 0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        y = batch.y[:,target] \n",
    "        out = model(batch)\n",
    "        loss = criterion(out,y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        right += torch.eq(out > 5, y > 5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "def test(loader,verbose=False, target = 0):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    right = 0\n",
    "    tot = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        y = batch.y[:,0] # Arousal\n",
    "        out = model(batch)\n",
    "        if verbose:\n",
    "            print(out,y)\n",
    "        loss = criterion(out,y)\n",
    "        losses.append(loss.item())\n",
    "        right += torch.eq(out > 5, y > 5).sum().item()\n",
    "        tot += y.shape[0]\n",
    "    return np.array(losses).mean(), right/tot\n",
    "\n",
    "best_val_loss = np.inf\n",
    "esp = 0\n",
    "MAX_ESP = 40\n",
    "\n",
    "BS = 16\n",
    "\n",
    "k_folds = 5\n",
    "k_fold_size = len(train_dataset)/k_folds\n",
    "current_fold = 0 # Ranges from 0 to k_folds-1\n",
    "\n",
    "target = 0 # Valence\n",
    "for epoch in range(1, 10000):    \n",
    "    # KFOLD train/val split     \n",
    "    if epoch %10 == 0:\n",
    "        current_fold = current_fold+1 if current_fold < k_folds-1 else 0\n",
    "    from_idx, to_idx = int(k_fold_size*current_fold), int(k_fold_size*(current_fold+1))\n",
    "    kf_val_data = train_dataset[from_idx:to_idx]\n",
    "    a = train_dataset[:from_idx]\n",
    "    b = train_dataset[to_idx:]\n",
    "    kf_train_data = a + b\n",
    "    train_loader = DataLoader(kf_train_data, batch_size=BS, shuffle=False)\n",
    "    val_loader = DataLoader(kf_val_data, batch_size=BS)\n",
    "        \n",
    "    # Training and validation\n",
    "    train_loss, train_acc = train(train_loader, target = target)\n",
    "    val_loss, val_acc = test(val_loader , target = target)\n",
    "    print(f'Epoch {epoch} - Kfold:{current_fold} ;t loss: {train_loss:.5f} ;t acc: {train_acc:.2f} ;v loss: {val_loss:.5f} ;v acc: {val_acc:.2f}')\n",
    "\n",
    "    # Early stopping and checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        esp = 0\n",
    "        torch.save(model.state_dict(),'./best_params') \n",
    "    else:\n",
    "        esp += 1\n",
    "        if esp >= MAX_ESP:\n",
    "            break\n",
    "\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9160561829805374 ; Train acc: 0.8488636363636364\n",
      "Val loss: 0.8132187638963971 ; Val acc: 0.8409090909090909\n",
      "Test loss: 7.466709534327189 ; Test acc: 0.5111111111111111\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_params'))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BS)\n",
    "loss, acc = test(train_loader, False)\n",
    "print(f'Train loss: {loss} ; Train acc: {acc}')\n",
    "loss, acc = test(val_loader, False)\n",
    "print(f'Val loss: {loss} ; Val acc: {acc}')\n",
    "loss, acc = test(test_loader, False)\n",
    "print(f'Test loss: {loss} ; Test acc: {acc}')\n",
    "\n",
    "# TODO: scheduler(?) Loss/acc records"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
